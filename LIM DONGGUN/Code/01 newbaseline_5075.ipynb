{"cells":[{"cell_type":"markdown","metadata":{"id":"NtZuSVM6cnQH"},"source":["## 0. 준비"]},{"cell_type":"markdown","metadata":{"id":"VfmW0GNScnQI"},"source":["### 1) 라이브러리 설치"]},{"cell_type":"code","source":["import os\n","os.getcwd()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"H6GDsPyodD5i","executionInfo":{"status":"ok","timestamp":1725898429623,"user_tz":-540,"elapsed":609,"user":{"displayName":"Donggun Lim","userId":"16228281816473625159"}},"outputId":"87417153-0409-492a-e580-75ec09f815ad"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rqFOq8M9cnQJ","executionInfo":{"status":"ok","timestamp":1725898433444,"user_tz":-540,"elapsed":3141,"user":{"displayName":"Donggun Lim","userId":"16228281816473625159"}},"outputId":"e3b3730f-73fb-43f4-c0a1-c1e345d9cead"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.9)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n","Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.14.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"]}],"source":["!pip install wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-IOgvmkrcnQK"},"outputs":[],"source":[" #!wandb --help"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5apP8JeccnQK","executionInfo":{"status":"ok","timestamp":1725898436705,"user_tz":-540,"elapsed":3264,"user":{"displayName":"Donggun Lim","userId":"16228281816473625159"}},"outputId":"a8c698a8-f830-447c-c4ef-9e44e6e8f4b1"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}],"source":["#!wandb login #--relogin\n","# kkukky@naver.com  220215bca12e71dfd5815f1648ca8dbbb2c1bef8\n","#  17b70d1b235684f485db5bcc4b47788ca0e90fd2\n","import wandb\n","wandb.login(key=\"220215bca12e71dfd5815f1648ca8dbbb2c1bef8\")"]},{"cell_type":"code","source":["!pip install wandb\n","!pip install torch\n","!pip install pytorch_lightning\n","!pip install rouge"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KWBlf51GRla5","executionInfo":{"status":"ok","timestamp":1725898444046,"user_tz":-540,"elapsed":7351,"user":{"displayName":"Donggun Lim","userId":"16228281816473625159"}},"outputId":"47cff640-3d55-459d-a06b-585dbf201134"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.10/dist-packages (2.4.0)\n","Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.4.0+cu121)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.5)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.2)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.6.1)\n","Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.4.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.1)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.12.2)\n","Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (0.11.7)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.10.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (71.0.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.15.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.4)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch_lightning) (1.26.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->pytorch_lightning) (1.3.0)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.8)\n","Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBFBErF5cnQK"},"outputs":[],"source":["\n","import pandas as pd             # 데이터 프레임을 다루기 위한 라이브러리입니다. 주로 데이터 처리 및 분석에 사용됩니다.\n","import os                       # 운영 체제와 상호작용하기 위한 모듈로, 파일 및 디렉터리 작업에 사용됩니다.\n","import re                       # 정규 표현식을 사용하여 문자열을 검색, 처리하는 데 사용됩니다.\n","import json                     # JSON 형식의 데이터를 처리하기 위한 라이브러리입니다.\n","import yaml                     # YAML 형식의 데이터를 처리하기 위한 라이브러리입니다.\n","from glob import glob           # 특정 패턴에 맞는 파일 경로들을 리스트로 반환하는 모듈입니다.\n","from tqdm import tqdm           # 반복문에 대한 진행 상황을 시각적으로 보여주는 라이브러리입니다.\n","from pprint import pprint       # 데이터를 좀 더 읽기 쉽게 출력하기 위한 라이브러리입니다.\n","import torch                    # PyTorch 라이브러리로, 딥러닝 모델을 구축하고 학습하기 위한 핵심 라이브러리입니다.\n","import pytorch_lightning as pl  # PyTorch의 고수준 API로, 모델 학습을 간소화하고 구조화된 방식으로 진행할 수 있습니다.\n","from rouge import Rouge         # 텍스트 요약 및 생성 모델의 성능을 평가하기 위해 사용하는 지표 중 하나입니다.\n","\n","from torch.utils.data import Dataset, DataLoader  # 데이터셋을 다루고, 이를 모델 학습에 사용할 수 있도록 배치(batch) 단위로 나누는 데 사용됩니다.\n","from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig  # 트랜스포머 모델을 위한 라이브러리로, 토크나이저와 모델을 불러오는 데 사용됩니다.\n","from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer  # Seq2Seq (Sequence-to-Sequence) 모델 학습을 위한 도구와 설정을 제공합니다.\n","from transformers import Trainer, TrainingArguments  # 일반적인 모델 학습을 위한 도구와 설정을 제공합니다.\n","from transformers import EarlyStoppingCallback  # 학습 과정에서 성능 향상이 없을 때 조기 종료를 할 수 있도록 도와주는 콜백 함수입니다.\n","\n","import wandb                    # 모델 학습 과정을 쉽게 추적하고 시각화할 수 있는 툴입니다. 주로 실험 관리 및 결과 기록에 사용됩니다.\n","\n","#!wandb login #--relogin\n","# kkukky@naver.com     220215bca12e71dfd5815f1648ca8dbbb2c1bef8\n","# kkukky81@gmail.com   17b70d1b235684f485db5bcc4b47788ca0e90fd2\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3xr8XpppcnQL"},"source":["### 2) 모델 선택, title 만들기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JyxBMJtVcnQM","outputId":"ad5e3278-4764-4a0c-85a7-76857a4352e5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725898573611,"user_tz":-540,"elapsed":603,"user":{"displayName":"Donggun Lim","userId":"16228281816473625159"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["###    jx7789_kobart_summary_v33x5075p-l6-n4-b5    ###\n"]}],"source":["from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","### bart 계열\n","# model = \"digit82/kobart-summarization\"\n","# model = \"NLPBada/kobart-chat-persona-extraction-v2\"\n","# model = \"EbanLee/kobart-summary-v3\"\n","model = 'jx7789/kobart_summary_v3'\n","# model = 'gogamza/kobart-summarization'\n","\n","### T5 계열\n","# model = \"t5-small\"\n","# model = \"KETI-AIR/ke-t5-base-ko\" 값이 안나옴\n","# model = \"eenzeenee/t5-base-korean-summarization\" 안됨\n","# model = 'psyche/KoT5-summarization' #안됨\n","# model = 'csebuetnlp/mT5_multilingual_XLSum'\n","\n","\n","train = \"train_3x5075p.csv\"\n","para = \"3x5075p-l6-n4-b5\"\n","\n","\n","title = model.replace('/','_') + para\n","\n","# config 설정에 tokenizer 모듈이 사용되므로 미리 tokenizer를 정의해줍니다.\n","#tokenizer = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")\n","tokenizer = AutoTokenizer.from_pretrained(model)\n","\n","#from transformers import AutoModel\n","\n","#model = AutoModel.from_pretrained(model, force_download=True)\n","\n","print('###   ', title, '   ###')\n","\n","#output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n"]},{"cell_type":"markdown","metadata":{"id":"d9rtdKDScnQM"},"source":["### 3) config 파일 만들기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MQ3y0ECqcnQM"},"outputs":[],"source":["batch = 16\n","config_data = {\n","    \"general\": {\n","        \"data_path\": \"/content/\",  # 모델 학습에 사용할 데이터가 저장된 경로를 지정합니다.\n","        #\"model_name\": \"digit82/kobart-summarization\",  # 사용할 사전 학습된 모델의 이름을 지정합니다.\n","        \"model_name\": model,  # 사용할 사전 학습된 모델의 이름을 지정합니다.\n","        \"output_dir\": \"/content/\"  # 모델의 출력물(예: 생성된 텍스트)을 저장할 디렉터리를 지정합니다.\n","    },\n","    \"training\": {\n","        \"overwrite_output_dir\": True,       # True로 설정하면, 기존에 존재하는 출력 디렉터리 내용을 덮어씁니다.\n","        \"num_train_epochs\": 20,             # 전체 데이터셋을 몇 번 반복해서 학습할지를 설정합니다. (20번)\n","        \"learning_rate\": 1e-6,              # 학습률(learning rate)을 설정합니다.\n","        \"per_device_train_batch_size\": batch,  # 각 디바이스(예: GPU)에서 한 번에 학습할 데이터 배치의 크기를 설정합니다. (50)\n","        \"per_device_eval_batch_size\": batch,   # 평가 시 사용할 배치 크기를 설정합니다. (32)\n","        \"warmup_ratio\": 0.1,                # 학습 초기에 학습률을 천천히 증가시키는 비율을 설정합니다.\n","        \"weight_decay\": 0.01,               # 가중치 감쇠(weight decay) 값을 설정합니다. (과적합 방지를 위해 사용)\n","        \"lr_scheduler_type\": 'cosine_with_restarts',      # 학습률 스케줄러 유형을 'cosine'으로 설정합니다.\n","        \"optim\": 'adamw_torch',             # 옵티마이저(optimizer)로 AdamW를 사용합니다.\n","        \"gradient_accumulation_steps\": 1,   # 기울기(gradient) 축적을 위한 스텝 수를 설정합니다. (1이면 축적 없이 바로 업데이트)\n","        \"evaluation_strategy\": 'epoch',     # 평가를 언제 수행할지 설정합니다. ('epoch'는 각 에폭 종료 시 평가)\n","        \"save_strategy\": 'epoch',           # 모델을 저장할 시점을 설정합니다. ('epoch'는 각 에폭 종료 시 저장)\n","        \"save_total_limit\": 7,              # 최대 저장할 체크포인트 수를 설정합니다. (가장 최근 7개만 유지)\n","        \"fp16\": True,                       # 반정밀도(floating point 16) 연산을 사용할지 설정합니다. (True이면 메모리 절약 및 속도 향상)\n","        \"load_best_model_at_end\": True,     # 학습이 종료될 때 가장 성능이 좋은 모델을 불러옵니다.\n","        \"seed\": 42,                         # 랜덤 시드를 설정하여 실험의 재현성을 보장합니다.\n","        \"logging_dir\": \"./logs\",            # 학습 로그를 저장할 디렉터리를 설정합니다.\n","        \"logging_strategy\": \"epoch\",        # 로그를 언제 기록할지 설정합니다. ('epoch'는 각 에폭 종료 시 기록)\n","        \"predict_with_generate\": True,      # 평가 시 텍스트 생성을 수행할지 설정합니다.\n","        \"generation_max_length\": 200,       # 텍스트 생성 시 최대 길이를 설정합니다. (100 토큰)\n","        \"do_train\": True,                   # 모델을 학습할지 여부를 설정합니다. (True로 설정)\n","        \"do_eval\": True,                    # 모델을 평가할지 여부를 설정합니다. (True로 설정)\n","        \"early_stopping_patience\": 3,       # 조기 종료를 위한 인내 기간(몇 번의 에폭 동안 성능 개선이 없으면 종료)을 설정합니다. (3)\n","        \"early_stopping_threshold\": 0.001,  # 조기 종료를 위한 성능 개선 최소 임계값을 설정합니다. (0.001)\n","        \"report_to\": \"wandb\"                # (선택 사항) wandb를 사용하여 학습 과정을 보고할지 설정합니다.\n","    },\n","    \"inference\": {\n","        \"ckt_path\": \"/content/code/\",  # 학습된 모델의 체크포인트 파일 경로를 설정합니다.\n","        \"result_path\": \"/content/prediction/\",  # 추론 결과를 저장할 경로를 설정합니다.\n","        \"no_repeat_ngram_size\": 4,  # 생성된 텍스트에서 동일한 n-gram이 반복되지 않도록 설정합니다. (2-gram 기준)\n","        \"early_stopping\": True,  # 조기 종료를 사용할지 여부를 설정합니다.\n","        \"generate_max_length\": 200,  # 생성 텍스트의 최대 길이를 설정합니다. (100 토큰)\n","        \"num_beams\": 5,  # 빔 서치(beam search) 시 사용할 빔 수를 설정합니다. 1~5까지 가능함. (4)\n","        \"batch_size\": batch,  # 추론 시 사용할 배치 크기를 설정합니다. (32)\n","        # 모델 생성 결과에서 불필요한 토큰들을 제거합니다.\n","        \"remove_tokens\": ['<usr>', f\"{tokenizer.bos_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n","    },\n","    \"tokenizer\": {\n","        \"encoder_max_len\": 1024,  # 입력 텍스트를 인코딩할 때 최대 길이를 설정합니다. (512 토큰)\n","        \"decoder_max_len\": 200,  # 출력 텍스트(생성된 텍스트)를 디코딩할 때 최대 길이를 설정합니다. (100 토큰)\n","        \"bos_token\": f\"{tokenizer.bos_token}\",  # 시작 토큰(beginning of sentence)을 지정합니다.\n","        \"eos_token\": f\"{tokenizer.eos_token}\",  # 끝 토큰(end of sentence)을 지정합니다.\n","        # 특정 단어들이 분해되지 않도록, special_tokens을 지정하여 토크나이저에서 처리할 때 이들 단어를 그대로 유지합니다.\n","        \"special_tokens\": [ '#Person1#',     #76737\n","                            '#Person2#',     #70211\n","                            '#Person3#',     #452\n","                            '#Person4#',     #41\n","                            '#Person5#',     #5\n","                            '#Person6#',     #9\n","                            '#Person7#',     #3\n","                            '#SSN#',         #3\n","                            '#PhoneNumber#', #203\n","                            '#Address#',     #45\n","                            '#Email#',       #17\n","                            '#CarNumber#',   #6\n","                            '#CardNumber#'   #10\n","                            '#DateOfBirth#', #8\n","                            '#PassportNumber#']  #7\n","                            # #Person 2#    띄어쓰기 오타 1개 o\n","                            # 9547,9548     #Person1      #없는 오타 2개 o\n","                            # 9547,9548     #Person2      #없는 오타 2개 o\n","                            # 9750,9779     Person1#      #없는 오타 2개 o\n","                            # 420           #PhoneNumber  #없는 오타 1개 o\n","                            # #Person#      숫자 없는 오타 1개 o\n","                            # 839      #사람1만기 시 계정 갱신 o\n","                            # 1033sum  이 사람2#에게 내일 아침 o\n","                            # 1125     사람1#: 제니, 이번 o\n","                            # 1133     사람2#은 그 기간 동 o\n","                            # 1030     이 사람2#에게 내일 o\n","                            # 1142     사람1#: 실례합니다. 저는 o\n","                            # 1199     사람1#은 시험에 대한 준비가 된 o\n","                            # 1213     #하지만 장기간의 o\n","                            # 1236     #고객님, 크루즈 컨트롤에 o\n","                            # 1250     ##여기 있습니다. 스티븐 o\n","                            # 1266     #고객님, 저희는 고객이 화나 o\n","                            # 1278     #고객님, 죄송합니다만 계 o\n","                            # 1281     #잠깐만요, 버전 7 o\n","                            # 1283     #어디 보자. 네, 그런 방 o\n","                            # 1301     #샐러드용 드레싱은 o\n","                            # 1302     #페리에와 짐 빔 세 o\n","                            # 1306     #나 부엌에 있어. . . o\n","                            # 1322     #여기서 만나서 반갑 o\n","                            # 1547     #작은 걸로 주세 o\n","                            # 1609     #여기 있습니다. o\n","                            # 정상 8320   음표는 G#이라고 써있어.\n","                            # 10370    회사 #에서 기술자로 근  o\n","                            # 11716    ##: 안녕, 프란시스   #이 두개 o\n","                            # 4001     (Person A가 탈의실에서 나옴) 스웨터는 어떠셨나요? o\n","                            # 2255     전화번호는 610-555-1234입니다. o\n","                            # 2719     네, 488-6361입니다. 3시까지 저 o\n","                            # 2980     전화번호는 513-3284입니다. o\n","    },\n","    # (선택 사항) wandb 설정: wandb 홈페이지에서 받은 entity, project, run_name 정보를 설정합니다.\n","    \"wandb\": {\n","         \"entity\": \"kkukky-empty\",  # wandb에서 실험을 기록할 엔티티(entity)를 설정합니다.\n","         \"project\": \"NLP2\",  # wandb 프로젝트 이름을 설정합니다.\n","         \"name\": title # wandb에서 실행(run) 이름을 설정합니다.\n","    }\n","}\n","\n","\n","### 변수 적용 간편화를 위해 코드 옮김\n","\n","# 모델의 구성 정보를 YAML 파일로 저장합니다.\n","config_path = \"./config_bart.yaml\"\n","with open(config_path, \"w\") as file:\n","    yaml.dump(config_data, file, allow_unicode=True)\n","\n","# 저장된 config 파일을 불러옵니다.\n","config_path = \"./config_bart.yaml\"\n","with open(config_path, \"r\") as file:\n","    config = yaml.safe_load(file)\n","\n","# 불러온 config 파일의 전체 내용을 확인합니다.\n","# pprint(config)\n"]},{"cell_type":"markdown","metadata":{"id":"8GFCt-zucnQN"},"source":["### 4) 데이터 확인"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bGFB77e5SXFw","executionInfo":{"status":"ok","timestamp":1725898502922,"user_tz":-540,"elapsed":54747,"user":{"displayName":"Donggun Lim","userId":"16228281816473625159"}},"outputId":"30124deb-f4c7-4423-d359-e1d7427f294f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqgUZ2YhcnQN","outputId":"69123a91-7942-40fe-a0e1-9e20a08b5890","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"error","timestamp":1725898502922,"user_tz":-540,"elapsed":15,"user":{"displayName":"Donggun Lim","userId":"16228281816473625159"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/train_3x5070p.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-e9445c9551c6>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# train data의 구조와 내용을 확인합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train_3x5070p.csv'"]}],"source":["# config에 저장된 데이터 경로를 통해 train과 validation data를 불러옵니다.\n","data_path = config['general']['data_path']\n","print(data_path)\n","\n","# train data의 구조와 내용을 확인합니다.\n","train_df = pd.read_csv(os.path.join(data_path,train))\n","train_df.tail()"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ryX9Ce6XSVoq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kpVQRVNlcnQN"},"outputs":[],"source":["# validation data의 구조와 내용을 확인합니다.\n","val_df = pd.read_csv(os.path.join(data_path,'dev.csv'))\n","val_df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IPgn0DcJcnQO"},"outputs":[],"source":["# validation data의 구조와 내용을 확인합니다.\n","test_df = pd.read_csv(os.path.join(data_path,'test.csv'))\n","test_df.tail()"]},{"cell_type":"markdown","metadata":{"id":"42oO4hpmcnQO"},"source":["## 1. 모델 트레이닝"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ovbz__WRcnQO","outputId":"86a805e3-8fc9-4680-b204-bb7268f7f11c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725898506046,"user_tz":-540,"elapsed":475,"user":{"displayName":"Donggun Lim","userId":"16228281816473625159"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["---------- device : cuda:0 ----------\n","2.4.0+cu121\n"]}],"source":["# 사용할 device를 정의합니다. GPU가 사용 가능하면 'cuda:0', 그렇지 않으면 'cpu'를 사용합니다.\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('-' * 10, f'device : {device}', '-' * 10,)\n","print(torch.__version__)"]},{"cell_type":"markdown","metadata":{"id":"6FqKyf7vcnQO"},"source":["### 1) 토크나이저와 모델 로드"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0vq86UgBcnQO"},"outputs":[],"source":["def load_tokenizer_and_model_for_train(config, device):\n","    print('-' * 10, 'Load tokenizer & model', '-' * 10,)\n","    print('-' * 10, f'Model Name : {config[\"general\"][\"model_name\"]}', '-' * 10,)\n","\n","     #모델 이름 불러오기:\n","    # config['general']['model_name']을 통해 사전 학습된 모델의 이름을 설정 파일에서 불러옵니다.\n","    # 이는 Hugging Face의 모델 허브에서 가져올 모델의 이름입니다.\n","    model_name = config['general']['model_name']\n","\n","\n","    ### BART ##################\n","    # BART 설정 로드:\n","    # BartConfig를 사용하여 지정된 모델 이름으로부터 BART의 설정을 불러옵니다.\n","    # 이 설정은 모델의 구조와 하이퍼파라미터를 정의합니다.\n","    bart_config = BartConfig().from_pretrained(model_name)\n","\n","    # 토크나이저 로드:\n","    # AutoTokenizer.from_pretrained를 사용하여 지정된 모델 이름으로부터 토크나이저를 불러옵니다.\n","    # 이 토크나이저는 텍스트 데이터를 토큰화하는 역할을 합니다.\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","    # 사전 학습된 모델 로드:\n","    # BartForConditionalGeneration.from_pretrained를 사용하여 사전 학습된 BART 모델을 불러옵니다.\n","    # 이 모델은 텍스트 생성, 요약 등의 작업을 수행하는 데 사용됩니다.\n","    generate_model = BartForConditionalGeneration.from_pretrained(model_name, config=bart_config)\n","\n","\n","    # 특수 토큰 추가:\n","    # special_tokens_dict를 사용하여 설정에서 정의한 특수 토큰을 토크나이저에 추가합니다.\n","    # 이는 예를 들어, 특정 인물이나 장소를 나타내는 토큰을 추가하는 등의 작업에 사용됩니다.\n","    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n","    tokenizer.add_special_tokens(special_tokens_dict)\n","    # 토크나이저에 추가된 토큰에 맞추어 모델의 토큰 임베딩 크기를 조정합니다.\n","    # 모델의 토큰 임베딩 크기를 업데이트된 토크나이저의 크기에 맞게 조정합니다.\n","    generate_model.resize_token_embeddings(len(tokenizer))\n","\n","\n","    # 모델을 지정된 디바이스로 이동:\n","    # 모델을 device 변수에 지정된 디바이스(GPU 또는 CPU)로 이동시킵니다.\n","    # 이를 통해 모델이 올바른 디바이스에서 실행되도록 합니다.\n","    generate_model.to(device)\n","\n","\n","    # 모델 설정 출력:\n","    # 로드된 모델의 설정을 출력하여, 모델의 구조와 하이퍼파라미터 등을 확인할 수 있습니다.\n","    print(generate_model.config)\n","\n","    print('-' * 10, 'Load tokenizer & model complete', '-' * 10,)\n","\n","    # 모델과 토크나이저 반환:\n","    return generate_model, tokenizer\n","\n","    # 최종적으로 불러온 모델과 토크나이저를 반환하여, 이후 학습 과정에서 사용할 수 있도록 준비합니다.\n","    # 이 함수는 학습을 위해 필요한 모델과 토크나이저를 설정 파일에 따라 자동으로 불러오고 준비해 줍니다.\n","    # 특수 토큰의 추가 및 모델 임베딩의 재구성도 함께 처리하여, 모델이 주어진 작업에 최적화될 수 있도록 돕습니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gO_pSlYtcnQO","outputId":"d2c72ab0-6ac3-45c3-9354-efcd3194ce6c","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["da9e969636f94232a6f9829a7300a679","fbc2752fa0614afb8d182313f1ddd4d2","18215a3e8bc04f72affa677b58025dd2","f2bbd5455cbb4767a451f03a331e4616","865987da9e534001a2093d5b6a5e8b65","dc50187c270d466c88818ace007b8072","b86a100a5a3045b69dd94fbd5296453e","dcd5483fb5ff47c697d43363606a8e1f","5756de7b7a3f4488a177165b7442f282","d2f5a1c7587449ef857db0882d7de2fb","7e5c1960691542268486d53173d13ed3","9e0aca1b12b94b939b042bd789ade628","cb003be3aacb4bf8896441ce075c106f","c7651bba12cd472e8738f6fa36913a50","96de1db8c9a148cf98cae751f9f16fe4","0fa024f0c3bc4c6989cd2945a4db933a","dfdebbc1b35e4c30bddb21f06dbe7fa6","113686109e42464d95e18500121ae3c4","78a8271e111142d7b27f33b27051afde","7b9bb99ffba041a2b46fa0d960287271","f7eab16e1680470d9b60dbeb0ebd256c","61887e0ed1534c718af0757bbf4a4a9c"]},"executionInfo":{"status":"ok","timestamp":1725898518895,"user_tz":-540,"elapsed":5885,"user":{"displayName":"Donggun Lim","userId":"16228281816473625159"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","#######  사용할 모델과 토크나이저를 로드합니다.  ################################################################################\n","---------- Load tokenizer & model ----------\n","---------- Model Name : jx7789/kobart_summary_v3 ----------\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.82k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da9e969636f94232a6f9829a7300a679"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/496M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e0aca1b12b94b939b042bd789ade628"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["BartConfig {\n","  \"_name_or_path\": \"jx7789/kobart_summary_v3\",\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 1,\n","  \"classif_dropout\": 0.1,\n","  \"classifier_dropout\": 0.1,\n","  \"d_model\": 768,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 3072,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 6,\n","  \"decoder_start_token_id\": 1,\n","  \"do_blenderbot_90_layernorm\": false,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 3072,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 6,\n","  \"eos_token_id\": 1,\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": false,\n","  \"forced_eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"NEGATIVE\",\n","    \"1\": \"POSITIVE\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"kobart_version\": 2.0,\n","  \"label2id\": {\n","    \"NEGATIVE\": 0,\n","    \"POSITIVE\": 1\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 64,\n","  \"max_position_embeddings\": 1026,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 3,\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.44.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 30046\n","}\n","\n","---------- Load tokenizer & model complete ----------\n","---------- tokenizer special tokens :  {'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>', 'additional_special_tokens': ['#Person1#', '#Person2#', '#Person3#', '#Person4#', '#Person5#', '#Person6#', '#Person7#', '#SSN#', '#PhoneNumber#', '#Address#', '#Email#', '#CarNumber#', '#CardNumber##DateOfBirth#', '#PassportNumber#']} ----------\n"]}],"source":["# 사용할 모델과 토크나이저를 로드합니다.\n","print(\"\\n#######  사용할 모델과 토크나이저를 로드합니다.  ################################################################################\")\n","generate_model, tokenizer = load_tokenizer_and_model_for_train(config, device)\n","print('-' * 10, \"tokenizer special tokens : \", tokenizer.special_tokens_map, '-' * 10)"]},{"cell_type":"markdown","metadata":{"id":"HmZOblMBcnQP"},"source":["### 2) 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CyzhcScccnQP"},"outputs":[],"source":["\n","# 데이터 전처리를 위한 클래스로, 데이터셋을 데이터프레임으로 변환하고 인코더와 디코더의 입력을 생성합니다.\n","class Preprocess:\n","    def __init__(self,\n","            bos_token: str,\n","            eos_token: str,\n","        ) -> None:\n","\n","        # 클래스 초기화 메서드입니다. 시작 토큰(bos_token)과 종료 토큰(eos_token)을 인스턴스 변수로 저장합니다.\n","        self.bos_token = bos_token\n","        self.eos_token = eos_token\n","\n","    @staticmethod\n","    # 파일 경로를 입력 받아 CSV 파일을 읽고, 필요한 컬럼들만 선택하여 데이터프레임으로 반환합니다.\n","    def make_set_as_df(file_path, is_train = True):\n","\n","        # is_train이 True면 학습용 데이터로, False면 테스트용 데이터로 처리합니다.\n","        if is_train:\n","            df = pd.read_csv(file_path)\n","            train_df = df[['fname','dialogue','summary']]\n","            return train_df\n","\n","        # 테스트 데이터인 경우, 'fname'과 'dialogue' 컬럼만 선택하여 반환합니다.\n","        else:\n","            df = pd.read_csv(file_path)\n","            test_df = df[['fname','dialogue']]\n","            return test_df\n","\n","\n","    # BART 모델의 입력과 출력을 생성하는 메서드입니다.\n","    def make_input(self, dataset, is_test=False):\n","\n","        # is_test가 True면 테스트 데이터를 위한 입력만 생성하고, False면 학습 데이터를 위한 입력과 출력을 생성합니다.\n","        if is_test:\n","            # 테스트 데이터의 경우, 인코더 입력과 디코더의 시작 토큰으로만 구성된 입력을 반환합니다.\n","            # 인코더 입력으로 'dialogue' 텍스트를 사용합니다.\n","            encoder_input = dataset['dialogue']\n","            # 디코더 입력은 시작 토큰으로만 구성합니다.\n","            decoder_input = [self.bos_token] * len(dataset['dialogue'])\n","\n","            return encoder_input.tolist(), list(decoder_input)\n","        else:\n","            # 학습 데이터의 경우, 인코더 입력, 디코더 입력, 디코더 출력을 모두 생성합니다.\n","            encoder_input = dataset['dialogue']  # 인코더 입력으로 'dialogue' 텍스트를 사용합니다.\n","\n","            # 디코더 입력은 시작 토큰(bos_token)과 요약 텍스트(summary)로 구성됩니다.\n","            decoder_input = dataset['summary'].apply(lambda x: self.bos_token + str(x))\n","\n","            # 디코더 출력은 요약 텍스트(summary)와 종료 토큰(eos_token)으로 구성됩니다.\n","            decoder_output = dataset['summary'].apply(lambda x: str(x) + self.eos_token)\n","\n","            # 리스트로 변환하여 반환합니다.\n","            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RO_3kG_acnQP","outputId":"5f3f9b04-3b21-4086-cbab-59488725f1df","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725898537605,"user_tz":-540,"elapsed":494,"user":{"displayName":"Donggun Lim","userId":"16228281816473625159"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","#######  학습에 사용할 데이터셋을 전처리하고 로드합니다.  ########################################################################\n"]}],"source":["# 학습에 사용할 데이터셋을 전처리하고 로드합니다.\n","print(\"\\n#######  학습에 사용할 데이터셋을 전처리하고 로드합니다.  ########################################################################\")\n","# 시작 토큰(beginning of sentence)과 종료 토큰(end of sentence)을 설정합니다.\n","preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])\n"]},{"cell_type":"markdown","metadata":{"id":"wTDvBIm_cnQP"},"source":["### 3) 학습 및 검증 데이터셋 준비"]},{"cell_type":"markdown","metadata":{"id":"fHi4hrpJcnQP"},"source":["#### 3-1) Train, Validation, Test 클래스 정의"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"luv_ayA6cnQP"},"outputs":[],"source":["# Train에 사용되는 Dataset 클래스를 정의합니다.\n","class DatasetForTrain(Dataset):\n","    def __init__(self, encoder_input, decoder_input, labels, len):\n","        # 학습 데이터셋 초기화 메서드입니다. 인코더 입력, 디코더 입력, 레이블, 데이터 길이를 저장합니다.\n","        self.encoder_input = encoder_input\n","        self.decoder_input = decoder_input\n","        self.labels = labels\n","        self.len = len\n","\n","    def __getitem__(self, idx):\n","        # 주어진 인덱스(idx)에 해당하는 데이터를 반환하는 메서드입니다.\n","\n","        # 인코더 입력 데이터에서 해당 인덱스의 데이터를 복사하여 `item` 딕셔너리에 저장합니다.\n","        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}  # item[input_ids], item[attention_mask]\n","\n","        # 디코더 입력 데이터에서 해당 인덱스의 데이터를 복사하여 `item2` 딕셔너리에 저장합니다.\n","        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}  # item2[input_ids], item2[attention_mask]\n","\n","        # `item2` 딕셔너리의 'input_ids'와 'attention_mask'를 각각 'decoder_input_ids'와 'decoder_attention_mask'로 이름을 변경합니다.\n","        item2['decoder_input_ids'] = item2['input_ids']\n","        item2['decoder_attention_mask'] = item2['attention_mask']\n","        item2.pop('input_ids')  # 'input_ids' 키 제거\n","        item2.pop('attention_mask')  # 'attention_mask' 키 제거\n","\n","        # `item` 딕셔너리에 디코더의 입력 정보를 추가합니다.\n","        item.update(item2)  # item[input_ids], item[attention_mask], item[decoder_input_ids], item[decoder_attention_mask]\n","\n","        # 레이블로 사용할 'input_ids' 값을 `item` 딕셔너리에 추가합니다.\n","        item['labels'] = self.labels['input_ids'][idx]  # item[input_ids], item[attention_mask], item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n","\n","        return item\n","\n","    def __len__(self):\n","        # 데이터셋의 길이를 반환하는 메서드입니다.\n","        return self.len\n","\n","\n","# Validation에 사용되는 Dataset 클래스를 정의합니다.\n","class DatasetForVal(Dataset):\n","    def __init__(self, encoder_input, decoder_input, labels, len):\n","        # 검증 데이터셋 초기화 메서드입니다. 학습 데이터셋과 동일한 구조로 정의됩니다.\n","        self.encoder_input = encoder_input\n","        self.decoder_input = decoder_input\n","        self.labels = labels\n","        self.len = len\n","\n","    def __getitem__(self, idx):\n","        # 주어진 인덱스(idx)에 해당하는 데이터를 반환하는 메서드입니다.\n","        # 학습 데이터셋과 동일하게 정의됩니다.\n","        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()} # item[input_ids], item[attention_mask]\n","        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()} # item2[input_ids], item2[attention_mask]\n","        item2['decoder_input_ids'] = item2['input_ids']\n","        item2['decoder_attention_mask'] = item2['attention_mask']\n","        item2.pop('input_ids')\n","        item2.pop('attention_mask')\n","        item.update(item2) #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask]\n","        item['labels'] = self.labels['input_ids'][idx] #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n","        return item\n","\n","    def __len__(self):\n","        # 데이터셋의 길이를 반환하는 메서드입니다.\n","        return self.len\n","\n","\n","# Test에 사용되는 Dataset 클래스를 정의합니다.\n","class DatasetForInference(Dataset):     # inference뜻 : 추론\n","    def __init__(self, encoder_input, test_id, len):\n","        # 테스트 데이터셋 초기화 메서드입니다. 인코더 입력, 테스트 ID, 데이터 길이를 저장합니다.\n","        self.encoder_input = encoder_input\n","        self.test_id = test_id\n","        self.len = len\n","\n","    def __getitem__(self, idx):\n","        # 주어진 인덱스(idx)에 해당하는 데이터를 반환하는 메서드입니다.\n","        # 인코더 입력 데이터에서 해당 인덱스의 데이터를 복사하여 `item` 딕셔너리에 저장합니다.\n","        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n","        item['ID'] = self.test_id[idx]\n","        return item\n","\n","    def __len__(self):\n","        # 데이터셋의 길이를 반환하는 메서드입니다.\n","        return self.len\n"]},{"cell_type":"markdown","metadata":{"id":"WYT6sRxzcnQP"},"source":["#### 3-2) Train, Validation 데이터셋 준비"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gcsISQBMcnQQ"},"outputs":[],"source":["def prepare_train_dataset(config, preprocessor, data_path, tokenizer):\n","\n","    ### 데이터 로드 및 변환:\n","    #학습 및 검증 데이터 파일을 읽어 데이터프레임으로 변환합니다.\n","    #데이터프레임으로 변환된 데이터에서 대화(dialogue)와 요약(summary) 텍스트를 각각 학습 입력과 라벨로 사용합니다.\n","    # 데이터셋 경로를 지정합니다.\n","    train_file_path = os.path.join(data_path, train)  # 학습 데이터 파일 경로\n","    val_file_path = os.path.join(data_path, 'dev.csv')  # 검증 데이터 파일 경로\n","    # 학습(train)과 검증(validation) 데이터셋을 데이터프레임으로 변환합니다.\n","    train_data = preprocessor.make_set_as_df(train_file_path)  # 학습 데이터 로드\n","    val_data = preprocessor.make_set_as_df(val_file_path)  # 검증 데이터 로드\n","    # 로드된 학습 데이터와 라벨의 첫 번째 샘플을 출력합니다.\n","    print('-' * 150)\n","    print(f'train_data:\\n {train_data[\"dialogue\"][0]}')\n","    print(f'train_label:\\n {train_data[\"summary\"][0]}')\n","    # 로드된 검증 데이터와 라벨의 첫 번째 샘플을 출력합니다.\n","    print('-' * 150)\n","    print(f'val_data:\\n {val_data[\"dialogue\"][0]}')\n","    print(f'val_label:\\n {val_data[\"summary\"][0]}')\n","\n","\n","    ### 데이터 전처리:\n","    # Preprocess 클래스의 make_input 메서드를 사용하여 인코더 입력, 디코더 입력, 디코더 출력을 생성합니다.\n","    # 이 과정에서 BART 모델에 적합한 형태로 텍스트 데이터를 구성합니다.\n","    # 학습 데이터에 대해 인코더 입력, 디코더 입력, 디코더 출력을 생성합니다.\n","    encoder_input_train, decoder_input_train, decoder_output_train = preprocessor.make_input(train_data)\n","    # 검증 데이터에 대해 인코더 입력, 디코더 입력, 디코더 출력을 생성합니다.\n","    encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)\n","    print('-' * 10, 'Load data complete', '-' * 10)\n","\n","\n","    ### 토큰화:\n","    # 텍스트 데이터를 tokenizer를 사용하여 토큰화합니다. 토큰화 과정에서:\n","    # 텍스트를 토큰 ID로 변환합니다.\n","    # 필요한 경우, 패딩(padding=True), 최대 길이(max_length), 특수 토큰(add_special_tokens=True) 등을 추가합니다.\n","    # 반환된 데이터는 PyTorch 텐서(return_tensors=\"pt\")로 반환됩니다.\n","    # 학습 데이터의 인코더 입력을 토크나이저로 토큰화합니다.\n","    tokenized_encoder_inputs = tokenizer(\n","        encoder_input_train, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False\n","    )\n","    # 학습 데이터의 디코더 입력을 토크나이저로 토큰화합니다.\n","    tokenized_decoder_inputs = tokenizer(\n","        decoder_input_train, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False\n","    )\n","    # 학습 데이터의 디코더 출력을 토크나이저로 토큰화합니다.\n","    tokenized_decoder_ouputs = tokenizer(\n","        decoder_output_train, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False\n","    )\n","\n","\n","    ### 데이터셋 생성:\n","    # 학습 데이터와 검증 데이터를 각각 DatasetForTrain과 DatasetForVal 클래스로 감싸 데이터셋 객체를 생성합니다.\n","    # 이 객체들은 PyTorch의 DataLoader와 함께 사용되어 모델 학습 및 검증에 활용됩니다.\n","    # 학습용 데이터셋을 생성합니다.\n","    train_inputs_dataset = DatasetForTrain(\n","        tokenized_encoder_inputs, tokenized_decoder_inputs, tokenized_decoder_ouputs, len(encoder_input_train)\n","    )\n","    # 검증 데이터의 인코더 입력을 토크나이저로 토큰화합니다.\n","    val_tokenized_encoder_inputs = tokenizer(\n","        encoder_input_val, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False\n","    )\n","    # 검증 데이터의 디코더 입력을 토크나이저로 토큰화합니다.\n","    val_tokenized_decoder_inputs = tokenizer(\n","        decoder_input_val, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False\n","    )\n","    # 검증 데이터의 디코더 출력을 토크나이저로 토큰화합니다.\n","    val_tokenized_decoder_ouputs = tokenizer(\n","        decoder_output_val, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False\n","    )\n","    # 검증용 데이터셋을 생성합니다.\n","    val_inputs_dataset = DatasetForVal(\n","        val_tokenized_encoder_inputs, val_tokenized_decoder_inputs, val_tokenized_decoder_ouputs, len(encoder_input_val)\n","    )\n","    print('-' * 10, 'Make dataset complete', '-' * 10)\n","\n","\n","    ### 출력 및 반환:\n","    # 학습용 데이터셋과 검증용 데이터셋을 반환합니다. 이를 통해 모델 학습 과정에서 필요한 데이터를 제공하게 됩니다.\n","    return train_inputs_dataset, val_inputs_dataset\n","\n","\n","    # 이 함수는 모델 학습 및 검증을 위한 데이터 준비 과정에서 필요한\n","    # 모든 전처리, 토큰화, 데이터셋 생성을 자동으로 처리하여, 최종적으로 Dataset 객체를 반환합니다.\n","    # 이를 통해 모델 학습 및 검증을 효율적으로 수행할 수 있습니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UJDUs7RucnQQ","outputId":"5a877cb9-2032-4ba3-b41f-44ed41c23cac","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725898620093,"user_tz":-540,"elapsed":33869,"user":{"displayName":"Donggun Lim","userId":"16228281816473625159"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","#######  학습 및 검증 데이터셋을 준비합니다.  ###################################################################################\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","train_data:\n"," #Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\n","#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\n","#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\n","#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\n","#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\n","#Person2#: 알겠습니다.\n","#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\n","#Person2#: 네.\n","#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \n","#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\n","#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\n","#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n","train_label:\n"," 스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니다. 호킨스 의사는 스미스씨가 담배를 끊는 데 도움이 될 수 있는 수업과 약물에 대한 정보를 제공할 것입니다.\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","val_data:\n"," #Person1#: 안녕하세요, 오늘 하루 어떠셨어요? \n","#Person2#: 요즘 숨쉬기가 좀 힘들어요.\n","#Person1#: 최근에 감기 같은 것에 걸리신 적이 있나요?\n","#Person2#: 아니요, 감기는 아니에요. 그냥 숨을 쉴 때마다 가슴이 무겁게 느껴져요.\n","#Person1#: 알고 있는 알레르기가 있나요?\n","#Person2#: 아니요, 알고 있는 알레르기는 없어요.\n","#Person1#: 이런 증상이 항상 나타나나요, 아니면 활동할 때 주로 나타나나요?\n","#Person2#: 운동을 할 때 많이 나타나요.\n","#Person1#: 저는 당신을 폐 전문의에게 보내서 천식에 대한 검사를 받게 할 거예요.\n","#Person2#: 도와주셔서 감사합니다, 의사 선생님.\n","val_label:\n"," #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다. \n","---------- Load data complete ----------\n","---------- Make dataset complete ----------\n"]}],"source":["print(\"\\n#######  학습 및 검증 데이터셋을 준비합니다.  ###################################################################################\")\n","# 학습 및 검증 데이터셋을 준비합니다.\n","data_path = config['general']['data_path']  # 데이터 경로를 설정에서 가져옵니다.\n","train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config, preprocessor, data_path, tokenizer)\n"]},{"cell_type":"markdown","metadata":{"id":"zwquaibscnQQ"},"source":["### 4) Trainer 클래스 초기화"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jNjWMbbpcnQQ"},"outputs":[],"source":["def compute_metrics(config, tokenizer, pred):\n","    ### rouge = Rouge():\n","    # ROUGE 점수를 계산하기 위해 Rouge 클래스를 초기화합니다.\n","    # ROUGE는 주로 텍스트 요약의 품질을 평가할 때 사용되는 지표입니다.\n","    rouge = Rouge()\n","\n","\n","    ### pred.predictions 및 pred.label_ids:\n","    # predictions: 모델이 예측한 토큰 ID의 배열입니다.\n","    # label_ids: 실제 레이블(정답) 토큰 ID의 배열입니다.\n","    # 예측된 토큰 ID와 실제 레이블 ID를 가져옵니다.\n","    predictions = pred.predictions\n","    labels = pred.label_ids\n","\n","\n","    ### 패딩 토큰 처리:\n","    # 예측 값과 레이블에서 -100으로 표시된 패딩 토큰을 실제 패딩 토큰 ID로 교체하여 평가에서 패딩이 영향을 미치지 않도록 합니다.\n","    # 모델 출력 중 패딩 토큰을 의미하는 -100 값을 tokenizer의 패딩 토큰 ID로 변경합니다.\n","    predictions[predictions == -100] = tokenizer.pad_token_id\n","    labels[labels == -100] = tokenizer.pad_token_id\n","\n","\n","    ### 토큰 디코딩:\n","    # 토큰 ID 배열을 원래의 텍스트 문자열로 변환합니다.\n","    # batch_decode는 여러 개의 토큰 배열을 한꺼번에 디코딩합니다.\n","    # 예측된 토큰 ID를 텍스트로 디코딩합니다.\n","    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)\n","    # 실제 레이블의 토큰 ID도 텍스트로 디코딩합니다.\n","    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n","\n","\n","    ### 불필요한 토큰 제거:\n","    # 모델이 생성한 텍스트에서 사전에 정의된 불필요한 토큰을 제거하여 평가의 정확성을 높입니다.\n","    # 평가를 위해 불필요한 토큰들을 제거합니다.\n","    replaced_predictions = decoded_preds.copy()  # 예측된 텍스트 복사\n","    replaced_labels = labels.copy()  # 실제 레이블 텍스트 복사\n","    remove_tokens = config['inference']['remove_tokens']  # 제거할 토큰 목록을 config에서 가져옵니다.\n","    # 각 불필요한 토큰을 제거합니다.\n","    for token in remove_tokens:\n","        replaced_predictions = [sentence.replace(token, \" \") for sentence in replaced_predictions]\n","        replaced_labels = [sentence.replace(token, \" \") for sentence in replaced_labels]\n","\n","\n","    ### 출력:\n","    # 평가를 위해 디코딩된 예측 텍스트와 실제 레이블의 일부 샘플을 출력합니다.\n","    # 첫 번째, 두 번째, 세 번째 예측과 실제 레이블을 출력합니다.\n","    print('-' * 150)\n","    print(f\"PRED: {replaced_predictions[0]}\")\n","    print(f\"GOLD: {replaced_labels[0]}\")\n","    print('-' * 150)\n","    print(f\"PRED: {replaced_predictions[1]}\")\n","    print(f\"GOLD: {replaced_labels[1]}\")\n","    print('-' * 150)\n","    print(f\"PRED: {replaced_predictions[2]}\")\n","    print(f\"GOLD: {replaced_labels[2]}\")\n","\n","\n","    ### ROUGE 점수 계산:\n","    # replaced_predictions와 replaced_labels를 사용하여 ROUGE 점수를 계산합니다.\n","    # ROUGE-1, ROUGE-2, ROUGE-L 등의 F1 점수를 계산하여 반환합니다.\n","    # 최종적으로 ROUGE 점수를 계산합니다.\n","    results = rouge.get_scores(replaced_predictions, replaced_labels, avg=True)\n","\n","\n","    ### 결과 반환:\n","    # ROUGE 점수 중 F1-score를 추출하여 딕셔너리 형태로 반환합니다.\n","    result = {key: value[\"f\"] for key, value in results.items()}\n","    return result\n","\n","\n","    # 이 함수는 모델이 생성한 텍스트의 품질을 ROUGE 지표로 평가하여,\n","    # 모델 성능을 평가하는 데 중요한 역할을 합니다.\n","    # ROUGE 점수를 통해 텍스트 요약 또는 생성 모델의 정확성을 평가할 수 있습니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vz9-ZOqucnQQ"},"outputs":[],"source":["def load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset):\n","    print('-' * 10, 'Make training arguments', '-' * 10,)\n","\n","    ### Seq2SeqTrainingArguments 설정:\n","    # 학습과 관련된 다양한 설정값들을 정의하는 Seq2SeqTrainingArguments 객체를 생성합니다.\n","    # 학습률, 배치 크기, 에포크 수, 로그 저장 위치 등 다양한 하이퍼파라미터와 옵션들이 포함됩니다.\n","    # 학습을 위한 설정값들을 정의합니다.\n","    training_args = Seq2SeqTrainingArguments(\n","        output_dir=config['general']['output_dir'],                                     # 모델 출력 디렉터리\n","        overwrite_output_dir=config['training']['overwrite_output_dir'],                # 출력 디렉터리를 덮어쓸지 여부\n","        num_train_epochs=config['training']['num_train_epochs'],                        # 전체 학습 에포크 수\n","        learning_rate=config['training']['learning_rate'],                              # 학습률\n","        per_device_train_batch_size=config['training']['per_device_train_batch_size'],  # 학습 시 디바이스당 배치 크기\n","        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],    # 평가 시 디바이스당 배치 크기\n","        warmup_ratio=config['training']['warmup_ratio'],                                # 학습 초기에 학습률을 점진적으로 증가시키는 비율\n","        weight_decay=config['training']['weight_decay'],                                # 가중치 감쇠 (과적합 방지)\n","        lr_scheduler_type=config['training']['lr_scheduler_type'],                      # 학습률 스케줄러 유형\n","        optim=config['training']['optim'],                                              # 옵티마이저 종류\n","        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],  # 기울기(gradient) 축적 단계 수\n","        evaluation_strategy=config['training']['evaluation_strategy'],                  # 학습 중 평가 전략 (예: 에포크마다 평가)\n","        save_strategy=config['training']['save_strategy'],                              # 모델 저장 전략\n","        save_total_limit=config['training']['save_total_limit'],                        # 저장할 체크포인트의 최대 개수\n","        fp16=config['training']['fp16'],                                                # 반정밀도(float16) 연산 사용 여부\n","        load_best_model_at_end=config['training']['load_best_model_at_end'],            # 학습 종료 시 가장 좋은 모델 로드\n","        seed=config['training']['seed'],                                                # 랜덤 시드 설정\n","        logging_dir=config['training']['logging_dir'],                                  # 로그 저장 디렉터리\n","        logging_strategy=config['training']['logging_strategy'],                        # 로그 기록 전략 (예: 에포크마다 기록)\n","        predict_with_generate=config['training']['predict_with_generate'],              # 텍스트 생성 후 평가 지표를 계산할지 여부\n","        generation_max_length=config['training']['generation_max_length'],              # 텍스트 생성 시 최대 길이\n","        do_train=config['training']['do_train'],                                        # 학습 수행 여부\n","        do_eval=config['training']['do_eval'],                                          # 평가 수행 여부\n","        report_to=config['training']['report_to']                                       # (선택 사항) wandb로 학습 과정을 기록할지 여부\n","    )\n","\n","\n","    ### wandb 초기화 (선택 사항):\n","    # (선택 사항) wandb를 사용하여 학습 과정을 추적할 때 초기화합니다.\n","    # WandB(Weights & Biases)로 학습 과정을 추적하고 시각화하려면 wandb.init()을 사용해 초기화할 수 있습니다.\n","    # 이 부분은 현재 주석 처리되어 있으며, 필요한 경우 활성화할 수 있습니다.\n","    wandb.init(\n","         entity=config['wandb']['entity'],\n","         project=config['wandb']['project'],\n","         name=config['wandb']['name']\n","    )\n","    # (선택 사항) 모델 체크포인트를 wandb에 저장하도록 환경 변수를 설정합니다.\n","    os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n","    os.environ[\"WANDB_WATCH\"] = \"false\"\n","\n","\n","    ### EarlyStoppingCallback 설정:\n","    # EarlyStoppingCallback을 사용하여 학습이 진행될 때 검증 손실이 더 이상 개선되지 않으면 학습을 조기에 중단시킵니다.\n","    # 설정된 early_stopping_patience와 early_stopping_threshold에 따라 작동합니다.\n","    # EarlyStoppingCallback: 검증 손실이 더 이상 개선되지 않을 때 학습을 중단시키는 콜백을 설정합니다.\n","    MyCallback = EarlyStoppingCallback(\n","        early_stopping_patience=config['training']['early_stopping_patience'],      # 개선이 없을 경우 중단까지 기다릴 에포크 수\n","        early_stopping_threshold=config['training']['early_stopping_threshold']     # 개선으로 간주할 최소 손실 감소량\n","    )\n","    print('-' * 10, 'Make training arguments complete', '-' * 10,)\n","    print('-' * 10, 'Make trainer', '-' * 10,)\n","\n","\n","    ### Seq2SeqTrainer 초기화:\n","    # Seq2SeqTrainer는 Hugging Face의 transformers 라이브러리에서 제공하는 훈련 도구로,\n","    # 시퀀스-투-시퀀스 모델의 학습과 평가를 위한 도구입니다.\n","    # 학습할 모델, 설정값, 학습/검증 데이터셋, 평가 메트릭 함수, 콜백 등을 인자로 받아 초기화합니다.\n","    trainer = Seq2SeqTrainer(\n","        model=generate_model,  # 학습할 모델\n","        args=training_args,  # 학습 설정값\n","        train_dataset=train_inputs_dataset,  # 학습 데이터셋\n","        eval_dataset=val_inputs_dataset,  # 검증 데이터셋\n","        compute_metrics=lambda pred: compute_metrics(config, tokenizer, pred),  # 성능 평가를 위한 메트릭 함수\n","        callbacks=[MyCallback]  # EarlyStoppingCallback을 포함한 콜백 리스트\n","    )\n","    print('-' * 10, 'Make trainer complete', '-' * 10,)\n","\n","    ### Trainer 객체 반환:\n","    # 생성된 trainer 객체를 반환하여, 이후 학습을 진행할 수 있게 합니다.\n","    return trainer\n","\n","\n","    # 이 함수는 모델 학습을 시작하기 위한 모든 준비 작업을 자동으로 처리하며,\n","    # 사용자에게 최적화된 Trainer 객체를 제공합니다. 이를 통해 학습 및 평가를 손쉽게 수행할 수 있습니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gyWFKzH9cnQQ","outputId":"ebcc954a-53fb-4254-89ab-d143f97eac2f","colab":{"base_uri":"https://localhost:8080/","height":327},"executionInfo":{"status":"ok","timestamp":1725898623687,"user_tz":-540,"elapsed":3611,"user":{"displayName":"Donggun Lim","userId":"16228281816473625159"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkkukky\u001b[0m (\u001b[33mkkukky-empty\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"stream","name":"stdout","text":["\n","#######  학습을 위한 Trainer 클래스를 초기화합니다.  ############################################################################\n","---------- Make training arguments ----------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.17.9"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20240909_161700-c5podi0a</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/kkukky-empty/NLP2/runs/c5podi0a' target=\"_blank\">jx7789_kobart_summary_v33x5075p-l6-n4-b5</a></strong> to <a href='https://wandb.ai/kkukky-empty/NLP2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/kkukky-empty/NLP2' target=\"_blank\">https://wandb.ai/kkukky-empty/NLP2</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/kkukky-empty/NLP2/runs/c5podi0a' target=\"_blank\">https://wandb.ai/kkukky-empty/NLP2/runs/c5podi0a</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["---------- Make training arguments complete ----------\n","---------- Make trainer ----------\n","---------- Make trainer complete ----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"]}],"source":["print(\"\\n#######  학습을 위한 Trainer 클래스를 초기화합니다.  ############################################################################\")\n","# 학습을 위한 Trainer 클래스를 초기화합니다.\n","trainer = load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset)"]},{"cell_type":"markdown","metadata":{"id":"wPCJSYOIcnQR"},"source":["### 5) 모델 학습, wandb 세션종료"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VjglWKuKcnQR","outputId":"997c9f16-0ba8-4176-bd94-6344bfaca925","colab":{"base_uri":"https://localhost:8080/","height":1000}},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","#######  모델 학습을 시작합니다.  ##############################################################################################\n","jx7789_kobart_summary_v33x5075p-l6-n4-b5\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2521' max='46720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 2521/46720 20:14 < 5:55:08, 2.07 it/s, Epoch 1.08/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge-1</th>\n","      <th>Rouge-2</th>\n","      <th>Rouge-l</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>5.969200</td>\n","      <td>2.295457</td>\n","      <td>0.183847</td>\n","      <td>0.033186</td>\n","      <td>0.179350</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person1#은 숨을 쉴 때마다 가슴이 무겁게 느껴져 숨쉬기가 힘들다고 말한다. #Peron2#는 숨 쉬기가 힘들다는 것을 알고 있는 알레르기는 없다고 한다. #Pesson2 #는 숨 쉴 때마다 숨이 무겁다고 느낀다.                                                                                                                                            \n","GOLD: #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다.                                                                             \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person1#은 헤이, 헤이, 지미 때문에 다리와 팔목을 운동하러 가자고 제안한다. 3시 30분에 헬스장에서 만나기로 한다.                                                                                                                                                                      \n","GOLD: #Person1#은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                                              \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person1#은 건강에 해로운 음식을 먹는 것을 멈춰야 한다고 말하지만 #Peron2#는 더 건강한 음식을 먹기 시작했다고 말한다. #Pesson1 #은 어떤 음식을 먹는지에 대해 이야기한다.#Perserson2 #는 주로 과일, 채소, 그리고 닭고기를 먹는다고 한다. #P   <unused47>  <unused72>  <unused71>  <unused15>  <unused13>  <unused59>  의 #P 동원해  <unused40>  <unused70>  <unused92>  <unused14>  <unused16>  <unused5>  <unused56>  <unused79>  경찰서는경찰서는경찰서는은 #Pre  <unused68>  <unused10>     <unused95>  <unused57>  <unused1>  #@URL#은은 ##P <unused47>경찰서는경찰서는  <unused46>  <unused76>  <unused51>  <unused37>  순은 닭고기는 구워서 먹으면 정말 건강에 좋다고 말한다.              \n","GOLD: #Person1#은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2#는 자신의 건강한 레시피를 #Person1#와 공유한다.                                                                              \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 5, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 1}\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/checkpoint-2336)... Done. 6.1s\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='28753' max='46720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [28753/46720 3:37:38 < 2:16:00, 2.20 it/s, Epoch 12.31/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge-1</th>\n","      <th>Rouge-2</th>\n","      <th>Rouge-l</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>5.969200</td>\n","      <td>2.295457</td>\n","      <td>0.183847</td>\n","      <td>0.033186</td>\n","      <td>0.179350</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.037400</td>\n","      <td>0.492957</td>\n","      <td>0.237283</td>\n","      <td>0.054335</td>\n","      <td>0.231195</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.305900</td>\n","      <td>0.451818</td>\n","      <td>0.247633</td>\n","      <td>0.058571</td>\n","      <td>0.242174</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.285100</td>\n","      <td>0.441203</td>\n","      <td>0.255987</td>\n","      <td>0.064791</td>\n","      <td>0.249340</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.273900</td>\n","      <td>0.434441</td>\n","      <td>0.265375</td>\n","      <td>0.069751</td>\n","      <td>0.259450</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.265600</td>\n","      <td>0.429413</td>\n","      <td>0.263306</td>\n","      <td>0.068517</td>\n","      <td>0.256801</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.258800</td>\n","      <td>0.425665</td>\n","      <td>0.263740</td>\n","      <td>0.071198</td>\n","      <td>0.257872</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.252900</td>\n","      <td>0.423262</td>\n","      <td>0.264472</td>\n","      <td>0.070836</td>\n","      <td>0.258717</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.247900</td>\n","      <td>0.421814</td>\n","      <td>0.266024</td>\n","      <td>0.070032</td>\n","      <td>0.259737</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.243700</td>\n","      <td>0.420427</td>\n","      <td>0.265206</td>\n","      <td>0.071927</td>\n","      <td>0.258978</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.240300</td>\n","      <td>0.419486</td>\n","      <td>0.269735</td>\n","      <td>0.073796</td>\n","      <td>0.263741</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.237200</td>\n","      <td>0.418968</td>\n","      <td>0.267792</td>\n","      <td>0.072880</td>\n","      <td>0.262098</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person1#은 숨을 쉴 때마다 가슴이 무겁고 숨쉬기가 힘들다고 말하고, 폐 전문의에게 천식에 대한 검사를 받게 할 예정입니다.                                                      \n","GOLD: #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다.                                                                             \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person1#은 지미에게 헬스장에서 만나자고 제안한다. # Person2#는 지미가 금요일에 다리를 할 수 있다고 말한다.                                                     \n","GOLD: #Person1#은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                                              \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person1#은 건강에 해로운 음식을 먹는 것을 멈춰야 한다고 생각하고, #Peron2#는 과일과 채소를 먹는다고 말한다.                                                      \n","GOLD: #Person1#은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2#는 자신의 건강한 레시피를 #Person1#와 공유한다.                                                                              \n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 5, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 1}\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/checkpoint-4672)... Done. 7.1s\n"]},{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person1#은 숨쉬기가 힘들고 감기는 아니지만 숨을 쉴 때마다 가슴이 무거워진다고 말합니다. 그들은 천식에 대한 검사를 받기로 결정했습니다.                                                                  \n","GOLD: #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다.                                                                             \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person1#은 지미에게 운동하러 가자고 제안하고, 지미는 헬스장에서 만나자고 제안한다.                                                                            \n","GOLD: #Person1#은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                                              \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 건강에 해로운 음식을 먹는 것을 멈추고 과일과 채소, 닭고기를 먹는다.                                                                             \n","GOLD: #Person1#은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2#는 자신의 건강한 레시피를 #Person1#와 공유한다.                                                                              \n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 5, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 1}\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/checkpoint-7008)... Done. 6.8s\n"]},{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 숨쉬기가 힘들고 감기는 아니지만 숨을 쉴 때마다 가슴이 무거워진다고 말합니다. 의사 선생님은 폐 전문의에게 천식 검사를 받게 할 것입니다.                                                       \n","GOLD: #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다.                                                                             \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person1#은 지미에게 운동하러 가자고 제안하고, 지미는 금요일에 다리와 팔목을 운동하자고 제안한다.                                                                  \n","GOLD: #Person1#은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                                              \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 건강에 해로운 음식을 먹는 것을 멈추고 과일, 채소, 닭고기를 먹는다.                                                                      \n","GOLD: #Person1#은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2#는 자신의 건강한 레시피를 #Person1#와 공유한다.                                                                              \n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 5, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 1}\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/checkpoint-9344)... Done. 6.8s\n"]},{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 숨을 쉴 때마다 가슴이 무겁고 숨쉬기가 힘들어서 천식에 대한 검사를 받으려고 합니다. 의사 선생님은 폐 전문의에게 검사를 받게 할 것입니다.                                                \n","GOLD: #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다.                                                                             \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person1#은 지미에게 운동하러 가자고 제안한다. 지미는 금요일에 다리를 할 수 있다고 동의한다.                                                           \n","GOLD: #Person1#은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                                              \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 건강에 해로운 음식을 먹는 것을 멈추고 과일, 채소, 닭고기를 먹는다.                                                             \n","GOLD: #Person1#은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2#는 자신의 건강한 레시피를 #Person1#와 공유한다.                                                                              \n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 5, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 1}\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/checkpoint-11680)... Done. 5.9s\n"]},{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 숨을 쉴 때마다 가슴이 무겁고 숨쉬기가 힘들어서 천식에 대한 검사를 받으려고 합니다.                                                                    \n","GOLD: #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다.                                                                             \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person1#은 지미에게 운동하러 가자고 제안하고, 지미는 동의한다. 그들은 금요일에 다리를 할 수 있다.                                                                   \n","GOLD: #Person1#은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                                              \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 건강에 해로운 음식을 먹는 것을 멈추고 과일, 채소, 닭고기를 먹는다.                                                                      \n","GOLD: #Person1#은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2#는 자신의 건강한 레시피를 #Person1#와 공유한다.                                                                              \n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 5, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 1}\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/checkpoint-14016)... Done. 7.6s\n"]},{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 숨을 쉴 때마다 가슴이 무겁고 숨쉬기가 힘들다고 말합니다. 의사 선생님은 이를 폐 전문의에게 검사를 받게 할 것입니다.                                                            \n","GOLD: #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다.                                                                             \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person1#은 지미에게 운동하러 가자고 제안한다. 지미는 동의하고 헬스장에서 만나기로 결정한다.                                                                  \n","GOLD: #Person1#은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                                              \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 건강에 해로운 음식을 먹는 것을 멈추고 과일, 채소, 그리고 닭고기를 먹는다.                                                                    \n","GOLD: #Person1#은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2#는 자신의 건강한 레시피를 #Person1#와 공유한다.                                                                              \n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 5, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 1}\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/checkpoint-16352)... Done. 7.2s\n"]},{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 숨을 쉴 때마다 가슴이 무겁고 숨쉬기가 힘들다고 말합니다. 의사 선생님은 이를 폐 전문의에게 검사를 받게 할 것입니다.                                                                            \n","GOLD: #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다.                                                                             \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person1#은 지미에게 운동하러 가자고 제안한다. 지미는 동의하고 헬스장에서 만나기로 결정한다.                                                                                  \n","GOLD: #Person1#은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                                              \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 건강에 해로운 음식을 먹는 것을 멈추고 과일, 채소, 그리고 닭고기를 먹는다.                                                                                    \n","GOLD: #Person1#은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2#는 자신의 건강한 레시피를 #Person1#와 공유한다.                                                                              \n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 5, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 1}\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/checkpoint-18688)... Done. 6.1s\n"]},{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 숨을 쉴 때마다 가슴이 무겁고 숨쉬기가 힘들다고 말합니다. 의사 선생님은 천식에 대한 검사를 받게 할 예정입니다.                                                                \n","GOLD: #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다.                                                                             \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   지미는 #Person1#에게 헬스장에서 운동하자고 제안한다. # Person2#는 주간 스케줄을 따르고 있기 때문에 두 날을 바꾸는 것을 제안한다.                                                            \n","GOLD: #Person1#은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                                              \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 건강에 해로운 음식을 먹는 것을 멈추고 과일, 채소, 그리고 닭고기를 먹는다.                                                                       \n","GOLD: #Person1#은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2#는 자신의 건강한 레시피를 #Person1#와 공유한다.                                                                              \n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 5, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 1}\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/checkpoint-21024)... Done. 9.1s\n"]},{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 숨을 쉴 때마다 가슴이 무겁고 숨쉬기가 힘들다고 말합니다. 의사는 천식에 대한 검사를 받게 할 예정입니다.                                                                   \n","GOLD: #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다.                                                                             \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   지미는 #Person1#에게 헬스장에서 운동하자고 제안한다. # Person2#는 주간 스케줄을 따르고 있기 때문에 두 날을 바꾸는 것을 제안한다.                                                              \n","GOLD: #Person1#은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                                              \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 건강에 해로운 음식을 먹는 것을 멈추고 과일, 채소, 그리고 닭고기를 먹는다.                                                                         \n","GOLD: #Person1#은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2#는 자신의 건강한 레시피를 #Person1#와 공유한다.                                                                              \n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 5, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 1}\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/checkpoint-23360)... Done. 6.4s\n"]},{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 숨을 쉴 때마다 가슴이 무겁고 숨쉬기가 힘들다고 말합니다. 의사는 천식에 대한 검사를 받게 할 예정입니다.                                                                 \n","GOLD: #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다.                                                                             \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   지미는 #Person1#에게 헬스장에서 운동하자고 제안한다. # Person2#는 주간 스케줄을 따르고 있기 때문에 두 날을 바꾸기로 한다.                                                             \n","GOLD: #Person1#은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                                              \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 건강에 해로운 음식을 먹는 것을 멈추고 과일, 채소, 그리고 닭고기를 먹는다.                                                                       \n","GOLD: #Person1#은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2#는 자신의 건강한 레시피를 #Person1#와 공유한다.                                                                              \n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 5, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 1}\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/checkpoint-25696)... Done. 6.8s\n"]},{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 숨을 쉴 때마다 가슴이 무겁고 숨쉬기가 힘들다고 말합니다. 의사는 천식에 대한 검사를 받게 할 예정입니다.                                                                           \n","GOLD: #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다.                                                                             \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   지미는 #Person1#에게 운동하러 가자고 제안한다. 그들은 금요일에 다리와 팔목을 운동하기로 결정한다.                                                                                \n","GOLD: #Person1#은 지미에게 운동하러 가자고 제안하고 팔과 배를 운동하도록 설득한다.                                                                                              \n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","PRED:   #Person2#는 건강에 해로운 음식을 먹는 것을 멈추고 과일, 채소, 그리고 닭고기를 먹는다.                                                                                 \n","GOLD: #Person1#은 건강에 해로운 음식을 먹는 것을 멈추려는 계획을 세우고, #Person2#는 자신의 건강한 레시피를 #Person1#와 공유한다.                                                                              \n"]},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 5, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 1}\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/checkpoint-28032)... Done. 6.7s\n"]}],"source":["print(\"\\n#######  모델 학습을 시작합니다.  ##############################################################################################\")\n","print(title)\n","# 모델 학습을 시작합니다.\n","trainer.train()\n","\n","print(\"\\n#######  wandb 세션을 종료합니다.  ############################################################################################\")\n","# (선택) 모델 학습이 완료된 후 wandb 세션을 종료합니다.\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"4UueHxdCcnQR"},"source":["## 2. 모델 추론하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"epeYOR32cnQR"},"outputs":[],"source":["# 이곳에 내가 사용할 wandb config 설정 \"추론에 사용할 ckt 경로 설정\"\n","# loaded_config['inference']['ckt_path'] = \"home/ckt\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hsIVVQ3fcnQR"},"outputs":[],"source":["# 사용할 device를 정의합니다. GPU가 사용 가능하면 'cuda:0', 그렇지 않으면 'cpu'를 사용합니다.\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('-' * 10, f'device : {device}', '-' * 10,)\n","print(torch.__version__)"]},{"cell_type":"markdown","metadata":{"id":"NQCHEQFDcnQR"},"source":["### 1) 모델과 토크나이저 불러오기"]},{"cell_type":"markdown","metadata":{"id":"Vt9O9DW4cnQR"},"source":["<font color=red size=32><b> ckt 값 바꾸기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1L2CUP0cnQV"},"outputs":[],"source":["멈춤"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-KSQSg4cnQV"},"outputs":[],"source":["checkpoint_num = '20254'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MAeC8Ny1cnQV"},"outputs":[],"source":["# 추론을 위한 tokenizer와 학습시킨 모델을 불러옵니다.\n","def load_tokenizer_and_model_for_test(config, device):\n","    print('-' * 10, 'Load tokenizer & model', '-' * 10,)\n","\n","    print(\"\\n######  설정 파일에서 모델 이름과 체크포인트 경로를 불러옵니다.\")\n","    ### 모델 이름과 체크포인트 경로 설정:\n","    # 설정 파일에서 모델 이름과 체크포인트 경로를 불러옵니다.\n","    # config['general']['model_name']: 사전 학습된 모델의 이름을 설정 파일에서 가져옵니다.\n","    # config['inference']['ckt_path']: 학습된 모델의 체크포인트 경로를 설정 파일에서 가져옵니다. 이 경로는 학습이 완료된 후 저장된 모델의 위치를 나타냅니다.\n","    model_name = config['general']['model_name']\n","    ckt_path = config['inference']['ckt_path']  # 학습된 모델의 체크포인트 경로\n","    ckt_path = \"/home/code/checkpoint-\" + checkpoint_num\n","    print(model_name, \" / \", ckt_path)\n","    print('-' * 10, f'Model Name : {model_name}', '-' * 10,)\n","\n","\n","    ### 토크나이저 로드 및 특수 토큰 추가:\n","    # AutoTokenizer.from_pretrained(model_name): 사전 학습된 모델 이름을 사용하여 토크나이저를 로드합니다.\n","    # tokenizer.add_special_tokens(special_tokens_dict): 설정 파일에서 정의된 특수 토큰을 토크나이저에 추가합니다. 이는 모델이 특정 단어들을 특수 토큰으로 처리할 수 있도록 합니다.\n","    print(\"\\n######  지정된 모델 이름으로부터 토크나이저를 로드합니다.\")\n","    # 지정된 모델 이름으로부터 토크나이저를 로드합니다.\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    print(\"\\n######  설정 파일에서 정의된 특수 토큰을 토크나이저에 추가합니다.\")\n","    # 설정 파일에서 정의된 특수 토큰을 토크나이저에 추가합니다.\n","    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n","    tokenizer.add_special_tokens(special_tokens_dict)\n","\n","\n","    ### 모델 로드:\n","    # BartForConditionalGeneration.from_pretrained(ckt_path): 학습된 모델의 체크포인트에서 BART 모델을 로드합니다.\n","    # generate_model.resize_token_embeddings(len(tokenizer)): 토크나이저에 추가된 특수 토큰에 맞춰 모델의 토큰 임베딩 크기를 재조정합니다.\n","    print(\"\\n######  학습된 체크포인트에서 BART 모델을 로드합니다.\")\n","    # 학습된 체크포인트에서 BART 모델을 로드합니다.\n","    generate_model = BartForConditionalGeneration.from_pretrained(ckt_path)\n","    print(\"\\n######  추가된 토큰에 맞게 모델의 토큰 임베딩 크기를 조정합니다.\")\n","    # 추가된 토큰에 맞게 모델의 토큰 임베딩 크기를 조정합니다.\n","    generate_model.resize_token_embeddings(len(tokenizer))\n","\n","\n","    ### 디바이스로 모델 이동:\n","    # generate_model.to(device): 로드된 모델을 지정된 디바이스(GPU 또는 CPU)로 이동시켜,\n","    # 추론 작업을 해당 디바이스에서 수행할 수 있도록 합니다.\n","    print(\"\\n######  모델을 지정된 디바이스(GPU 또는 CPU)로 이동시킵니다.\")\n","    # 모델을 지정된 디바이스(GPU 또는 CPU)로 이동시킵니다.\n","    generate_model.to(device)\n","\n","    print('-' * 10, 'Load tokenizer & model complete', '-' * 10,)\n","\n","\n","    ### 모델과 토크나이저 반환:\n","    # 최종적으로 불러온 모델과 토크나이저를 반환하여, 이후 추론 작업에서 사용할 수 있도록 합니다.\n","    return generate_model, tokenizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZ6xQn-fcnQV"},"outputs":[],"source":["print(\"\\n#######  추론을 위한 모델과 토크나이저를 불러옵니다.  ##############################################################################\")\n","# 추론을 위한 모델과 토크나이저를 불러옵니다.\n","generate_model, tokenizer = load_tokenizer_and_model_for_test(config, device)"]},{"cell_type":"markdown","metadata":{"id":"S9SLt-GTcnQV"},"source":["### 2) 데이터 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6td9NZpcnQW"},"outputs":[],"source":["print(\"\\n#######  데이터 경로와 전처리기를 설정합니다.  ####################################################################################\")\n","# 데이터 경로와 전처리기를 설정합니다.\n","data_path = config['general']['data_path']\n","preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])"]},{"cell_type":"markdown","metadata":{"id":"1GbjKtNIcnQW"},"source":["### 3) 테스트 데이터셋 준비"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yX8zsAJZcnQW"},"outputs":[],"source":["# tokenization 과정까지 진행된 최종적으로 모델에 입력될 데이터를 출력합니다.\n","def prepare_test_dataset(config, preprocessor, tokenizer):\n","\n","    ### 테스트 데이터 로드:\n","    # test_file_path에서 test.csv 파일을 읽어옵니다.\n","    test_file_path = os.path.join(config['general']['data_path'], 'test.csv')\n","    # Preprocess 클래스의 make_set_as_df 메서드를 사용하여 테스트 데이터를 데이터프레임으로 변환합니다. 이때, is_train=False로 설정하여 학습과 달리 요약(summary)이 포함되지 않은 테스트 데이터를 처리합니다.\n","    # test_data['fname']는 테스트 데이터의 고유 ID로, 각 샘플을 식별하는 데 사용됩니다.\n","    # 테스트 데이터를 데이터프레임으로 변환합니다. is_train=False로 설정하여 테스트 데이터셋을 로드합니다.\n","    test_data = preprocessor.make_set_as_df(test_file_path, is_train=False)\n","    test_id = test_data['fname']  # 테스트 데이터의 고유 ID (fname)을 가져옵니다.\n","\n","\n","    # 데이터 확인:\n","    # 테스트 데이터의 첫 번째 대화(dialogue) 샘플을 출력하여, 데이터가 올바르게 로드되었는지 확인합니다.\n","    print('-' * 150)\n","    print(f'test_data:\\n{test_data[\"dialogue\"][0]}')\n","    print('-' * 150)\n","\n","\n","    ### 인코더 및 디코더 입력 생성:\n","    # Preprocess 클래스의 make_input 메서드를 사용하여 인코더 입력과 디코더 입력을 생성합니다.\n","    # 테스트 데이터의 경우, 디코더 입력은 시작 토큰(bos_token)만 포함합니다.\n","    # 테스트 데이터에 대해 인코더 입력과 디코더 입력을 생성합니다. is_test=True로 설정하여 디코더 입력에만 시작 토큰을 넣습니다.\n","    encoder_input_test, decoder_input_test = preprocessor.make_input(test_data, is_test=True)\n","    print('-' * 10, 'Load data complete', '-' * 10,)\n","\n","\n","    ### 토큰화:\n","    # tokenizer를 사용하여 인코더 입력과 디코더 입력을 토큰화합니다.\n","    # 이 과정에서 패딩, 특수 토큰 추가, 최대 길이 제한 등을 설정하여 텐서 형태로 반환합니다.\n","    # 테스트 데이터의 인코더 입력을 토크나이저로 토큰화합니다.\n","    test_tokenized_encoder_inputs = tokenizer(\n","        encoder_input_test, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False,\n","    )\n","    # 테스트 데이터의 디코더 입력을 토크나이저로 토큰화합니다.\n","    test_tokenized_decoder_inputs = tokenizer(\n","        decoder_input_test, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False,\n","    )\n","\n","\n","    ### 테스트 데이터셋 준비:\n","    # DatasetForInference 클래스를 사용하여 토큰화된 입력 데이터와 테스트 ID를 포함한 데이터셋 객체를 생성합니다.\n","    # 이 데이터셋 객체는 모델이 예측을 수행하는 데 사용됩니다.\n","    test_encoder_inputs_dataset = DatasetForInference(test_tokenized_encoder_inputs, test_id, len(encoder_input_test))\n","    print('-' * 10, 'Make dataset complete', '-' * 10,)\n","\n","\n","    ### 결과 반환:\n","    # 원본 테스트 데이터(test_data)와 모델에 입력될 토큰화된 데이터셋(test_encoder_inputs_dataset)을 반환합니다.\n","    return test_data, test_encoder_inputs_dataset\n","\n","\n","    # 이 함수는 테스트 데이터를 전처리하여 모델에 적합한 입력 데이터로 준비합니다.\n","    # 이를 통해 학습된 모델이 테스트 데이터에 대한 예측을 정확하게 수행할 수 있도록 합니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4XXoAaYAcnQW"},"outputs":[],"source":["print(\"\\n#######  테스트 데이터셋을 준비합니다.  ##########################################################################################\")\n","# 테스트 데이터셋을 준비합니다.\n","test_data, test_encoder_inputs_dataset = prepare_test_dataset(config, preprocessor, tokenizer)\n"]},{"cell_type":"markdown","metadata":{"id":"1epgkXhxcnQW"},"source":["### 4) 데이터 로더 생성 / 테스트 데이터 추론"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GVEAv6lUcnQW"},"outputs":[],"source":["print(\"\\n#######  데이터 로더를 생성합니다. 배치 크기는 설정 파일에서 가져옵니다.  ##########################################################\")\n","# 데이터 로더를 생성합니다. 배치 크기는 설정 파일에서 가져옵니다.\n","dataloader = DataLoader(test_encoder_inputs_dataset, batch_size=config['inference']['batch_size'])\n","\n","summary = []  # 요약문을 저장할 리스트\n","text_ids = []  # 텍스트 ID를 저장할 리스트\n","\n","with torch.no_grad():  # 추론 중에는 기울기 계산을 비활성화하여 메모리를 절약합니다.\n","    for item in tqdm(dataloader):  # 데이터 로더를 통해 배치 단위로 데이터에 접근합니다.\n","        text_ids.extend(item['ID'])  # 현재 배치의 텍스트 ID를 저장합니다.\n","        generated_ids = generate_model.generate(\n","            input_ids=item['input_ids'].to(device),  # 인코더 입력을 디바이스로 이동시켜 모델에 전달합니다.\n","            no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],  # 반복 n-gram 방지 설정\n","            early_stopping=config['inference']['early_stopping'],  # 조기 종료 설정\n","            max_length=config['inference']['generate_max_length'],  # 생성할 텍스트의 최대 길이\n","            num_beams=config['inference']['num_beams'],  # 빔 서치(beam search)의 빔 수 설정\n","        )\n","        for ids in generated_ids:  # 생성된 요약문 ID를 디코딩하여 텍스트로 변환합니다.\n","            result = tokenizer.decode(ids, skip_special_tokens=False)  # 특수 토큰을 건너뛰고 디코딩합니다.\n","            summary.append(result)  # 생성된 요약문을 리스트에 추가합니다."]},{"cell_type":"markdown","metadata":{"id":"2bripAzfcnQW"},"source":["### 5) 최종 결과/요약문 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"txpJRuU4cnQX"},"outputs":[],"source":["print(\"\\n#######  스페셜 토큰을 제거하여 최종 요약문을 전처리합니다.  ######################################################################\")\n","# 스페셜 토큰을 제거하여 최종 요약문을 전처리합니다.\n","remove_tokens = config['inference']['remove_tokens']\n","preprocessed_summary = summary.copy()\n","for token in remove_tokens:\n","    preprocessed_summary = [sentence.replace(token, \" \") for sentence in preprocessed_summary]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TgSYB3Z6cnQX"},"outputs":[],"source":["\n","for i in range(len(preprocessed_summary)):\n","    tmp = preprocessed_summary[i]\n","    left_trimmed = tmp.lstrip()  # 왼쪽 공백 제거\n","    right_trimmed = left_trimmed.rstrip()  # 오른쪽 공백 제거\n","    tmp = right_trimmed\n","    tmp = tmp.replace('# ', '#')\n","    preprocessed_summary[i] = tmp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4UrOHKVycnQX"},"outputs":[],"source":["preprocessed_summary"]},{"cell_type":"markdown","metadata":{"id":"hnZ-c35xcnQX"},"source":["### 6) 최종 결과 데이터프레임으로 정리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHaFna3mcnQX"},"outputs":[],"source":["print(\"\\n#######  최종 결과를 데이터프레임으로 정리합니다. ###############################################################################\")\n","# 최종 결과를 데이터프레임으로 정리합니다.\n","output = pd.DataFrame({\n","    \"fname\": test_data['fname'],  # 파일 이름\n","    \"summary\": preprocessed_summary,  # 전처리된 요약문\n","})"]},{"cell_type":"markdown","metadata":{"id":"Z3uWFXWjcnQX"},"source":["### 7) CSV 파일 저장"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_xBANs_1cnQY"},"outputs":[],"source":["print(\"#######  \", model.replace(\"/\", \"-\") + para + \"-ck\" + checkpoint_num + \"_output.csv\" + \"  파일이 완성되었습니다.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DCXrDa3zcnQY"},"outputs":[],"source":["print(\"\\n#######  결과를 저장할 경로를 설정하고, 결과를 CSV 파일로 저장합니다.  ##########################################################\")\n","# 결과를 저장할 경로를 설정하고, 결과를 CSV 파일로 저장합니다.\n","result_path = config['inference']['result_path']\n","if not os.path.exists(result_path):\n","    os.makedirs(result_path)  # 경로가 존재하지 않으면 디렉토리를 생성합니다.\n","output.to_csv(os.path.join(result_path, model.replace(\"/\", \"-\") + para + \"-ck\" + checkpoint_num + \"_output.csv\"), index=False)  # 결과를 CSV 파일로 저장합니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-hwPK-icnQY"},"outputs":[],"source":["# 결과 확인\n","output"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[],"gpuType":"L4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"da9e969636f94232a6f9829a7300a679":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fbc2752fa0614afb8d182313f1ddd4d2","IPY_MODEL_18215a3e8bc04f72affa677b58025dd2","IPY_MODEL_f2bbd5455cbb4767a451f03a331e4616"],"layout":"IPY_MODEL_865987da9e534001a2093d5b6a5e8b65"}},"fbc2752fa0614afb8d182313f1ddd4d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc50187c270d466c88818ace007b8072","placeholder":"​","style":"IPY_MODEL_b86a100a5a3045b69dd94fbd5296453e","value":"config.json: 100%"}},"18215a3e8bc04f72affa677b58025dd2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dcd5483fb5ff47c697d43363606a8e1f","max":1819,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5756de7b7a3f4488a177165b7442f282","value":1819}},"f2bbd5455cbb4767a451f03a331e4616":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2f5a1c7587449ef857db0882d7de2fb","placeholder":"​","style":"IPY_MODEL_7e5c1960691542268486d53173d13ed3","value":" 1.82k/1.82k [00:00&lt;00:00, 140kB/s]"}},"865987da9e534001a2093d5b6a5e8b65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc50187c270d466c88818ace007b8072":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b86a100a5a3045b69dd94fbd5296453e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dcd5483fb5ff47c697d43363606a8e1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5756de7b7a3f4488a177165b7442f282":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d2f5a1c7587449ef857db0882d7de2fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e5c1960691542268486d53173d13ed3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e0aca1b12b94b939b042bd789ade628":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cb003be3aacb4bf8896441ce075c106f","IPY_MODEL_c7651bba12cd472e8738f6fa36913a50","IPY_MODEL_96de1db8c9a148cf98cae751f9f16fe4"],"layout":"IPY_MODEL_0fa024f0c3bc4c6989cd2945a4db933a"}},"cb003be3aacb4bf8896441ce075c106f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfdebbc1b35e4c30bddb21f06dbe7fa6","placeholder":"​","style":"IPY_MODEL_113686109e42464d95e18500121ae3c4","value":"pytorch_model.bin: 100%"}},"c7651bba12cd472e8738f6fa36913a50":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78a8271e111142d7b27f33b27051afde","max":495743133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b9bb99ffba041a2b46fa0d960287271","value":495743133}},"96de1db8c9a148cf98cae751f9f16fe4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7eab16e1680470d9b60dbeb0ebd256c","placeholder":"​","style":"IPY_MODEL_61887e0ed1534c718af0757bbf4a4a9c","value":" 496M/496M [00:01&lt;00:00, 268MB/s]"}},"0fa024f0c3bc4c6989cd2945a4db933a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfdebbc1b35e4c30bddb21f06dbe7fa6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"113686109e42464d95e18500121ae3c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78a8271e111142d7b27f33b27051afde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b9bb99ffba041a2b46fa0d960287271":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7eab16e1680470d9b60dbeb0ebd256c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61887e0ed1534c718af0757bbf4a4a9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}