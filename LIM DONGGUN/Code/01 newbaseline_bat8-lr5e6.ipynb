{"cells":[{"cell_type":"markdown","metadata":{"id":"_UvLnUoquU67"},"source":["## 0. 준비"]},{"cell_type":"markdown","metadata":{"id":"JVXQOkJKuU68"},"source":["### 1) 라이브러리 설치"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7eVJxImiuU68","outputId":"2f9cf35c-a720-43c1-abb2-f5b68819125a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wandb\n","  Downloading wandb-0.17.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n","Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n","Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n","Downloading wandb-0.17.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n"]}],"source":["!pip install wandb\n","!pip install torch\n","!pip install pytorch_lightning\n","!pip install rouge"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yNUePe30uU69"},"outputs":[],"source":[" #!wandb --help"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjpRJqc_uU69"},"outputs":[],"source":["#!wandb login #--relogin\n","# kkukky@naver.com  220215bca12e71dfd5815f1648ca8dbbb2c1bef8\n","#  17b70d1b235684f485db5bcc4b47788ca0e90fd2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"--hRF5IeuU69"},"outputs":[],"source":["\n","import pandas as pd             # 데이터 프레임을 다루기 위한 라이브러리입니다. 주로 데이터 처리 및 분석에 사용됩니다.\n","import os                       # 운영 체제와 상호작용하기 위한 모듈로, 파일 및 디렉터리 작업에 사용됩니다.\n","import re                       # 정규 표현식을 사용하여 문자열을 검색, 처리하는 데 사용됩니다.\n","import json                     # JSON 형식의 데이터를 처리하기 위한 라이브러리입니다.\n","import yaml                     # YAML 형식의 데이터를 처리하기 위한 라이브러리입니다.\n","from glob import glob           # 특정 패턴에 맞는 파일 경로들을 리스트로 반환하는 모듈입니다.\n","from tqdm import tqdm           # 반복문에 대한 진행 상황을 시각적으로 보여주는 라이브러리입니다.\n","from pprint import pprint       # 데이터를 좀 더 읽기 쉽게 출력하기 위한 라이브러리입니다.\n","import torch                    # PyTorch 라이브러리로, 딥러닝 모델을 구축하고 학습하기 위한 핵심 라이브러리입니다.\n","import pytorch_lightning as pl  # PyTorch의 고수준 API로, 모델 학습을 간소화하고 구조화된 방식으로 진행할 수 있습니다.\n","from rouge import Rouge         # 텍스트 요약 및 생성 모델의 성능을 평가하기 위해 사용하는 지표 중 하나입니다.\n","\n","from torch.utils.data import Dataset, DataLoader  # 데이터셋을 다루고, 이를 모델 학습에 사용할 수 있도록 배치(batch) 단위로 나누는 데 사용됩니다.\n","from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig  # 트랜스포머 모델을 위한 라이브러리로, 토크나이저와 모델을 불러오는 데 사용됩니다.\n","from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer  # Seq2Seq (Sequence-to-Sequence) 모델 학습을 위한 도구와 설정을 제공합니다.\n","from transformers import Trainer, TrainingArguments  # 일반적인 모델 학습을 위한 도구와 설정을 제공합니다.\n","from transformers import EarlyStoppingCallback  # 학습 과정에서 성능 향상이 없을 때 조기 종료를 할 수 있도록 도와주는 콜백 함수입니다.\n","\n","import wandb                    # 모델 학습 과정을 쉽게 추적하고 시각화할 수 있는 툴입니다. 주로 실험 관리 및 결과 기록에 사용됩니다.\n","\n","wandb.login(key=\"220215bca12e71dfd5815f1648ca8dbbb2c1bef8\")\n","\n","#!wandb login #--relogin\n","# kkukky@naver.com     220215bca12e71dfd5815f1648ca8dbbb2c1bef8\n","# kkukky81@gmail.com   17b70d1b235684f485db5bcc4b47788ca0e90fd2\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_zdzJm6-uU6-"},"source":["### 2) 모델 선택, title 만들기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rLtG2OueuU6-"},"outputs":[],"source":["from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","### bart 계열\n","# model = \"digit82/kobart-summarization\"\n","# model = \"NLPBada/kobart-chat-persona-extraction-v2\"\n","# model = \"EbanLee/kobart-summary-v3\"\n","model = 'jx7789/kobart_summary_v3'\n","# model = \"ainize/kobart-news\" 아주 나쁨\n","# model = \"hyesunyun/update-summarization-bart-large-longformer\"\n","\n","### T5 계열\n","# model = \"t5-small\"\n","# model = \"KETI-AIR/ke-t5-base-ko\" 값이 안나옴\n","# model = \"eenzeenee/t5-base-korean-summarization\" 안됨\n","# model = 'psyche/KoT5-summarization' #안됨\n","# model = 'csebuetnlp/mT5_multilingual_XLSum'\n","\n","\n","train = \"train.csv\"\n","\n","para = \"-bat16-5l6-n4-b5\"\n","\n","title = model.replace('/','_') + para\n","\n","# config 설정에 tokenizer 모듈이 사용되므로 미리 tokenizer를 정의해줍니다.\n","#tokenizer = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")\n","tokenizer = AutoTokenizer.from_pretrained(model)\n","\n","#from transformers import AutoModel\n","\n","#model = AutoModel.from_pretrained(model, force_download=True)\n","\n","print('###   ', title, '   ###')\n","\n","#output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n"]},{"cell_type":"markdown","metadata":{"id":"1VY0zIejuU6-"},"source":["### 3) config 파일 만들기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1q0rrIB5uU6-"},"outputs":[],"source":["batch = 8\n","config_data = {\n","    \"general\": {\n","        \"data_path\": \"/content/drive/MyDrive/AILAB/NLP/data\",  # 모델 학습에 사용할 데이터가 저장된 경로를 지정합니다.\n","        #\"model_name\": \"digit82/kobart-summarization\",  # 사용할 사전 학습된 모델의 이름을 지정합니다.\n","        \"model_name\": model,  # 사용할 사전 학습된 모델의 이름을 지정합니다.\n","        \"output_dir\": \"./\"  # 모델의 출력물(예: 생성된 텍스트)을 저장할 디렉터리를 지정합니다.\n","    },\n","    \"training\": {\n","        \"overwrite_output_dir\": True,       # True로 설정하면, 기존에 존재하는 출력 디렉터리 내용을 덮어씁니다.\n","        \"num_train_epochs\": 20,             # 전체 데이터셋을 몇 번 반복해서 학습할지를 설정합니다. (20번)\n","        \"learning_rate\": 5e-6,              # 학습률(learning rate)을 설정합니다.\n","        \"per_device_train_batch_size\": batch,  # 각 디바이스(예: GPU)에서 한 번에 학습할 데이터 배치의 크기를 설정합니다. (50)\n","        \"per_device_eval_batch_size\": batch,   # 평가 시 사용할 배치 크기를 설정합니다. (32)\n","        \"warmup_ratio\": 0.1,                # 학습 초기에 학습률을 천천히 증가시키는 비율을 설정합니다.\n","        \"weight_decay\": 0.01,               # 가중치 감쇠(weight decay) 값을 설정합니다. (과적합 방지를 위해 사용)\n","        \"lr_scheduler_type\": 'cosine_with_restarts',      # 학습률 스케줄러 유형을 'cosine'으로 설정합니다.\n","        \"optim\": 'adamw_torch',             # 옵티마이저(optimizer)로 AdamW를 사용합니다.\n","        \"gradient_accumulation_steps\": 1,   # 기울기(gradient) 축적을 위한 스텝 수를 설정합니다. (1이면 축적 없이 바로 업데이트)\n","        \"evaluation_strategy\": 'epoch',     # 평가를 언제 수행할지 설정합니다. ('epoch'는 각 에폭 종료 시 평가)\n","        \"save_strategy\": 'epoch',           # 모델을 저장할 시점을 설정합니다. ('epoch'는 각 에폭 종료 시 저장)\n","        \"save_total_limit\": 7,              # 최대 저장할 체크포인트 수를 설정합니다. (가장 최근 7개만 유지)\n","        \"fp16\": True,                       # 반정밀도(floating point 16) 연산을 사용할지 설정합니다. (True이면 메모리 절약 및 속도 향상)\n","        \"load_best_model_at_end\": True,     # 학습이 종료될 때 가장 성능이 좋은 모델을 불러옵니다.\n","        \"seed\": 42,                         # 랜덤 시드를 설정하여 실험의 재현성을 보장합니다.\n","        \"logging_dir\": \"./logs\",            # 학습 로그를 저장할 디렉터리를 설정합니다.\n","        \"logging_strategy\": \"epoch\",        # 로그를 언제 기록할지 설정합니다. ('epoch'는 각 에폭 종료 시 기록)\n","        \"predict_with_generate\": True,      # 평가 시 텍스트 생성을 수행할지 설정합니다.\n","        \"generation_max_length\": 200,       # 텍스트 생성 시 최대 길이를 설정합니다. (100 토큰)\n","        \"do_train\": True,                   # 모델을 학습할지 여부를 설정합니다. (True로 설정)\n","        \"do_eval\": True,                    # 모델을 평가할지 여부를 설정합니다. (True로 설정)\n","        \"early_stopping_patience\": 3,       # 조기 종료를 위한 인내 기간(몇 번의 에폭 동안 성능 개선이 없으면 종료)을 설정합니다. (3)\n","        \"early_stopping_threshold\": 0.001,  # 조기 종료를 위한 성능 개선 최소 임계값을 설정합니다. (0.001)\n","        \"report_to\": \"wandb\"                # (선택 사항) wandb를 사용하여 학습 과정을 보고할지 설정합니다.\n","    },\n","    \"inference\": {\n","        \"ckt_path\": \"./\",  # 학습된 모델의 체크포인트 파일 경로를 설정합니다.\n","        \"result_path\": \"./prediction/\",  # 추론 결과를 저장할 경로를 설정합니다.\n","        \"no_repeat_ngram_size\": 4,  # 생성된 텍스트에서 동일한 n-gram이 반복되지 않도록 설정합니다. (2-gram 기준)\n","        \"early_stopping\": True,  # 조기 종료를 사용할지 여부를 설정합니다.\n","        \"generate_max_length\": 200,  # 생성 텍스트의 최대 길이를 설정합니다. (100 토큰)\n","        \"num_beams\": 5,  # 빔 서치(beam search) 시 사용할 빔 수를 설정합니다. 1~5까지 가능함. (4)\n","        \"batch_size\": batch,  # 추론 시 사용할 배치 크기를 설정합니다. (32)\n","        # 모델 생성 결과에서 불필요한 토큰들을 제거합니다.\n","        \"remove_tokens\": ['<usr>', f\"{tokenizer.bos_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n","    },\n","    \"tokenizer\": {\n","        \"encoder_max_len\": 1024,  # 입력 텍스트를 인코딩할 때 최대 길이를 설정합니다. (512 토큰)\n","        \"decoder_max_len\": 200,  # 출력 텍스트(생성된 텍스트)를 디코딩할 때 최대 길이를 설정합니다. (100 토큰)\n","        \"bos_token\": f\"{tokenizer.bos_token}\",  # 시작 토큰(beginning of sentence)을 지정합니다.\n","        \"eos_token\": f\"{tokenizer.eos_token}\",  # 끝 토큰(end of sentence)을 지정합니다.\n","        # 특정 단어들이 분해되지 않도록, special_tokens을 지정하여 토크나이저에서 처리할 때 이들 단어를 그대로 유지합니다.\n","        \"special_tokens\": [ '#Person1#',     #76737\n","                            '#Person2#',     #70211\n","                            '#Person3#',     #452\n","                            '#Person4#',     #41\n","                            '#Person5#',     #5\n","                            '#Person6#',     #9\n","                            '#Person7#',     #3\n","                            '#SSN#',         #3\n","                            '#PhoneNumber#', #203\n","                            '#Address#',     #45\n","                            '#Email#',       #17\n","                            '#CarNumber#',   #6\n","                            '#CardNumber#'   #10\n","                            '#DateOfBirth#', #8\n","                            '#PassportNumber#']  #7\n","                            # #Person 2#    띄어쓰기 오타 1개 o\n","                            # 9547,9548     #Person1      #없는 오타 2개 o\n","                            # 9547,9548     #Person2      #없는 오타 2개 o\n","                            # 9750,9779     Person1#      #없는 오타 2개 o\n","                            # 420           #PhoneNumber  #없는 오타 1개 o\n","                            # #Person#      숫자 없는 오타 1개 o\n","                            # 839      #사람1만기 시 계정 갱신 o\n","                            # 1033sum  이 사람2#에게 내일 아침 o\n","                            # 1125     사람1#: 제니, 이번 o\n","                            # 1133     사람2#은 그 기간 동 o\n","                            # 1030     이 사람2#에게 내일 o\n","                            # 1142     사람1#: 실례합니다. 저는 o\n","                            # 1199     사람1#은 시험에 대한 준비가 된 o\n","                            # 1213     #하지만 장기간의 o\n","                            # 1236     #고객님, 크루즈 컨트롤에 o\n","                            # 1250     ##여기 있습니다. 스티븐 o\n","                            # 1266     #고객님, 저희는 고객이 화나 o\n","                            # 1278     #고객님, 죄송합니다만 계 o\n","                            # 1281     #잠깐만요, 버전 7 o\n","                            # 1283     #어디 보자. 네, 그런 방 o\n","                            # 1301     #샐러드용 드레싱은 o\n","                            # 1302     #페리에와 짐 빔 세 o\n","                            # 1306     #나 부엌에 있어. . . o\n","                            # 1322     #여기서 만나서 반갑 o\n","                            # 1547     #작은 걸로 주세 o\n","                            # 1609     #여기 있습니다. o\n","                            # 정상 8320   음표는 G#이라고 써있어.\n","                            # 10370    회사 #에서 기술자로 근  o\n","                            # 11716    ##: 안녕, 프란시스   #이 두개 o\n","                            # 4001     (Person A가 탈의실에서 나옴) 스웨터는 어떠셨나요? o\n","                            # 2255     전화번호는 610-555-1234입니다. o\n","                            # 2719     네, 488-6361입니다. 3시까지 저 o\n","                            # 2980     전화번호는 513-3284입니다. o\n","    },\n","    # (선택 사항) wandb 설정: wandb 홈페이지에서 받은 entity, project, run_name 정보를 설정합니다.\n","    \"wandb\": {\n","            \"entity\": \"kkukky-empty\",  # wandb에서 실험을 기록할 엔티티(entity)를 설정합니다.\n","            \"project\": \"NLP2\",  # wandb 프로젝트 이름을 설정합니다.\n","            \"name\": title # wandb에서 실행(run) 이름을 설정합니다.\n","    }\n","}\n","\n","\n","### 변수 적용 간편화를 위해 코드 옮김\n","\n","# 모델의 구성 정보를 YAML 파일로 저장합니다.\n","config_path = \"./config_bart.yaml\"\n","with open(config_path, \"w\") as file:\n","    yaml.dump(config_data, file, allow_unicode=True)\n","\n","# 저장된 config 파일을 불러옵니다.\n","config_path = \"./config_bart.yaml\"\n","with open(config_path, \"r\") as file:\n","    config = yaml.safe_load(file)\n","\n","# 불러온 config 파일의 전체 내용을 확인합니다.\n","# pprint(config)\n"]},{"cell_type":"markdown","metadata":{"id":"HtMdEpI3uU6_"},"source":["### 4) 데이터 확인"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"rUcvcVjqw9kZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RqQ95T6ouU6_"},"outputs":[],"source":["# config에 저장된 데이터 경로를 통해 train과 validation data를 불러옵니다.\n","data_path = config['general']['data_path']\n","print(data_path)\n","\n","# train data의 구조와 내용을 확인합니다.\n","train_df = pd.read_csv(os.path.join(data_path, train))\n","train_df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"atzsUHHUuU6_"},"outputs":[],"source":["# validation data의 구조와 내용을 확인합니다.\n","val_df = pd.read_csv(os.path.join(data_path,'dev.csv'))\n","val_df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tji2nNySuU6_"},"outputs":[],"source":["# validation data의 구조와 내용을 확인합니다.\n","test_df = pd.read_csv(os.path.join(data_path,'test.csv'))\n","test_df.tail()"]},{"cell_type":"markdown","metadata":{"id":"RSbDC2PouU7A"},"source":["## 1. 모델 트레이닝"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y4ilJw9PuU7A"},"outputs":[],"source":["# 사용할 device를 정의합니다. GPU가 사용 가능하면 'cuda:0', 그렇지 않으면 'cpu'를 사용합니다.\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('-' * 10, f'device : {device}', '-' * 10,)\n","print(torch.__version__)"]},{"cell_type":"markdown","metadata":{"id":"QuQkMMnouU7A"},"source":["### 1) 토크나이저와 모델 로드"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rrv5IrRxuU7A"},"outputs":[],"source":["def load_tokenizer_and_model_for_train(config, device):\n","    print('-' * 10, 'Load tokenizer & model', '-' * 10,)\n","    print('-' * 10, f'Model Name : {config[\"general\"][\"model_name\"]}', '-' * 10,)\n","\n","     #모델 이름 불러오기:\n","    # config['general']['model_name']을 통해 사전 학습된 모델의 이름을 설정 파일에서 불러옵니다.\n","    # 이는 Hugging Face의 모델 허브에서 가져올 모델의 이름입니다.\n","    model_name = config['general']['model_name']\n","\n","\n","    ### BART ##################\n","    # BART 설정 로드:\n","    # BartConfig를 사용하여 지정된 모델 이름으로부터 BART의 설정을 불러옵니다.\n","    # 이 설정은 모델의 구조와 하이퍼파라미터를 정의합니다.\n","    bart_config = BartConfig().from_pretrained(model_name)\n","\n","    # 토크나이저 로드:\n","    # AutoTokenizer.from_pretrained를 사용하여 지정된 모델 이름으로부터 토크나이저를 불러옵니다.\n","    # 이 토크나이저는 텍스트 데이터를 토큰화하는 역할을 합니다.\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","    # 사전 학습된 모델 로드:\n","    # BartForConditionalGeneration.from_pretrained를 사용하여 사전 학습된 BART 모델을 불러옵니다.\n","    # 이 모델은 텍스트 생성, 요약 등의 작업을 수행하는 데 사용됩니다.\n","    generate_model = BartForConditionalGeneration.from_pretrained(model_name, config=bart_config)\n","\n","\n","    # 특수 토큰 추가:\n","    # special_tokens_dict를 사용하여 설정에서 정의한 특수 토큰을 토크나이저에 추가합니다.\n","    # 이는 예를 들어, 특정 인물이나 장소를 나타내는 토큰을 추가하는 등의 작업에 사용됩니다.\n","    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n","    tokenizer.add_special_tokens(special_tokens_dict)\n","    # 토크나이저에 추가된 토큰에 맞추어 모델의 토큰 임베딩 크기를 조정합니다.\n","    # 모델의 토큰 임베딩 크기를 업데이트된 토크나이저의 크기에 맞게 조정합니다.\n","    generate_model.resize_token_embeddings(len(tokenizer))\n","\n","\n","    # 모델을 지정된 디바이스로 이동:\n","    # 모델을 device 변수에 지정된 디바이스(GPU 또는 CPU)로 이동시킵니다.\n","    # 이를 통해 모델이 올바른 디바이스에서 실행되도록 합니다.\n","    generate_model.to(device)\n","\n","\n","    # 모델 설정 출력:\n","    # 로드된 모델의 설정을 출력하여, 모델의 구조와 하이퍼파라미터 등을 확인할 수 있습니다.\n","    print(generate_model.config)\n","\n","    print('-' * 10, 'Load tokenizer & model complete', '-' * 10,)\n","\n","    # 모델과 토크나이저 반환:\n","    return generate_model, tokenizer\n","\n","    # 최종적으로 불러온 모델과 토크나이저를 반환하여, 이후 학습 과정에서 사용할 수 있도록 준비합니다.\n","    # 이 함수는 학습을 위해 필요한 모델과 토크나이저를 설정 파일에 따라 자동으로 불러오고 준비해 줍니다.\n","    # 특수 토큰의 추가 및 모델 임베딩의 재구성도 함께 처리하여, 모델이 주어진 작업에 최적화될 수 있도록 돕습니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ywi-fJj0uU7A"},"outputs":[],"source":["# 사용할 모델과 토크나이저를 로드합니다.\n","print(\"\\n#######  사용할 모델과 토크나이저를 로드합니다.  ################################################################################\")\n","generate_model, tokenizer = load_tokenizer_and_model_for_train(config, device)\n","print('-' * 10, \"tokenizer special tokens : \", tokenizer.special_tokens_map, '-' * 10)"]},{"cell_type":"markdown","metadata":{"id":"BeElc0xxuU7A"},"source":["### 2) 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tH3PUGYeuU7A"},"outputs":[],"source":["\n","# 데이터 전처리를 위한 클래스로, 데이터셋을 데이터프레임으로 변환하고 인코더와 디코더의 입력을 생성합니다.\n","class Preprocess:\n","    def __init__(self,\n","            bos_token: str,\n","            eos_token: str,\n","        ) -> None:\n","\n","        # 클래스 초기화 메서드입니다. 시작 토큰(bos_token)과 종료 토큰(eos_token)을 인스턴스 변수로 저장합니다.\n","        self.bos_token = bos_token\n","        self.eos_token = eos_token\n","\n","    @staticmethod\n","    # 파일 경로를 입력 받아 CSV 파일을 읽고, 필요한 컬럼들만 선택하여 데이터프레임으로 반환합니다.\n","    def make_set_as_df(file_path, is_train = True):\n","\n","        # is_train이 True면 학습용 데이터로, False면 테스트용 데이터로 처리합니다.\n","        if is_train:\n","            df = pd.read_csv(file_path)\n","            train_df = df[['fname','dialogue','summary']]\n","            return train_df\n","\n","        # 테스트 데이터인 경우, 'fname'과 'dialogue' 컬럼만 선택하여 반환합니다.\n","        else:\n","            df = pd.read_csv(file_path)\n","            test_df = df[['fname','dialogue']]\n","            return test_df\n","\n","\n","    # BART 모델의 입력과 출력을 생성하는 메서드입니다.\n","    def make_input(self, dataset, is_test=False):\n","\n","        # is_test가 True면 테스트 데이터를 위한 입력만 생성하고, False면 학습 데이터를 위한 입력과 출력을 생성합니다.\n","        if is_test:\n","            # 테스트 데이터의 경우, 인코더 입력과 디코더의 시작 토큰으로만 구성된 입력을 반환합니다.\n","            # 인코더 입력으로 'dialogue' 텍스트를 사용합니다.\n","            encoder_input = dataset['dialogue']\n","            # 디코더 입력은 시작 토큰으로만 구성합니다.\n","            decoder_input = [self.bos_token] * len(dataset['dialogue'])\n","\n","            return encoder_input.tolist(), list(decoder_input)\n","        else:\n","            # 학습 데이터의 경우, 인코더 입력, 디코더 입력, 디코더 출력을 모두 생성합니다.\n","            encoder_input = dataset['dialogue']  # 인코더 입력으로 'dialogue' 텍스트를 사용합니다.\n","\n","            # 디코더 입력은 시작 토큰(bos_token)과 요약 텍스트(summary)로 구성됩니다.\n","            decoder_input = dataset['summary'].apply(lambda x: self.bos_token + str(x))\n","\n","            # 디코더 출력은 요약 텍스트(summary)와 종료 토큰(eos_token)으로 구성됩니다.\n","            decoder_output = dataset['summary'].apply(lambda x: str(x) + self.eos_token)\n","\n","            # 리스트로 변환하여 반환합니다.\n","            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mv0ALffOuU7B"},"outputs":[],"source":["# 학습에 사용할 데이터셋을 전처리하고 로드합니다.\n","print(\"\\n#######  학습에 사용할 데이터셋을 전처리하고 로드합니다.  ########################################################################\")\n","# 시작 토큰(beginning of sentence)과 종료 토큰(end of sentence)을 설정합니다.\n","preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])\n"]},{"cell_type":"markdown","metadata":{"id":"QGg8MTqsuU7B"},"source":["### 3) 학습 및 검증 데이터셋 준비"]},{"cell_type":"markdown","metadata":{"id":"snIYLPVzuU7B"},"source":["#### 3-1) Train, Validation, Test 클래스 정의"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dA-6hiOduU7B"},"outputs":[],"source":["# Train에 사용되는 Dataset 클래스를 정의합니다.\n","class DatasetForTrain(Dataset):\n","    def __init__(self, encoder_input, decoder_input, labels, len):\n","        # 학습 데이터셋 초기화 메서드입니다. 인코더 입력, 디코더 입력, 레이블, 데이터 길이를 저장합니다.\n","        self.encoder_input = encoder_input\n","        self.decoder_input = decoder_input\n","        self.labels = labels\n","        self.len = len\n","\n","    def __getitem__(self, idx):\n","        # 주어진 인덱스(idx)에 해당하는 데이터를 반환하는 메서드입니다.\n","\n","        # 인코더 입력 데이터에서 해당 인덱스의 데이터를 복사하여 `item` 딕셔너리에 저장합니다.\n","        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}  # item[input_ids], item[attention_mask]\n","\n","        # 디코더 입력 데이터에서 해당 인덱스의 데이터를 복사하여 `item2` 딕셔너리에 저장합니다.\n","        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}  # item2[input_ids], item2[attention_mask]\n","\n","        # `item2` 딕셔너리의 'input_ids'와 'attention_mask'를 각각 'decoder_input_ids'와 'decoder_attention_mask'로 이름을 변경합니다.\n","        item2['decoder_input_ids'] = item2['input_ids']\n","        item2['decoder_attention_mask'] = item2['attention_mask']\n","        item2.pop('input_ids')  # 'input_ids' 키 제거\n","        item2.pop('attention_mask')  # 'attention_mask' 키 제거\n","\n","        # `item` 딕셔너리에 디코더의 입력 정보를 추가합니다.\n","        item.update(item2)  # item[input_ids], item[attention_mask], item[decoder_input_ids], item[decoder_attention_mask]\n","\n","        # 레이블로 사용할 'input_ids' 값을 `item` 딕셔너리에 추가합니다.\n","        item['labels'] = self.labels['input_ids'][idx]  # item[input_ids], item[attention_mask], item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n","\n","        return item\n","\n","    def __len__(self):\n","        # 데이터셋의 길이를 반환하는 메서드입니다.\n","        return self.len\n","\n","\n","# Validation에 사용되는 Dataset 클래스를 정의합니다.\n","class DatasetForVal(Dataset):\n","    def __init__(self, encoder_input, decoder_input, labels, len):\n","        # 검증 데이터셋 초기화 메서드입니다. 학습 데이터셋과 동일한 구조로 정의됩니다.\n","        self.encoder_input = encoder_input\n","        self.decoder_input = decoder_input\n","        self.labels = labels\n","        self.len = len\n","\n","    def __getitem__(self, idx):\n","        # 주어진 인덱스(idx)에 해당하는 데이터를 반환하는 메서드입니다.\n","        # 학습 데이터셋과 동일하게 정의됩니다.\n","        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()} # item[input_ids], item[attention_mask]\n","        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()} # item2[input_ids], item2[attention_mask]\n","        item2['decoder_input_ids'] = item2['input_ids']\n","        item2['decoder_attention_mask'] = item2['attention_mask']\n","        item2.pop('input_ids')\n","        item2.pop('attention_mask')\n","        item.update(item2) #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask]\n","        item['labels'] = self.labels['input_ids'][idx] #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n","        return item\n","\n","    def __len__(self):\n","        # 데이터셋의 길이를 반환하는 메서드입니다.\n","        return self.len\n","\n","\n","# Test에 사용되는 Dataset 클래스를 정의합니다.\n","class DatasetForInference(Dataset):     # inference뜻 : 추론\n","    def __init__(self, encoder_input, test_id, len):\n","        # 테스트 데이터셋 초기화 메서드입니다. 인코더 입력, 테스트 ID, 데이터 길이를 저장합니다.\n","        self.encoder_input = encoder_input\n","        self.test_id = test_id\n","        self.len = len\n","\n","    def __getitem__(self, idx):\n","        # 주어진 인덱스(idx)에 해당하는 데이터를 반환하는 메서드입니다.\n","        # 인코더 입력 데이터에서 해당 인덱스의 데이터를 복사하여 `item` 딕셔너리에 저장합니다.\n","        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n","        item['ID'] = self.test_id[idx]\n","        return item\n","\n","    def __len__(self):\n","        # 데이터셋의 길이를 반환하는 메서드입니다.\n","        return self.len\n"]},{"cell_type":"markdown","metadata":{"id":"j1TNfNoouU7B"},"source":["#### 3-2) Train, Validation 데이터셋 준비"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5sotZePuU7B"},"outputs":[],"source":["def prepare_train_dataset(config, preprocessor, data_path, tokenizer):\n","\n","    ### 데이터 로드 및 변환:\n","    #학습 및 검증 데이터 파일을 읽어 데이터프레임으로 변환합니다.\n","    #데이터프레임으로 변환된 데이터에서 대화(dialogue)와 요약(summary) 텍스트를 각각 학습 입력과 라벨로 사용합니다.\n","    # 데이터셋 경로를 지정합니다.\n","    train_file_path = os.path.join(data_path, train)  # 학습 데이터 파일 경로\n","    val_file_path = os.path.join(data_path, 'dev.csv')  # 검증 데이터 파일 경로\n","    # 학습(train)과 검증(validation) 데이터셋을 데이터프레임으로 변환합니다.\n","    train_data = preprocessor.make_set_as_df(train_file_path)  # 학습 데이터 로드\n","    val_data = preprocessor.make_set_as_df(val_file_path)  # 검증 데이터 로드\n","    # 로드된 학습 데이터와 라벨의 첫 번째 샘플을 출력합니다.\n","    print('-' * 150)\n","    print(f'train_data:\\n {train_data[\"dialogue\"][0]}')\n","    print(f'train_label:\\n {train_data[\"summary\"][0]}')\n","    # 로드된 검증 데이터와 라벨의 첫 번째 샘플을 출력합니다.\n","    print('-' * 150)\n","    print(f'val_data:\\n {val_data[\"dialogue\"][0]}')\n","    print(f'val_label:\\n {val_data[\"summary\"][0]}')\n","\n","\n","    ### 데이터 전처리:\n","    # Preprocess 클래스의 make_input 메서드를 사용하여 인코더 입력, 디코더 입력, 디코더 출력을 생성합니다.\n","    # 이 과정에서 BART 모델에 적합한 형태로 텍스트 데이터를 구성합니다.\n","    # 학습 데이터에 대해 인코더 입력, 디코더 입력, 디코더 출력을 생성합니다.\n","    encoder_input_train, decoder_input_train, decoder_output_train = preprocessor.make_input(train_data)\n","    # 검증 데이터에 대해 인코더 입력, 디코더 입력, 디코더 출력을 생성합니다.\n","    encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)\n","    print('-' * 10, 'Load data complete', '-' * 10)\n","\n","\n","    ### 토큰화:\n","    # 텍스트 데이터를 tokenizer를 사용하여 토큰화합니다. 토큰화 과정에서:\n","    # 텍스트를 토큰 ID로 변환합니다.\n","    # 필요한 경우, 패딩(padding=True), 최대 길이(max_length), 특수 토큰(add_special_tokens=True) 등을 추가합니다.\n","    # 반환된 데이터는 PyTorch 텐서(return_tensors=\"pt\")로 반환됩니다.\n","    # 학습 데이터의 인코더 입력을 토크나이저로 토큰화합니다.\n","    tokenized_encoder_inputs = tokenizer(\n","        encoder_input_train, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False\n","    )\n","    # 학습 데이터의 디코더 입력을 토크나이저로 토큰화합니다.\n","    tokenized_decoder_inputs = tokenizer(\n","        decoder_input_train, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False\n","    )\n","    # 학습 데이터의 디코더 출력을 토크나이저로 토큰화합니다.\n","    tokenized_decoder_ouputs = tokenizer(\n","        decoder_output_train, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False\n","    )\n","\n","\n","    ### 데이터셋 생성:\n","    # 학습 데이터와 검증 데이터를 각각 DatasetForTrain과 DatasetForVal 클래스로 감싸 데이터셋 객체를 생성합니다.\n","    # 이 객체들은 PyTorch의 DataLoader와 함께 사용되어 모델 학습 및 검증에 활용됩니다.\n","    # 학습용 데이터셋을 생성합니다.\n","    train_inputs_dataset = DatasetForTrain(\n","        tokenized_encoder_inputs, tokenized_decoder_inputs, tokenized_decoder_ouputs, len(encoder_input_train)\n","    )\n","    # 검증 데이터의 인코더 입력을 토크나이저로 토큰화합니다.\n","    val_tokenized_encoder_inputs = tokenizer(\n","        encoder_input_val, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False\n","    )\n","    # 검증 데이터의 디코더 입력을 토크나이저로 토큰화합니다.\n","    val_tokenized_decoder_inputs = tokenizer(\n","        decoder_input_val, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False\n","    )\n","    # 검증 데이터의 디코더 출력을 토크나이저로 토큰화합니다.\n","    val_tokenized_decoder_ouputs = tokenizer(\n","        decoder_output_val, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False\n","    )\n","    # 검증용 데이터셋을 생성합니다.\n","    val_inputs_dataset = DatasetForVal(\n","        val_tokenized_encoder_inputs, val_tokenized_decoder_inputs, val_tokenized_decoder_ouputs, len(encoder_input_val)\n","    )\n","    print('-' * 10, 'Make dataset complete', '-' * 10)\n","\n","\n","    ### 출력 및 반환:\n","    # 학습용 데이터셋과 검증용 데이터셋을 반환합니다. 이를 통해 모델 학습 과정에서 필요한 데이터를 제공하게 됩니다.\n","    return train_inputs_dataset, val_inputs_dataset\n","\n","\n","    # 이 함수는 모델 학습 및 검증을 위한 데이터 준비 과정에서 필요한\n","    # 모든 전처리, 토큰화, 데이터셋 생성을 자동으로 처리하여, 최종적으로 Dataset 객체를 반환합니다.\n","    # 이를 통해 모델 학습 및 검증을 효율적으로 수행할 수 있습니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wixy26OjuU7C"},"outputs":[],"source":["print(\"\\n#######  학습 및 검증 데이터셋을 준비합니다.  ###################################################################################\")\n","# 학습 및 검증 데이터셋을 준비합니다.\n","data_path = config['general']['data_path']  # 데이터 경로를 설정에서 가져옵니다.\n","train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config, preprocessor, data_path, tokenizer)\n"]},{"cell_type":"markdown","metadata":{"id":"A2bzm0nruU7C"},"source":["### 4) Trainer 클래스 초기화"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esz4VT63uU7C"},"outputs":[],"source":["def compute_metrics(config, tokenizer, pred):\n","    ### rouge = Rouge():\n","    # ROUGE 점수를 계산하기 위해 Rouge 클래스를 초기화합니다.\n","    # ROUGE는 주로 텍스트 요약의 품질을 평가할 때 사용되는 지표입니다.\n","    rouge = Rouge()\n","\n","\n","    ### pred.predictions 및 pred.label_ids:\n","    # predictions: 모델이 예측한 토큰 ID의 배열입니다.\n","    # label_ids: 실제 레이블(정답) 토큰 ID의 배열입니다.\n","    # 예측된 토큰 ID와 실제 레이블 ID를 가져옵니다.\n","    predictions = pred.predictions\n","    labels = pred.label_ids\n","\n","\n","    ### 패딩 토큰 처리:\n","    # 예측 값과 레이블에서 -100으로 표시된 패딩 토큰을 실제 패딩 토큰 ID로 교체하여 평가에서 패딩이 영향을 미치지 않도록 합니다.\n","    # 모델 출력 중 패딩 토큰을 의미하는 -100 값을 tokenizer의 패딩 토큰 ID로 변경합니다.\n","    predictions[predictions == -100] = tokenizer.pad_token_id\n","    labels[labels == -100] = tokenizer.pad_token_id\n","\n","\n","    ### 토큰 디코딩:\n","    # 토큰 ID 배열을 원래의 텍스트 문자열로 변환합니다.\n","    # batch_decode는 여러 개의 토큰 배열을 한꺼번에 디코딩합니다.\n","    # 예측된 토큰 ID를 텍스트로 디코딩합니다.\n","    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)\n","    # 실제 레이블의 토큰 ID도 텍스트로 디코딩합니다.\n","    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n","\n","\n","    ### 불필요한 토큰 제거:\n","    # 모델이 생성한 텍스트에서 사전에 정의된 불필요한 토큰을 제거하여 평가의 정확성을 높입니다.\n","    # 평가를 위해 불필요한 토큰들을 제거합니다.\n","    replaced_predictions = decoded_preds.copy()  # 예측된 텍스트 복사\n","    replaced_labels = labels.copy()  # 실제 레이블 텍스트 복사\n","    remove_tokens = config['inference']['remove_tokens']  # 제거할 토큰 목록을 config에서 가져옵니다.\n","    # 각 불필요한 토큰을 제거합니다.\n","    for token in remove_tokens:\n","        replaced_predictions = [sentence.replace(token, \" \") for sentence in replaced_predictions]\n","        replaced_labels = [sentence.replace(token, \" \") for sentence in replaced_labels]\n","\n","\n","    ### 출력:\n","    # 평가를 위해 디코딩된 예측 텍스트와 실제 레이블의 일부 샘플을 출력합니다.\n","    # 첫 번째, 두 번째, 세 번째 예측과 실제 레이블을 출력합니다.\n","    print('-' * 150)\n","    print(f\"PRED: {replaced_predictions[0]}\")\n","    print(f\"GOLD: {replaced_labels[0]}\")\n","    print('-' * 150)\n","    print(f\"PRED: {replaced_predictions[1]}\")\n","    print(f\"GOLD: {replaced_labels[1]}\")\n","    print('-' * 150)\n","    print(f\"PRED: {replaced_predictions[2]}\")\n","    print(f\"GOLD: {replaced_labels[2]}\")\n","\n","\n","    ### ROUGE 점수 계산:\n","    # replaced_predictions와 replaced_labels를 사용하여 ROUGE 점수를 계산합니다.\n","    # ROUGE-1, ROUGE-2, ROUGE-L 등의 F1 점수를 계산하여 반환합니다.\n","    # 최종적으로 ROUGE 점수를 계산합니다.\n","    results = rouge.get_scores(replaced_predictions, replaced_labels, avg=True)\n","\n","\n","    ### 결과 반환:\n","    # ROUGE 점수 중 F1-score를 추출하여 딕셔너리 형태로 반환합니다.\n","    result = {key: value[\"f\"] for key, value in results.items()}\n","    return result\n","\n","\n","    # 이 함수는 모델이 생성한 텍스트의 품질을 ROUGE 지표로 평가하여,\n","    # 모델 성능을 평가하는 데 중요한 역할을 합니다.\n","    # ROUGE 점수를 통해 텍스트 요약 또는 생성 모델의 정확성을 평가할 수 있습니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EHeARDkquU7C"},"outputs":[],"source":["def load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset):\n","    print('-' * 10, 'Make training arguments', '-' * 10,)\n","\n","    ### Seq2SeqTrainingArguments 설정:\n","    # 학습과 관련된 다양한 설정값들을 정의하는 Seq2SeqTrainingArguments 객체를 생성합니다.\n","    # 학습률, 배치 크기, 에포크 수, 로그 저장 위치 등 다양한 하이퍼파라미터와 옵션들이 포함됩니다.\n","    # 학습을 위한 설정값들을 정의합니다.\n","    training_args = Seq2SeqTrainingArguments(\n","        output_dir=config['general']['output_dir'],                                     # 모델 출력 디렉터리\n","        overwrite_output_dir=config['training']['overwrite_output_dir'],                # 출력 디렉터리를 덮어쓸지 여부\n","        num_train_epochs=config['training']['num_train_epochs'],                        # 전체 학습 에포크 수\n","        learning_rate=config['training']['learning_rate'],                              # 학습률\n","        per_device_train_batch_size=config['training']['per_device_train_batch_size'],  # 학습 시 디바이스당 배치 크기\n","        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],    # 평가 시 디바이스당 배치 크기\n","        warmup_ratio=config['training']['warmup_ratio'],                                # 학습 초기에 학습률을 점진적으로 증가시키는 비율\n","        weight_decay=config['training']['weight_decay'],                                # 가중치 감쇠 (과적합 방지)\n","        lr_scheduler_type=config['training']['lr_scheduler_type'],                      # 학습률 스케줄러 유형\n","        optim=config['training']['optim'],                                              # 옵티마이저 종류\n","        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],  # 기울기(gradient) 축적 단계 수\n","        evaluation_strategy=config['training']['evaluation_strategy'],                  # 학습 중 평가 전략 (예: 에포크마다 평가)\n","        save_strategy=config['training']['save_strategy'],                              # 모델 저장 전략\n","        save_total_limit=config['training']['save_total_limit'],                        # 저장할 체크포인트의 최대 개수\n","        fp16=config['training']['fp16'],                                                # 반정밀도(float16) 연산 사용 여부\n","        load_best_model_at_end=config['training']['load_best_model_at_end'],            # 학습 종료 시 가장 좋은 모델 로드\n","        seed=config['training']['seed'],                                                # 랜덤 시드 설정\n","        logging_dir=config['training']['logging_dir'],                                  # 로그 저장 디렉터리\n","        logging_strategy=config['training']['logging_strategy'],                        # 로그 기록 전략 (예: 에포크마다 기록)\n","        predict_with_generate=config['training']['predict_with_generate'],              # 텍스트 생성 후 평가 지표를 계산할지 여부\n","        generation_max_length=config['training']['generation_max_length'],              # 텍스트 생성 시 최대 길이\n","        do_train=config['training']['do_train'],                                        # 학습 수행 여부\n","        do_eval=config['training']['do_eval'],                                          # 평가 수행 여부\n","        report_to=config['training']['report_to']                                       # (선택 사항) wandb로 학습 과정을 기록할지 여부\n","    )\n","\n","\n","    ### wandb 초기화 (선택 사항):\n","    # (선택 사항) wandb를 사용하여 학습 과정을 추적할 때 초기화합니다.\n","    # WandB(Weights & Biases)로 학습 과정을 추적하고 시각화하려면 wandb.init()을 사용해 초기화할 수 있습니다.\n","    # 이 부분은 현재 주석 처리되어 있으며, 필요한 경우 활성화할 수 있습니다.\n","    wandb.init(\n","         entity=config['wandb']['entity'],\n","         project=config['wandb']['project'],\n","         name=config['wandb']['name']\n","    )\n","    # (선택 사항) 모델 체크포인트를 wandb에 저장하도록 환경 변수를 설정합니다.\n","    os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n","    os.environ[\"WANDB_WATCH\"] = \"false\"\n","\n","\n","    ### EarlyStoppingCallback 설정:\n","    # EarlyStoppingCallback을 사용하여 학습이 진행될 때 검증 손실이 더 이상 개선되지 않으면 학습을 조기에 중단시킵니다.\n","    # 설정된 early_stopping_patience와 early_stopping_threshold에 따라 작동합니다.\n","    # EarlyStoppingCallback: 검증 손실이 더 이상 개선되지 않을 때 학습을 중단시키는 콜백을 설정합니다.\n","    MyCallback = EarlyStoppingCallback(\n","        early_stopping_patience=config['training']['early_stopping_patience'],      # 개선이 없을 경우 중단까지 기다릴 에포크 수\n","        early_stopping_threshold=config['training']['early_stopping_threshold']     # 개선으로 간주할 최소 손실 감소량\n","    )\n","    print('-' * 10, 'Make training arguments complete', '-' * 10,)\n","    print('-' * 10, 'Make trainer', '-' * 10,)\n","\n","\n","    ### Seq2SeqTrainer 초기화:\n","    # Seq2SeqTrainer는 Hugging Face의 transformers 라이브러리에서 제공하는 훈련 도구로,\n","    # 시퀀스-투-시퀀스 모델의 학습과 평가를 위한 도구입니다.\n","    # 학습할 모델, 설정값, 학습/검증 데이터셋, 평가 메트릭 함수, 콜백 등을 인자로 받아 초기화합니다.\n","    trainer = Seq2SeqTrainer(\n","        model=generate_model,  # 학습할 모델\n","        args=training_args,  # 학습 설정값\n","        train_dataset=train_inputs_dataset,  # 학습 데이터셋\n","        eval_dataset=val_inputs_dataset,  # 검증 데이터셋\n","        compute_metrics=lambda pred: compute_metrics(config, tokenizer, pred),  # 성능 평가를 위한 메트릭 함수\n","        callbacks=[MyCallback]  # EarlyStoppingCallback을 포함한 콜백 리스트\n","    )\n","    print('-' * 10, 'Make trainer complete', '-' * 10,)\n","\n","    ### Trainer 객체 반환:\n","    # 생성된 trainer 객체를 반환하여, 이후 학습을 진행할 수 있게 합니다.\n","    return trainer\n","\n","\n","    # 이 함수는 모델 학습을 시작하기 위한 모든 준비 작업을 자동으로 처리하며,\n","    # 사용자에게 최적화된 Trainer 객체를 제공합니다. 이를 통해 학습 및 평가를 손쉽게 수행할 수 있습니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-uYBpr8AuU7C"},"outputs":[],"source":["print(\"\\n#######  학습을 위한 Trainer 클래스를 초기화합니다.  ############################################################################\")\n","# 학습을 위한 Trainer 클래스를 초기화합니다.\n","trainer = load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset)"]},{"cell_type":"markdown","metadata":{"id":"6FnbXnrZuU7D"},"source":["### 5) 모델 학습, wandb 세션종료"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0wmOw41HuU7D"},"outputs":[],"source":["print(\"\\n#######  모델 학습을 시작합니다.  ##############################################################################################\")\n","print(title)\n","# 모델 학습을 시작합니다.\n","trainer.train()\n","\n","print(\"\\n#######  wandb 세션을 종료합니다.  ############################################################################################\")\n","# (선택) 모델 학습이 완료된 후 wandb 세션을 종료합니다.\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"1aNeSxAnuU7D"},"source":["## 2. 모델 추론하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xoppTq_4uU7D"},"outputs":[],"source":["# 이곳에 내가 사용할 wandb config 설정 \"추론에 사용할 ckt 경로 설정\"\n","# loaded_config['inference']['ckt_path'] = \"home/ckt\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y7yQbm0auU7D"},"outputs":[],"source":["# 사용할 device를 정의합니다. GPU가 사용 가능하면 'cuda:0', 그렇지 않으면 'cpu'를 사용합니다.\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('-' * 10, f'device : {device}', '-' * 10,)\n","print(torch.__version__)"]},{"cell_type":"markdown","metadata":{"id":"lOG8WafxuU7D"},"source":["### 1) 모델과 토크나이저 불러오기"]},{"cell_type":"markdown","metadata":{"id":"2KlKMdWNuU7D"},"source":["<font color=red size=32><b> ckt 값 바꾸기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PxK3z-ZGuU7D"},"outputs":[],"source":["멈춤"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FERPc5NQuU7E"},"outputs":[],"source":["checkpoint_num = \"62290\"\n","\n","# 추론을 위한 tokenizer와 학습시킨 모델을 불러옵니다.\n","def load_tokenizer_and_model_for_test(config, device):\n","    print('-' * 10, 'Load tokenizer & model', '-' * 10,)\n","\n","    print(\"\\n######  설정 파일에서 모델 이름과 체크포인트 경로를 불러옵니다.\")\n","    ### 모델 이름과 체크포인트 경로 설정:\n","    # 설정 파일에서 모델 이름과 체크포인트 경로를 불러옵니다.\n","    # config['general']['model_name']: 사전 학습된 모델의 이름을 설정 파일에서 가져옵니다.\n","    # config['inference']['ckt_path']: 학습된 모델의 체크포인트 경로를 설정 파일에서 가져옵니다. 이 경로는 학습이 완료된 후 저장된 모델의 위치를 나타냅니다.\n","    model_name = config['general']['model_name']\n","    ckt_path = config['inference']['ckt_path']  # 학습된 모델의 체크포인트 경로\n","    ckt_path = \"/home/code/checkpoint-\" + checkpoint_num\n","    print(model_name, \" / \", ckt_path)\n","    print('-' * 10, f'Model Name : {model_name}', '-' * 10,)\n","\n","\n","    ### 토크나이저 로드 및 특수 토큰 추가:\n","    # AutoTokenizer.from_pretrained(model_name): 사전 학습된 모델 이름을 사용하여 토크나이저를 로드합니다.\n","    # tokenizer.add_special_tokens(special_tokens_dict): 설정 파일에서 정의된 특수 토큰을 토크나이저에 추가합니다. 이는 모델이 특정 단어들을 특수 토큰으로 처리할 수 있도록 합니다.\n","    print(\"\\n######  지정된 모델 이름으로부터 토크나이저를 로드합니다.\")\n","    # 지정된 모델 이름으로부터 토크나이저를 로드합니다.\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    print(\"\\n######  설정 파일에서 정의된 특수 토큰을 토크나이저에 추가합니다.\")\n","    # 설정 파일에서 정의된 특수 토큰을 토크나이저에 추가합니다.\n","    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n","    tokenizer.add_special_tokens(special_tokens_dict)\n","\n","\n","    ### 모델 로드:\n","    # BartForConditionalGeneration.from_pretrained(ckt_path): 학습된 모델의 체크포인트에서 BART 모델을 로드합니다.\n","    # generate_model.resize_token_embeddings(len(tokenizer)): 토크나이저에 추가된 특수 토큰에 맞춰 모델의 토큰 임베딩 크기를 재조정합니다.\n","    print(\"\\n######  학습된 체크포인트에서 BART 모델을 로드합니다.\")\n","    # 학습된 체크포인트에서 BART 모델을 로드합니다.\n","    generate_model = BartForConditionalGeneration.from_pretrained(ckt_path)\n","    print(\"\\n######  추가된 토큰에 맞게 모델의 토큰 임베딩 크기를 조정합니다.\")\n","    # 추가된 토큰에 맞게 모델의 토큰 임베딩 크기를 조정합니다.\n","    generate_model.resize_token_embeddings(len(tokenizer))\n","\n","\n","    ### 디바이스로 모델 이동:\n","    # generate_model.to(device): 로드된 모델을 지정된 디바이스(GPU 또는 CPU)로 이동시켜,\n","    # 추론 작업을 해당 디바이스에서 수행할 수 있도록 합니다.\n","    print(\"\\n######  모델을 지정된 디바이스(GPU 또는 CPU)로 이동시킵니다.\")\n","    # 모델을 지정된 디바이스(GPU 또는 CPU)로 이동시킵니다.\n","    generate_model.to(device)\n","\n","    print('-' * 10, 'Load tokenizer & model complete', '-' * 10,)\n","\n","\n","    ### 모델과 토크나이저 반환:\n","    # 최종적으로 불러온 모델과 토크나이저를 반환하여, 이후 추론 작업에서 사용할 수 있도록 합니다.\n","    return generate_model, tokenizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7Kcf1W8uU7E"},"outputs":[],"source":["print(\"\\n#######  추론을 위한 모델과 토크나이저를 불러옵니다.  ##############################################################################\")\n","# 추론을 위한 모델과 토크나이저를 불러옵니다.\n","generate_model, tokenizer = load_tokenizer_and_model_for_test(config, device)"]},{"cell_type":"markdown","metadata":{"id":"PbYBahXLuU7E"},"source":["### 2) 데이터 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F0EInGukuU7E"},"outputs":[],"source":["print(\"\\n#######  데이터 경로와 전처리기를 설정합니다.  ####################################################################################\")\n","# 데이터 경로와 전처리기를 설정합니다.\n","data_path = config['general']['data_path']\n","preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])"]},{"cell_type":"markdown","metadata":{"id":"U5YoQOUFuU7E"},"source":["### 3) 테스트 데이터셋 준비"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pBlOm8g0uU7E"},"outputs":[],"source":["# tokenization 과정까지 진행된 최종적으로 모델에 입력될 데이터를 출력합니다.\n","def prepare_test_dataset(config, preprocessor, tokenizer):\n","\n","    ### 테스트 데이터 로드:\n","    # test_file_path에서 test.csv 파일을 읽어옵니다.\n","    test_file_path = os.path.join(config['general']['data_path'], 'test.csv')\n","    # Preprocess 클래스의 make_set_as_df 메서드를 사용하여 테스트 데이터를 데이터프레임으로 변환합니다. 이때, is_train=False로 설정하여 학습과 달리 요약(summary)이 포함되지 않은 테스트 데이터를 처리합니다.\n","    # test_data['fname']는 테스트 데이터의 고유 ID로, 각 샘플을 식별하는 데 사용됩니다.\n","    # 테스트 데이터를 데이터프레임으로 변환합니다. is_train=False로 설정하여 테스트 데이터셋을 로드합니다.\n","    test_data = preprocessor.make_set_as_df(test_file_path, is_train=False)\n","    test_id = test_data['fname']  # 테스트 데이터의 고유 ID (fname)을 가져옵니다.\n","\n","\n","    # 데이터 확인:\n","    # 테스트 데이터의 첫 번째 대화(dialogue) 샘플을 출력하여, 데이터가 올바르게 로드되었는지 확인합니다.\n","    print('-' * 150)\n","    print(f'test_data:\\n{test_data[\"dialogue\"][0]}')\n","    print('-' * 150)\n","\n","\n","    ### 인코더 및 디코더 입력 생성:\n","    # Preprocess 클래스의 make_input 메서드를 사용하여 인코더 입력과 디코더 입력을 생성합니다.\n","    # 테스트 데이터의 경우, 디코더 입력은 시작 토큰(bos_token)만 포함합니다.\n","    # 테스트 데이터에 대해 인코더 입력과 디코더 입력을 생성합니다. is_test=True로 설정하여 디코더 입력에만 시작 토큰을 넣습니다.\n","    encoder_input_test, decoder_input_test = preprocessor.make_input(test_data, is_test=True)\n","    print('-' * 10, 'Load data complete', '-' * 10,)\n","\n","\n","    ### 토큰화:\n","    # tokenizer를 사용하여 인코더 입력과 디코더 입력을 토큰화합니다.\n","    # 이 과정에서 패딩, 특수 토큰 추가, 최대 길이 제한 등을 설정하여 텐서 형태로 반환합니다.\n","    # 테스트 데이터의 인코더 입력을 토크나이저로 토큰화합니다.\n","    test_tokenized_encoder_inputs = tokenizer(\n","        encoder_input_test, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False,\n","    )\n","    # 테스트 데이터의 디코더 입력을 토크나이저로 토큰화합니다.\n","    test_tokenized_decoder_inputs = tokenizer(\n","        decoder_input_test, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False,\n","    )\n","\n","\n","    ### 테스트 데이터셋 준비:\n","    # DatasetForInference 클래스를 사용하여 토큰화된 입력 데이터와 테스트 ID를 포함한 데이터셋 객체를 생성합니다.\n","    # 이 데이터셋 객체는 모델이 예측을 수행하는 데 사용됩니다.\n","    test_encoder_inputs_dataset = DatasetForInference(test_tokenized_encoder_inputs, test_id, len(encoder_input_test))\n","    print('-' * 10, 'Make dataset complete', '-' * 10,)\n","\n","\n","    ### 결과 반환:\n","    # 원본 테스트 데이터(test_data)와 모델에 입력될 토큰화된 데이터셋(test_encoder_inputs_dataset)을 반환합니다.\n","    return test_data, test_encoder_inputs_dataset\n","\n","\n","    # 이 함수는 테스트 데이터를 전처리하여 모델에 적합한 입력 데이터로 준비합니다.\n","    # 이를 통해 학습된 모델이 테스트 데이터에 대한 예측을 정확하게 수행할 수 있도록 합니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ne6EZM_UuU7F"},"outputs":[],"source":["print(\"\\n#######  테스트 데이터셋을 준비합니다.  ##########################################################################################\")\n","# 테스트 데이터셋을 준비합니다.\n","test_data, test_encoder_inputs_dataset = prepare_test_dataset(config, preprocessor, tokenizer)\n"]},{"cell_type":"markdown","metadata":{"id":"N003ia6SuU7F"},"source":["### 4) 데이터 로더 생성 / 테스트 데이터 추론"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BrLKvTZJuU7F"},"outputs":[],"source":["print(\"\\n#######  데이터 로더를 생성합니다. 배치 크기는 설정 파일에서 가져옵니다.  ##########################################################\")\n","# 데이터 로더를 생성합니다. 배치 크기는 설정 파일에서 가져옵니다.\n","dataloader = DataLoader(test_encoder_inputs_dataset, batch_size=config['inference']['batch_size'])\n","\n","summary = []  # 요약문을 저장할 리스트\n","text_ids = []  # 텍스트 ID를 저장할 리스트\n","\n","with torch.no_grad():  # 추론 중에는 기울기 계산을 비활성화하여 메모리를 절약합니다.\n","    for item in tqdm(dataloader):  # 데이터 로더를 통해 배치 단위로 데이터에 접근합니다.\n","        text_ids.extend(item['ID'])  # 현재 배치의 텍스트 ID를 저장합니다.\n","        generated_ids = generate_model.generate(\n","            input_ids=item['input_ids'].to(device),  # 인코더 입력을 디바이스로 이동시켜 모델에 전달합니다.\n","            no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],  # 반복 n-gram 방지 설정\n","            early_stopping=config['inference']['early_stopping'],  # 조기 종료 설정\n","            max_length=config['inference']['generate_max_length'],  # 생성할 텍스트의 최대 길이\n","            num_beams=config['inference']['num_beams'],  # 빔 서치(beam search)의 빔 수 설정\n","        )\n","        for ids in generated_ids:  # 생성된 요약문 ID를 디코딩하여 텍스트로 변환합니다.\n","            result = tokenizer.decode(ids, skip_special_tokens=False)  # 특수 토큰을 건너뛰고 디코딩합니다.\n","            summary.append(result)  # 생성된 요약문을 리스트에 추가합니다."]},{"cell_type":"markdown","metadata":{"id":"lxgQ7icRuU7F"},"source":["### 5) 최종 결과/요약문 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHDCtvuwuU7F"},"outputs":[],"source":["print(\"\\n#######  스페셜 토큰을 제거하여 최종 요약문을 전처리합니다.  ######################################################################\")\n","# 스페셜 토큰을 제거하여 최종 요약문을 전처리합니다.\n","remove_tokens = config['inference']['remove_tokens']\n","preprocessed_summary = summary.copy()\n","for token in remove_tokens:\n","    preprocessed_summary = [sentence.replace(token, \" \") for sentence in preprocessed_summary]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PtbMUqh2uU7F"},"outputs":[],"source":["\n","for i in range(len(preprocessed_summary)):\n","    tmp = preprocessed_summary[i]\n","    left_trimmed = tmp.lstrip()  # 왼쪽 공백 제거\n","    right_trimmed = left_trimmed.rstrip()  # 오른쪽 공백 제거\n","    tmp = right_trimmed\n","    tmp = tmp.replace('# ', '#')\n","    preprocessed_summary[i] = tmp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJZa3zFLuU7G"},"outputs":[],"source":["preprocessed_summary"]},{"cell_type":"markdown","metadata":{"id":"JfLH5VjSuU7G"},"source":["### 6) 최종 결과 데이터프레임으로 정리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLSNp03LuU7G"},"outputs":[],"source":["print(\"\\n#######  최종 결과를 데이터프레임으로 정리합니다. ###############################################################################\")\n","# 최종 결과를 데이터프레임으로 정리합니다.\n","output = pd.DataFrame({\n","    \"fname\": test_data['fname'],  # 파일 이름\n","    \"summary\": preprocessed_summary,  # 전처리된 요약문\n","})"]},{"cell_type":"markdown","metadata":{"id":"X6o9oX9vuU7G"},"source":["### 7) CSV 파일 저장"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6IyTO0icuU7G"},"outputs":[],"source":["print(\"#######  \", model.replace(\"/\", \"-\") + para + \"-ck\" + checkpoint_num + \"_output.csv\" + \"  파일명이 완성되었습니다.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BQN_8HkvuU7G"},"outputs":[],"source":["print(\"\\n#######  결과를 저장할 경로를 설정하고, 결과를 CSV 파일로 저장합니다.  ##########################################################\")\n","# 결과를 저장할 경로를 설정하고, 결과를 CSV 파일로 저장합니다.\n","result_path = config['inference']['result_path']\n","if not os.path.exists(result_path):\n","    os.makedirs(result_path)  # 경로가 존재하지 않으면 디렉토리를 생성합니다.\n","output.to_csv(os.path.join(result_path, model.replace(\"/\", \"-\") + para + \"-ck\" + checkpoint_num + \"_output.csv\"), index=False)  # 결과를 CSV 파일로 저장합니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PM0SmPc5uU7G"},"outputs":[],"source":["# 결과 확인\n","output"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}