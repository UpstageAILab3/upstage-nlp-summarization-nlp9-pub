{"cells":[{"cell_type":"markdown","metadata":{"id":"JqB26h1QLf_5"},"source":["## 0. 준비"]},{"cell_type":"markdown","metadata":{"id":"VtsWg5qwLf_8"},"source":["### 1) 라이브러리 설치"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M37ZNsiULf_8"},"outputs":[],"source":["# !pip install wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T82JP95qLf_9"},"outputs":[],"source":["# !wandb --help"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QaI81a0RLf_-","outputId":"47434594-bb56-4de0-f4ba-ed4320468e6c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkkukky81\u001b[0m (\u001b[33mdonggunlim\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]}],"source":["!wandb login #--relogin\n","# 17b70d1b235684f485db5bcc4b47788ca0e90fd2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cQdKb1xjLf_-"},"outputs":[],"source":["\n","import pandas as pd             # 데이터 프레임을 다루기 위한 라이브러리입니다. 주로 데이터 처리 및 분석에 사용됩니다.\n","import os                       # 운영 체제와 상호작용하기 위한 모듈로, 파일 및 디렉터리 작업에 사용됩니다.\n","import re                       # 정규 표현식을 사용하여 문자열을 검색, 처리하는 데 사용됩니다.\n","import json                     # JSON 형식의 데이터를 처리하기 위한 라이브러리입니다.\n","import yaml                     # YAML 형식의 데이터를 처리하기 위한 라이브러리입니다.\n","from glob import glob           # 특정 패턴에 맞는 파일 경로들을 리스트로 반환하는 모듈입니다.\n","from tqdm import tqdm           # 반복문에 대한 진행 상황을 시각적으로 보여주는 라이브러리입니다.\n","from pprint import pprint       # 데이터를 좀 더 읽기 쉽게 출력하기 위한 라이브러리입니다.\n","import torch                    # PyTorch 라이브러리로, 딥러닝 모델을 구축하고 학습하기 위한 핵심 라이브러리입니다.\n","import pytorch_lightning as pl  # PyTorch의 고수준 API로, 모델 학습을 간소화하고 구조화된 방식으로 진행할 수 있습니다.\n","from rouge import Rouge         # 텍스트 요약 및 생성 모델의 성능을 평가하기 위해 사용하는 지표 중 하나입니다.\n","\n","from torch.utils.data import Dataset, DataLoader  # 데이터셋을 다루고, 이를 모델 학습에 사용할 수 있도록 배치(batch) 단위로 나누는 데 사용됩니다.\n","from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig  # 트랜스포머 모델을 위한 라이브러리로, 토크나이저와 모델을 불러오는 데 사용됩니다.\n","from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer  # Seq2Seq (Sequence-to-Sequence) 모델 학습을 위한 도구와 설정을 제공합니다.\n","from transformers import Trainer, TrainingArguments  # 일반적인 모델 학습을 위한 도구와 설정을 제공합니다.\n","from transformers import EarlyStoppingCallback  # 학습 과정에서 성능 향상이 없을 때 조기 종료를 할 수 있도록 도와주는 콜백 함수입니다.\n","\n","import wandb                    # 모델 학습 과정을 쉽게 추적하고 시각화할 수 있는 툴입니다. 주로 실험 관리 및 결과 기록에 사용됩니다.\n","\n","\n","### 추가\n","#from rouge_score import rouge_scorer\n"]},{"cell_type":"markdown","metadata":{"id":"X4DF404-Lf__"},"source":["### 2) config file 만들기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A3GEXfWqLf__","outputId":"36466b21-e7a5-42d0-bd91-3900576a8a35"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/home/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  do_lower_case = self.original_tokenizer.basic_tokenizer.do_lower_case\n"]}],"source":["from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","# model = \"digit82/kobart-summarization\"\n","# model = \"NLPBada/kobart-chat-persona-extraction-v2\"\n","# model = \"EbanLee/kobart-summary-v3\"\n","# model = \"KETI-AIR/ke-t5-base-ko\" 값이 안나옴\n","# model = \"t5-small\"\n","# model = 'jx7789/kobart_summary_v3'\n","# model = \"EbanLee/kobart-summary-v3\"\n","# model = \"eenzeenee/t5-base-korean-summarization\" 안됨\n","# model = 'psyche/KoT5-summarization' 안됨\n","model = 'csebuetnlp/mT5_multilingual_XLSum'\n","\n","train = \"train_agu.csv\"\n","\n","para = \"test\"\n","\n","\n","# config 설정에 tokenizer 모듈이 사용되므로 미리 tokenizer를 정의해줍니다.\n","#tokenizer = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")\n","tokenizer = AutoTokenizer.from_pretrained(model)\n","\n","#from transformers import AutoModel\n","\n","#model = AutoModel.from_pretrained(model, force_download=True)\n","\n","# \"digit82/kobart-summarization\"이라는 이름의 사전 학습된 KoBART 모델에 맞는 토크나이저를 로드합니다.\n","# 이 토크나이저는 한국어 텍스트 요약을 위해 사전 학습된 모델에 사용됩니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDB-k4uULgAA"},"outputs":[],"source":["config_data = {\n","    \"general\": {\n","        \"data_path\": \"../data/\",  # 모델 학습에 사용할 데이터가 저장된 경로를 지정합니다.\n","        #\"model_name\": \"digit82/kobart-summarization\",  # 사용할 사전 학습된 모델의 이름을 지정합니다.\n","        \"model_name\": model,  # 사용할 사전 학습된 모델의 이름을 지정합니다.\n","        \"output_dir\": \"./\"  # 모델의 출력물(예: 생성된 텍스트)을 저장할 디렉터리를 지정합니다.\n","    },\n","    \"tokenizer\": {\n","        \"encoder_max_len\": 1024,  # 입력 텍스트를 인코딩할 때 최대 길이를 설정합니다. (512 토큰)\n","        \"decoder_max_len\": 200,  # 출력 텍스트(생성된 텍스트)를 디코딩할 때 최대 길이를 설정합니다. (100 토큰)\n","        \"bos_token\": f\"{tokenizer.bos_token}\",  # 시작 토큰(beginning of sentence)을 지정합니다.\n","        \"eos_token\": f\"{tokenizer.eos_token}\",  # 끝 토큰(end of sentence)을 지정합니다.\n","        # 특정 단어들이 분해되지 않도록, special_tokens을 지정하여 토크나이저에서 처리할 때 이들 단어를 그대로 유지합니다.\n","        \"special_tokens\": ['#Person1#',     #76737\n","                           '#Person2#',     #70211\n","                           '#Person3#',     #452\n","                           '#Person4#',     #41\n","                           '#Person5#',     #5\n","                           '#Person6#',     #9\n","                           '#Person7#',     #3\n","                           '#SSN#',         #3\n","                           '#PhoneNumber#', #203\n","                           '#Address#',     #45\n","                           '#Email#',       #17\n","                           '#CarNumber#',   #6\n","                           '#CardNumber#'   #10\n","                           '#DateOfBirth#', #8\n","                           '#PassportNumber#']  #7\n","                            # #Person 2#    띄어쓰기 오타 1개 o\n","                            # 9547,9548     #Person1      #없는 오타 2개 o\n","                            # 9547,9548     #Person2      #없는 오타 2개 o\n","                            # 9750,9779     Person1#      #없는 오타 2개 o\n","                            # 420           #PhoneNumber  #없는 오타 1개 o\n","                            # #Person#      숫자 없는 오타 1개 o\n","                            # 839      #사람1만기 시 계정 갱신 o\n","                            # 1033sum  이 사람2#에게 내일 아침 o\n","                            # 1125     사람1#: 제니, 이번 o\n","                            # 1133     사람2#은 그 기간 동 o\n","                            # 1030     이 사람2#에게 내일 o\n","                            # 1142     사람1#: 실례합니다. 저는 o\n","                            # 1199     사람1#은 시험에 대한 준비가 된 o\n","                            # 1213     #하지만 장기간의 o\n","                            # 1236     #고객님, 크루즈 컨트롤에 o\n","                            # 1250     ##여기 있습니다. 스티븐 o\n","                            # 1266     #고객님, 저희는 고객이 화나 o\n","                            # 1278     #고객님, 죄송합니다만 계 o\n","                            # 1281     #잠깐만요, 버전 7 o\n","                            # 1283     #어디 보자. 네, 그런 방 o\n","                            # 1301     #샐러드용 드레싱은 o\n","                            # 1302     #페리에와 짐 빔 세 o\n","                            # 1306     #나 부엌에 있어. . . o\n","                            # 1322     #여기서 만나서 반갑 o\n","                            # 1547     #작은 걸로 주세 o\n","                            # 1609     #여기 있습니다. o\n","                            # 정상 8320   음표는 G#이라고 써있어.\n","                            # 10370    회사 #에서 기술자로 근  o\n","                            # 11716    ##: 안녕, 프란시스   #이 두개 o\n","                            # 4001     (Person A가 탈의실에서 나옴) 스웨터는 어떠셨나요? o\n","                            # 2255     전화번호는 610-555-1234입니다. o\n","                            # 2719     네, 488-6361입니다. 3시까지 저 o\n","                            # 2980     전화번호는 513-3284입니다. o\n","    },\n","    \"training\": {\n","        \"overwrite_output_dir\": True,       # True로 설정하면, 기존에 존재하는 출력 디렉터리 내용을 덮어씁니다.\n","        \"num_train_epochs\": 20,             # 전체 데이터셋을 몇 번 반복해서 학습할지를 설정합니다. (20번)\n","        \"learning_rate\": 1e-5,              # 학습률(learning rate)을 설정합니다.\n","        \"per_device_train_batch_size\": 16,  # 각 디바이스(예: GPU)에서 한 번에 학습할 데이터 배치의 크기를 설정합니다. (50)\n","        \"per_device_eval_batch_size\": 16,   # 평가 시 사용할 배치 크기를 설정합니다. (32)\n","        \"warmup_ratio\": 0.1,                # 학습 초기에 학습률을 천천히 증가시키는 비율을 설정합니다.\n","        \"weight_decay\": 0.01,               # 가중치 감쇠(weight decay) 값을 설정합니다. (과적합 방지를 위해 사용)\n","        \"lr_scheduler_type\": 'cosine_with_restarts',      # 학습률 스케줄러 유형을 'cosine'으로 설정합니다.\n","        \"optim\": 'adamw_torch',             # 옵티마이저(optimizer)로 AdamW를 사용합니다.\n","        \"gradient_accumulation_steps\": 1,   # 기울기(gradient) 축적을 위한 스텝 수를 설정합니다. (1이면 축적 없이 바로 업데이트)\n","        \"evaluation_strategy\": 'epoch',     # 평가를 언제 수행할지 설정합니다. ('epoch'는 각 에폭 종료 시 평가)\n","        \"save_strategy\": 'epoch',           # 모델을 저장할 시점을 설정합니다. ('epoch'는 각 에폭 종료 시 저장)\n","        \"save_total_limit\": 5,              # 최대 저장할 체크포인트 수를 설정합니다. (가장 최근 5개만 유지)\n","        \"fp16\": True,                       # 반정밀도(floating point 16) 연산을 사용할지 설정합니다. (True이면 메모리 절약 및 속도 향상)\n","        \"load_best_model_at_end\": True,     # 학습이 종료될 때 가장 성능이 좋은 모델을 불러옵니다.\n","        \"seed\": 42,                         # 랜덤 시드를 설정하여 실험의 재현성을 보장합니다.\n","        \"logging_dir\": \"./logs\",            # 학습 로그를 저장할 디렉터리를 설정합니다.\n","        \"logging_strategy\": \"epoch\",        # 로그를 언제 기록할지 설정합니다. ('epoch'는 각 에폭 종료 시 기록)\n","        \"predict_with_generate\": True,      # 평가 시 텍스트 생성을 수행할지 설정합니다.\n","        \"generation_max_length\": 200,       # 텍스트 생성 시 최대 길이를 설정합니다. (100 토큰)\n","        \"do_train\": True,                   # 모델을 학습할지 여부를 설정합니다. (True로 설정)\n","        \"do_eval\": True,                    # 모델을 평가할지 여부를 설정합니다. (True로 설정)\n","        \"early_stopping_patience\": 3,       # 조기 종료를 위한 인내 기간(몇 번의 에폭 동안 성능 개선이 없으면 종료)을 설정합니다. (3)\n","        \"early_stopping_threshold\": 0.001,  # 조기 종료를 위한 성능 개선 최소 임계값을 설정합니다. (0.001)\n","        \"report_to\": \"wandb\"                # (선택 사항) wandb를 사용하여 학습 과정을 보고할지 설정합니다.\n","    },\n","    # (선택 사항) wandb 설정: wandb 홈페이지에서 받은 entity, project, run_name 정보를 설정합니다.\n","    \"wandb\": {\n","         \"entity\": \"donggunlim\",  # wandb에서 실험을 기록할 엔티티(entity)를 설정합니다.\n","         \"project\": \"NLP\",  # wandb 프로젝트 이름을 설정합니다.\n","         \"name\": model + para # wandb에서 실행(run) 이름을 설정합니다.\n","    },\n","    \"inference\": {\n","        \"ckt_path\": \"home/code/\",  # 학습된 모델의 체크포인트 파일 경로를 설정합니다.\n","        \"result_path\": \"./prediction/\",  # 추론 결과를 저장할 경로를 설정합니다.\n","        \"no_repeat_ngram_size\": 2,  # 생성된 텍스트에서 동일한 n-gram이 반복되지 않도록 설정합니다. (2-gram 기준)\n","        \"early_stopping\": True,  # 조기 종료를 사용할지 여부를 설정합니다.\n","        \"generate_max_length\": 200,  # 생성 텍스트의 최대 길이를 설정합니다. (100 토큰)\n","        \"num_beams\": 4,  # 빔 서치(beam search) 시 사용할 빔 수를 설정합니다. (4)\n","        \"batch_size\": 16,  # 추론 시 사용할 배치 크기를 설정합니다. (32)\n","        # 모델 생성 결과에서 불필요한 토큰들을 제거합니다.\n","        \"remove_tokens\": ['<usr>', f\"{tokenizer.bos_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n","    }\n","}\n","\n","\n","### 변수 적용 간편화를 위해 코드 옮김\n","\n","# 모델의 구성 정보를 YAML 파일로 저장합니다.\n","config_path = \"./config.yaml\"\n","with open(config_path, \"w\") as file:\n","    yaml.dump(config_data, file, allow_unicode=True)\n","\n","\n","# 저장된 config 파일을 불러옵니다.\n","config_path = \"./config.yaml\"\n","\n","with open(config_path, \"r\") as file:\n","    config = yaml.safe_load(file)\n","\n","# 불러온 config 파일의 전체 내용을 확인합니다.\n","# pprint(loaded_config)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqrM5T1oLgAA"},"outputs":[],"source":["#config['wandb']['name'] = input()"]},{"cell_type":"markdown","metadata":{"id":"zUEIAm3TLgAA"},"source":["### 3) 데이터 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VboBviG6LgAA","outputId":"fa5be83f-fcee-4294-c458-9e53979688d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["../data/\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fname</th>\n","      <th>dialogue</th>\n","      <th>summary</th>\n","      <th>topic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>24909</th>\n","      <td>train_12455</td>\n","      <td>#Person1# 실례합니다 맨체스터 출신의 그린 씨이신가요 .\\n#Person2#...</td>\n","      <td>탄 링은 흰머리와 수염으로 쉽게 인식되는 그린 씨를 만나 호텔로 데려갈 예정입니다....</td>\n","      <td>누군가를 태우다</td>\n","    </tr>\n","    <tr>\n","      <th>24910</th>\n","      <td>train_12456</td>\n","      <td>#Person1# 이윙 씨가 ! 우리가 컨퍼런스 센터에 오후 ! 4시에 . 도착해야...</td>\n","      <td>#Person1#과 #Person2#는 이윙 씨가 늦지 않도록 요청했기 때문에 컨퍼...</td>\n","      <td>컨퍼런스 센터</td>\n","    </tr>\n","    <tr>\n","      <th>24911</th>\n","      <td>train_12457</td>\n","      <td>#Person1# 오늘 , 어떻게 도와드릴까요\\n#Person2# 차를 . 빌리고 ...</td>\n","      <td>#Person2#는 #Person1#의 도움으로 5일 동안 소형 차를 빌립니다.</td>\n","      <td>차 렌트</td>\n","    </tr>\n","    <tr>\n","      <th>24912</th>\n","      <td>train_12458</td>\n","      <td>#Person1# 오늘 , 좀 ! 행복해 : : 보이지 : 않아 ? 무슨 일 , 있...</td>\n","      <td>#Person2#의 엄마가 일자리를 잃었다. #Person2#는 엄마가 우울해하지 ...</td>\n","      <td>실직</td>\n","    </tr>\n","    <tr>\n","      <th>24913</th>\n","      <td>train_12459</td>\n","      <td>#Person1# 엄마 ; ; 다음 토요일에 : 이 , ! 삼촌네 , 가족을 , 방...</td>\n","      <td>#Person1#은 다음 토요일에 이 삼촌네를 방문할 때 가방을 어떻게 싸야 할지 ...</td>\n","      <td>짐 싸기</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             fname                                           dialogue  \\\n","24909  train_12455  #Person1# 실례합니다 맨체스터 출신의 그린 씨이신가요 .\\n#Person2#...   \n","24910  train_12456  #Person1# 이윙 씨가 ! 우리가 컨퍼런스 센터에 오후 ! 4시에 . 도착해야...   \n","24911  train_12457  #Person1# 오늘 , 어떻게 도와드릴까요\\n#Person2# 차를 . 빌리고 ...   \n","24912  train_12458  #Person1# 오늘 , 좀 ! 행복해 : : 보이지 : 않아 ? 무슨 일 , 있...   \n","24913  train_12459  #Person1# 엄마 ; ; 다음 토요일에 : 이 , ! 삼촌네 , 가족을 , 방...   \n","\n","                                                 summary     topic  \n","24909  탄 링은 흰머리와 수염으로 쉽게 인식되는 그린 씨를 만나 호텔로 데려갈 예정입니다....  누군가를 태우다  \n","24910  #Person1#과 #Person2#는 이윙 씨가 늦지 않도록 요청했기 때문에 컨퍼...   컨퍼런스 센터  \n","24911       #Person2#는 #Person1#의 도움으로 5일 동안 소형 차를 빌립니다.      차 렌트  \n","24912  #Person2#의 엄마가 일자리를 잃었다. #Person2#는 엄마가 우울해하지 ...        실직  \n","24913  #Person1#은 다음 토요일에 이 삼촌네를 방문할 때 가방을 어떻게 싸야 할지 ...      짐 싸기  "]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["# config에 저장된 데이터 경로를 통해 train과 validation data를 불러옵니다.\n","data_path = config['general']['data_path']\n","print(data_path)\n","\n","# train data의 구조와 내용을 확인합니다.\n","train_df = pd.read_csv(os.path.join(data_path, train))\n","train_df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHhJ0_qsLgAB","outputId":"912c4e7a-68d8-42c2-9049-f7f66199b3bb"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fname</th>\n","      <th>dialogue</th>\n","      <th>summary</th>\n","      <th>topic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>494</th>\n","      <td>dev_495</td>\n","      <td>#Person1#: 이제 새해가 되어서 새로운 시작을 하려고 결심했어. \\r\\n#P...</td>\n","      <td>#Person1#은 새해에 금연을 하고 커밍아웃하기로 결정했습니다. #Person2...</td>\n","      <td>새해</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>dev_496</td>\n","      <td>#Person1#: 너, 조랑 결혼했지? \\r\\n#Person2#: 조? 무슨 말인...</td>\n","      <td>#Person1#은 #Person2#가 조와 결혼했다고 생각했다. #Person2#...</td>\n","      <td>사랑에 빠지다</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>dev_497</td>\n","      <td>#Person1#: 무엇을 도와드릴까요, 부인?\\r\\n#Person2#: 몇 주 동...</td>\n","      <td>#Person2#의 차에서 이상한 소리가 납니다. #Person1#는 브레이크를 교...</td>\n","      <td>소음</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>dev_498</td>\n","      <td>#Person1#: 안녕하세요, 아마존 고객 서비스입니다. 무엇을 도와드릴까요?\\n...</td>\n","      <td>#Person2#님이 아마존 고객 서비스에 전화하여 아마존에서 받은 책에 한 페이지...</td>\n","      <td>빠진 페이지</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>dev_499</td>\n","      <td>#Person1#: 여름이 다 되어간다는 게 믿기지 않아.\\r\\n#Person2#:...</td>\n","      <td>#Person2#는 #Person1#에게 여름 휴가 동안 파티를 도와주는 회사에서 ...</td>\n","      <td>여름 휴가</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       fname                                           dialogue  \\\n","494  dev_495  #Person1#: 이제 새해가 되어서 새로운 시작을 하려고 결심했어. \\r\\n#P...   \n","495  dev_496  #Person1#: 너, 조랑 결혼했지? \\r\\n#Person2#: 조? 무슨 말인...   \n","496  dev_497  #Person1#: 무엇을 도와드릴까요, 부인?\\r\\n#Person2#: 몇 주 동...   \n","497  dev_498  #Person1#: 안녕하세요, 아마존 고객 서비스입니다. 무엇을 도와드릴까요?\\n...   \n","498  dev_499  #Person1#: 여름이 다 되어간다는 게 믿기지 않아.\\r\\n#Person2#:...   \n","\n","                                               summary    topic  \n","494  #Person1#은 새해에 금연을 하고 커밍아웃하기로 결정했습니다. #Person2...       새해  \n","495  #Person1#은 #Person2#가 조와 결혼했다고 생각했다. #Person2#...  사랑에 빠지다  \n","496  #Person2#의 차에서 이상한 소리가 납니다. #Person1#는 브레이크를 교...       소음  \n","497  #Person2#님이 아마존 고객 서비스에 전화하여 아마존에서 받은 책에 한 페이지...   빠진 페이지  \n","498  #Person2#는 #Person1#에게 여름 휴가 동안 파티를 도와주는 회사에서 ...    여름 휴가  "]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["# validation data의 구조와 내용을 확인합니다.\n","val_df = pd.read_csv(os.path.join(data_path,'dev.csv'))\n","val_df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crwTs06PLgAB","outputId":"35a051e7-2b95-478a-d47c-1078e3adad95"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fname</th>\n","      <th>dialogue</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>494</th>\n","      <td>test_495</td>\n","      <td>#Person1#: 헤이, 찰리, 학교 끝나고 우리 집에 와서 나랑 비디오 게임 할...</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>test_496</td>\n","      <td>#Person1#: 어떻게 컨트리 음악에 관심을 가지게 되었나요?\\r\\n#Perso...</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>test_497</td>\n","      <td>#Person1#: 실례합니다, 앨리스. 이곳을 사용해본 적이 없는데, 기계를 어떻...</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>test_498</td>\n","      <td>#Person1#: 매튜? 안녕!\\r\\n#Person2#: 스티브! 오랜만이네! 얼...</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>test_499</td>\n","      <td>#Person1#: 헤이, 벳시, 좋은 소식 들었어?\\n#Person2#: 아니, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        fname                                           dialogue\n","494  test_495  #Person1#: 헤이, 찰리, 학교 끝나고 우리 집에 와서 나랑 비디오 게임 할...\n","495  test_496  #Person1#: 어떻게 컨트리 음악에 관심을 가지게 되었나요?\\r\\n#Perso...\n","496  test_497  #Person1#: 실례합니다, 앨리스. 이곳을 사용해본 적이 없는데, 기계를 어떻...\n","497  test_498  #Person1#: 매튜? 안녕!\\r\\n#Person2#: 스티브! 오랜만이네! 얼...\n","498  test_499  #Person1#: 헤이, 벳시, 좋은 소식 들었어?\\n#Person2#: 아니, ..."]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["# validation data의 구조와 내용을 확인합니다.\n","test_df = pd.read_csv(os.path.join(data_path,'test.csv'))\n","test_df.tail()"]},{"cell_type":"markdown","metadata":{"id":"BRhRK0p0LgAB"},"source":["### 4) 데이터 증강"]},{"cell_type":"markdown","metadata":{"id":"M7pxQ4wNLgAB"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8RER6koSLgAC","outputId":"3490deeb-f145-48fd-d6fb-53c17fc31b22"},"outputs":[{"ename":"NameError","evalue":"name '멈춤' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43m멈춤\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#!pip install koeda\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name '멈춤' is not defined"]}],"source":["멈춤\n","#!pip install koeda\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y72EsE8nLgAC","outputId":"add56bb2-ee2d-4763-cd69-a0d37cd0a741"},"outputs":[{"name":"stdout","output_type":"stream","text":["아버지가 응접실에 들어가신다\n","['아버지가 독방에 들어가신다', '아버지가 안방에 들어가신다']\n"]}],"source":["### 깃허브 예시\n","\n","from koeda import EDA\n","\n","eda = EDA(morpheme_analyzer=\"Okt\", alpha_sr=0.3, alpha_ri=0.3, alpha_rs=0.3, prob_rd=0.3)\n","\n","text = \"아버지가 방에 들어가신다\"\n","\n","result = eda(text)\n","print(result)\n","# 아버지가 정실에 들어가신다\n","\n","result = eda(text, p=(0.9, 0.9, 0.9, 0.9), repetition=2)\n","print(result)\n","# ['아버지가 객실 아빠 안방 방에 정실 들어가신다', '아버지가 탈의실 방 휴게실 에 안방 탈의실 들어가신다']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TKurC6_2LgAC","outputId":"932bd482-62e0-4635-c888-cf1866d28cad"},"outputs":[{"name":"stdout","output_type":"stream","text":["어머니 ; 가 ! 집을 나가신다\n","['어머니가 집을 , 나가신다', ': 어머니 , 가 집 ! 을 . 나가신다']\n"]}],"source":["### 깃허브 예시\n","\n","from koeda import AEDA\n","\n","aeda = AEDA(morpheme_analyzer=\"Okt\", punc_ratio=0.3, punctuations=[\".\", \",\", \"!\", \"?\", \";\", \":\"])\n","\n","text = \"어머니가 집을 나가신다\"\n","\n","result = aeda(text)\n","print(result)\n","# 어머니가 ! 집을 , 나가신다\n","\n","result = aeda(text, p=0.9, repetition=2)\n","print(result)\n","# ['! 어머니가 ! 집 ; 을 ? 나가신다', '. 어머니 ? 가 . 집 , 을 , 나가신다']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDzfTsHhLgAC","outputId":"54c55eb8-7720-465c-f4de-301f33554d4d"},"outputs":[{"name":"stdout","output_type":"stream","text":["#Person1#: 헤이, 지미. 나중에 운동하러 가자.\n","#Person2#: 그래. 몇 시에 갈까?\n","#Person1#: 3시 30분 어때?\n","#Person2#: 좋아. 오늘은 다리와 팔목을 운동하자.\n","#Person1#: 헤이. 나 방금 농구를 했는데, 다리가 좀 아파. 오늘은 팔과 배를 운동하자.\n","#Person2#: 나는 주간 스케줄을 따르고 있어. 너 때문에 모든 게 망가지고 있어.\n","#Person1#: 제발. 우리는 단지 두 날을 바꾸는 거야. 금요일에 다리를 할 수 있어.\n","#Person2#: 알았어. 그럼 3시 30분에 헬스장에서 만나자.\n"]}],"source":["#for i in range(len(train_df)):\n","print(val_df['dialogue'][1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8i0eVtftLgAC","outputId":"326f2491-7eb2-4169-b7fd-b108ffabd852"},"outputs":[{"name":"stdout","output_type":"stream","text":["['#', 'Person1', '#', ':', '헤이', ',', '지미', '.', '나중에', '운동하러', '가자', '.', '#', 'Person2', '#', ':', '그래', '.', '몇', '시에', '갈까', '?', '#', 'Person1', '#', ':', '3시', '30분', '어때', '?', '#', 'Person2', '#', ':', '좋아', '.', '오늘은', '다리와', '팔목을', '운동하자', '.', '#', 'Person1', '#', ':', '헤이', '.', '나', '방금', '농구를', '했는데', ',', '다리가', '좀', '아파', '.', '오늘은', '팔과', '배를', '운동하자', '.', '#', 'Person2', '#', ':', '나는', '주간', '스케줄을', '따르고', '있어', '.', '너', '때문에', '모든', '게', '망가지고', '있어', '.', '#', 'Person1', '#', ':', '제발', '.', '우리는', '단지', '두', '날을', '바꾸는', '거야', '.', '금요일에', '다리를', '할', '수', '있어', '.', '#', 'Person2', '#', ':', '알았어', '.', '그럼', '3시', '30분에', '헬스장에서', '만나자', '.']\n","원문: #Person1#: 헤이, 지미. 나중에 운동하러 가자.\n","#Person2#: 그래. 몇 시에 갈까?\n","#Person1#: 3시 30분 어때?\n","#Person2#: 좋아. 오늘은 다리와 팔목을 운동하자.\n","#Person1#: 헤이. 나 방금 농구를 했는데, 다리가 좀 아파. 오늘은 팔과 배를 운동하자.\n","#Person2#: 나는 주간 스케줄을 따르고 있어. 너 때문에 모든 게 망가지고 있어.\n","#Person1#: 제발. 우리는 단지 두 날을 바꾸는 거야. 금요일에 다리를 할 수 있어.\n","#Person2#: 알았어. 그럼 3시 30분에 헬스장에서 만나자.\n","AEDA 적용된 문장: # . Person1 # : 헤이 ? , 지미 . ; ! 나중에 운동하러 ? 가자 : . # Person2 # : 그래 ; . . ! 몇 시에 갈까 ? ? , # ; Person1 ? # : 3시 30분 어때 ? # Person2 # ; : 좋아 . 오늘은 ; 다리와 ; 팔목을 : 운동하자 . . : # Person1 # ? ; : ? 헤이 . 나 방금 농구를 했는데 ; , ! 다리가 . 좀 아파 . 오늘은 팔과 배를 운동하자 . # Person2 # : ; ; 나는 , 주간 스케줄을 따르고 있어 . : 너 때문에 모든 게 망가지고 . 있어 ! ! . # Person1 . # ? : : 제발 . 우리는 ; 단지 두 날을 바꾸는 ! 거야 . , 금요일에 다리를 ; 할 : 수 있어 . # Person2 # : 알았어 : , . 그럼 3시 , 30분에 ! 헬스장에서 . 만나자 .\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt_tab to\n","[nltk_data]     /data/ephemeral/home/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["import random\n","import nltk\n","nltk.download('punkt_tab')\n","from nltk.tokenize import word_tokenize\n","\n","# 랜덤 시드를 설정하여 재현성을 확보\n","random.seed(1)\n","\n","# 구두점 리스트 (AEDA에서 사용할 구두점 기호들)\n","PUNCTUATIONS = ['.', ',', '!', '?', ';', ':']\n","\n","# AEDA 증강 함수\n","def aeda(sentence, punctuations=PUNCTUATIONS, num_punct=3, special_tokens=None):\n","    # 문장을 단어 단위로 토큰화\n","    words = word_tokenize(sentence)\n","\n","    # Special tokens을 보호하기 위해 특별히 지정된 토큰을 제외한 나머지 단어들에 대해 구두점 삽입\n","    new_words = []\n","    print(words)\n","    for word in words:\n","        if special_tokens and word in special_tokens:\n","            # Special tokens은 그대로 유지\n","            print(word)\n","            new_words.append(word)\n","        else:\n","            # 일반 단어일 경우 구두점 삽입 기회를 부여\n","            new_words.append(word)\n","            # 주어진 수만큼 구두점을 랜덤 위치에 삽입\n","            for _ in range(num_punct):\n","                if random.random() > 0.8:  # 50% 확률로 구두점 삽입\n","                    new_words.append(random.choice(punctuations))\n","\n","    # 증강된 문장을 합쳐서 반환\n","    return ' '.join(new_words)\n","\n","# 예시 문장과 Special tokens\n","sentence = val_df['dialogue'][1]\n","special_tokens = ['#Person1#',     #76737\n","                  '#Person2#',     #70211\n","                  '#Person3#',     #452\n","                  '#Person4#',     #41\n","                  '#Person5#',     #5\n","                  '#Person6#',     #9\n","                  '#Person7#',     #3\n","                  '#SSN#',         #3\n","                  '#PhoneNumber#', #203\n","                  '#Address#',     #45\n","                  '#Email#',       #17\n","                  '#CarNumber#',   #6\n","                  '#CardNumber#'   #10\n","                  '#DateOfBirth#', #8\n","                  '#PassportNumber#']  #7\n","\n","# AEDA 적용 (special tokens을 보호)\n","augmented_sentence = aeda(sentence, num_punct=2, special_tokens=special_tokens)\n","\n","# 결과 출력\n","print(f\"원문: {sentence}\")\n","print(f\"AEDA 적용된 문장: {augmented_sentence}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3EpMnWe6LgAC","outputId":"6200d33c-9378-42cc-8ffd-706739a62987"},"outputs":[{"name":"stdout","output_type":"stream","text":["원문:\n","#Person1#: 헤이, 지미. 나중에 운동하러 가자.\n","#Person2#: 그래. 몇 시에 갈까?\n","#Person1#: 3시 30분 어때?\n","#Person2#: 좋아. 오늘은 다리와 팔목을 운동하자.\n","#Person1#: 헤이. 나 방금 농구를 했는데, 다리가 좀 아파. 오늘은 팔과 배를 운동하자.\n","#Person2#: 나는 주간 스케줄을 따르고 있어. 너 때문에 모든 게 망가지고 있어.\n","#Person1#: 제발. 우리는 단지 두 날을 바꾸는 거야. 금요일에 다리를 할 수 있어.\n","#Person2#: 알았어. 그럼 3시 30분에 헬스장에서 만나자.\n","\n","AEDA 적용된 문장:\n","#Person1# 헤이 . 지미 나중에 운동하러 . 가자 ?\n","#Person2# 그래 . 몇 ; 시에 ! 갈까\n","#Person1# 3시 ? 30분 : 어때\n","#Person2# 좋아 오늘은 다리와 팔목을 ; 운동하자 .\n","#Person1# 헤이 ! 나 : 방금 ; 농구를 : 했는데 ? 다리가 ; 좀 ? 아파 ? 오늘은 : 팔과 . 배를 ; 운동하자 !\n","#Person2# 나는 주간 ; 스케줄을 따르고 , 있어 , 너 때문에 ! 모든 게 . 망가지고 : 있어 ,\n","#Person1# 제발 우리는 ; 단지 ? 두 날을 바꾸는 거야 금요일에 다리를 ; ! 할 . 수 있어 ,\n","#Person2# 알았어 , 그럼 3시 30분에 헬스장에서 ? ? 만나자\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt_tab to\n","[nltk_data]     /data/ephemeral/home/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["import random\n","import nltk\n","nltk.download('punkt_tab')\n","#from nltk.tokenize import word_tokenize\n","from nltk.tokenize import RegexpTokenizer\n","tokenizer = RegexpTokenizer(r'#\\w+#|\\w+')\n","\n","# 랜덤 시드를 설정하여 재현성을 확보\n","random.seed(1)\n","\n","# 구두점 리스트 (AEDA에서 사용할 구두점 기호들)\n","PUNCTUATIONS = ['.', ',', '!', '?', ';', ':']\n","\n","# AEDA 증강 함수 (줄바꿈을 유지)\n","def aeda_with_newlines(text, punctuations=PUNCTUATIONS, num_punct=3, special_tokens=None):\n","    # 줄바꿈을 기준으로 문장 분리\n","    lines = text.split('\\n')\n","    augmented_lines = []\n","\n","    for line in lines:\n","        if line.strip():  # 빈 줄은 제외\n","            words = tokenizer.tokenize(line)\n","            new_words = []\n","            for word in words:\n","                if special_tokens and word in special_tokens:\n","                    # Special tokens은 그대로 유지\n","                    new_words.append(word)\n","                else:\n","                    # 일반 단어일 경우 구두점 삽입 기회를 부여\n","                    new_words.append(word)\n","                    for _ in range(num_punct):\n","                        if random.random() > 0.7:  # 50% 확률로 구두점 삽입\n","                            new_words.append(random.choice(punctuations))\n","            augmented_lines.append(' '.join(new_words))\n","        else:\n","            # 빈 줄은 그대로 유지\n","            augmented_lines.append('')\n","\n","    # 줄바꿈을 다시 추가하여 원래 형식으로 반환\n","    return '\\n'.join(augmented_lines)\n","\n","# 예시 텍스트 (줄바꿈 포함)와 Special tokens\n","text = val_df['dialogue'][1]\n","special_tokens = ['#Person1#',     #76737\n","                  '#Person2#',     #70211\n","                  '#Person3#',     #452\n","                  '#Person4#',     #41\n","                  '#Person5#',     #5\n","                  '#Person6#',     #9\n","                  '#Person7#',     #3\n","                  '#SSN#',         #3\n","                  '#PhoneNumber#', #203\n","                  '#Address#',     #45\n","                  '#Email#',       #17\n","                  '#CarNumber#',   #6\n","                  '#CardNumber#'   #10\n","                  '#DateOfBirth#', #8\n","                  '#PassportNumber#']  #7\n","\n","# AEDA 적용 (줄바꿈 유지)\n","augmented_text = aeda_with_newlines(text, num_punct=2, special_tokens=special_tokens)\n","\n","# 결과 출력\n","print(f\"원문:\\n{sentence}\")\n","print(f\"\\nAEDA 적용된 문장:\\n{augmented_text}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gdoJRpZtLgAD"},"outputs":[],"source":["aeda_train_df = []\n","for i in range(len(train_df)):\n","    text = train_df['dialogue'][i]\n","    text = aeda_with_newlines(text, num_punct=2, special_tokens=special_tokens)\n","    aeda_train_df.append(text)\n","    #print(text)\n","    #print(aeda_train[i])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wSUdcE3wLgAD"},"outputs":[],"source":["aeda_train_df = pd.DataFrame(aeda_train_df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A2MgIj9ULgAD","outputId":"00fbc6dd-34d1-4afe-d059-65d89c88b088"},"outputs":[{"data":{"text/plain":["Index(['fname', 'dialogue', 'summary', 'topic'], dtype='object')"]},"execution_count":285,"metadata":{},"output_type":"execute_result"}],"source":["train_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3iuftjuPLgAD","outputId":"f9199c98-4993-4ed6-d35f-2b039c2bc50d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fname</th>\n","      <th>dialogue</th>\n","      <th>summary</th>\n","      <th>topic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>train_0</td>\n","      <td>#Person1# 안녕하세요 스미스씨 저는 : 호킨스 ? . 의사입니다 오늘 ! ;...</td>\n","      <td>스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니...</td>\n","      <td>건강검진 받기</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>train_1</td>\n","      <td>#Person1# 안녕하세요 : 파커 부인 어떻게 지내셨나요\\n#Person2# 안...</td>\n","      <td>파커 부인이 리키를 데리고 백신 접종을 하러 갔다. 피터스 박사는 기록을 확인한 후...</td>\n","      <td>백신</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>train_2</td>\n","      <td>#Person1# 실례합니다 열쇠 . 한 묶음 보셨나요\\n#Person2# 어떤 종...</td>\n","      <td>#Person1#은 열쇠 한 묶음을 찾고 있고, 그것을 찾기 위해 #Person2#...</td>\n","      <td>열쇠 찾기</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>train_3</td>\n","      <td>#Person1# 왜 너는 ? . 여자친구가 있다는 ; 걸 말해주지 않았어\\n#Pe...</td>\n","      <td>#Person1#은 #Person2#가 여자친구가 있고 그녀와 결혼할 것이라는 사실...</td>\n","      <td>여자친구가 있다</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>train_4</td>\n","      <td>#Person1# 안녕 , 숙녀분들 . , 오늘 ? 밤 당신들은 정말 , 멋져 보여...</td>\n","      <td>말릭이 니키에게 춤을 요청한다. 말릭이 발을 밟는 것을 신경 쓰지 않는다면 니키는 ...</td>\n","      <td>댄스</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>12452</th>\n","      <td>train_12455</td>\n","      <td>#Person1# 실례합니다 맨체스터 출신의 그린 씨이신가요 .\\n#Person2#...</td>\n","      <td>탄 링은 흰머리와 수염으로 쉽게 인식되는 그린 씨를 만나 호텔로 데려갈 예정입니다....</td>\n","      <td>누군가를 태우다</td>\n","    </tr>\n","    <tr>\n","      <th>12453</th>\n","      <td>train_12456</td>\n","      <td>#Person1# 이윙 씨가 ! 우리가 컨퍼런스 센터에 오후 ! 4시에 . 도착해야...</td>\n","      <td>#Person1#과 #Person2#는 이윙 씨가 늦지 않도록 요청했기 때문에 컨퍼...</td>\n","      <td>컨퍼런스 센터</td>\n","    </tr>\n","    <tr>\n","      <th>12454</th>\n","      <td>train_12457</td>\n","      <td>#Person1# 오늘 , 어떻게 도와드릴까요\\n#Person2# 차를 . 빌리고 ...</td>\n","      <td>#Person2#는 #Person1#의 도움으로 5일 동안 소형 차를 빌립니다.</td>\n","      <td>차 렌트</td>\n","    </tr>\n","    <tr>\n","      <th>12455</th>\n","      <td>train_12458</td>\n","      <td>#Person1# 오늘 , 좀 ! 행복해 : : 보이지 : 않아 ? 무슨 일 , 있...</td>\n","      <td>#Person2#의 엄마가 일자리를 잃었다. #Person2#는 엄마가 우울해하지 ...</td>\n","      <td>실직</td>\n","    </tr>\n","    <tr>\n","      <th>12456</th>\n","      <td>train_12459</td>\n","      <td>#Person1# 엄마 ; ; 다음 토요일에 : 이 , ! 삼촌네 , 가족을 , 방...</td>\n","      <td>#Person1#은 다음 토요일에 이 삼촌네를 방문할 때 가방을 어떻게 싸야 할지 ...</td>\n","      <td>짐 싸기</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>12457 rows × 4 columns</p>\n","</div>"],"text/plain":["             fname                                           dialogue  \\\n","0          train_0  #Person1# 안녕하세요 스미스씨 저는 : 호킨스 ? . 의사입니다 오늘 ! ;...   \n","1          train_1  #Person1# 안녕하세요 : 파커 부인 어떻게 지내셨나요\\n#Person2# 안...   \n","2          train_2  #Person1# 실례합니다 열쇠 . 한 묶음 보셨나요\\n#Person2# 어떤 종...   \n","3          train_3  #Person1# 왜 너는 ? . 여자친구가 있다는 ; 걸 말해주지 않았어\\n#Pe...   \n","4          train_4  #Person1# 안녕 , 숙녀분들 . , 오늘 ? 밤 당신들은 정말 , 멋져 보여...   \n","...            ...                                                ...   \n","12452  train_12455  #Person1# 실례합니다 맨체스터 출신의 그린 씨이신가요 .\\n#Person2#...   \n","12453  train_12456  #Person1# 이윙 씨가 ! 우리가 컨퍼런스 센터에 오후 ! 4시에 . 도착해야...   \n","12454  train_12457  #Person1# 오늘 , 어떻게 도와드릴까요\\n#Person2# 차를 . 빌리고 ...   \n","12455  train_12458  #Person1# 오늘 , 좀 ! 행복해 : : 보이지 : 않아 ? 무슨 일 , 있...   \n","12456  train_12459  #Person1# 엄마 ; ; 다음 토요일에 : 이 , ! 삼촌네 , 가족을 , 방...   \n","\n","                                                 summary     topic  \n","0      스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니...   건강검진 받기  \n","1      파커 부인이 리키를 데리고 백신 접종을 하러 갔다. 피터스 박사는 기록을 확인한 후...        백신  \n","2      #Person1#은 열쇠 한 묶음을 찾고 있고, 그것을 찾기 위해 #Person2#...     열쇠 찾기  \n","3      #Person1#은 #Person2#가 여자친구가 있고 그녀와 결혼할 것이라는 사실...  여자친구가 있다  \n","4      말릭이 니키에게 춤을 요청한다. 말릭이 발을 밟는 것을 신경 쓰지 않는다면 니키는 ...        댄스  \n","...                                                  ...       ...  \n","12452  탄 링은 흰머리와 수염으로 쉽게 인식되는 그린 씨를 만나 호텔로 데려갈 예정입니다....  누군가를 태우다  \n","12453  #Person1#과 #Person2#는 이윙 씨가 늦지 않도록 요청했기 때문에 컨퍼...   컨퍼런스 센터  \n","12454       #Person2#는 #Person1#의 도움으로 5일 동안 소형 차를 빌립니다.      차 렌트  \n","12455  #Person2#의 엄마가 일자리를 잃었다. #Person2#는 엄마가 우울해하지 ...        실직  \n","12456  #Person1#은 다음 토요일에 이 삼촌네를 방문할 때 가방을 어떻게 싸야 할지 ...      짐 싸기  \n","\n","[12457 rows x 4 columns]"]},"execution_count":286,"metadata":{},"output_type":"execute_result"}],"source":["tmp = train_df.copy()\n","tmp['dialogue'] = aeda_train_df\n","tmp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Kn_k4dWLgAE"},"outputs":[],"source":["tmp.to_csv(\"./aeda_train.csv\", index=False)  # 결과를 CSV 파일로 저장합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMdQ6b3OLgAE","outputId":"2d29607c-ce74-47cb-e1a1-006dd81a069d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fname</th>\n","      <th>dialogue</th>\n","      <th>summary</th>\n","      <th>topic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>train_0</td>\n","      <td>#Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나...</td>\n","      <td>스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니...</td>\n","      <td>건강검진 받기</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>train_1</td>\n","      <td>#Person1#: 안녕하세요, 파커 부인, 어떻게 지내셨나요?\\n#Person2#...</td>\n","      <td>파커 부인이 리키를 데리고 백신 접종을 하러 갔다. 피터스 박사는 기록을 확인한 후...</td>\n","      <td>백신</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>train_2</td>\n","      <td>#Person1#: 실례합니다, 열쇠 한 묶음 보셨나요?\\n#Person2#: 어떤...</td>\n","      <td>#Person1#은 열쇠 한 묶음을 찾고 있고, 그것을 찾기 위해 #Person2#...</td>\n","      <td>열쇠 찾기</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>train_3</td>\n","      <td>#Person1#: 왜 너는 여자친구가 있다는 걸 말해주지 않았어?\\n#Person...</td>\n","      <td>#Person1#은 #Person2#가 여자친구가 있고 그녀와 결혼할 것이라는 사실...</td>\n","      <td>여자친구가 있다</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>train_4</td>\n","      <td>#Person1#: 안녕, 숙녀분들! 오늘 밤 당신들은 정말 멋져 보여. 이 춤을 ...</td>\n","      <td>말릭이 니키에게 춤을 요청한다. 말릭이 발을 밟는 것을 신경 쓰지 않는다면 니키는 ...</td>\n","      <td>댄스</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>12452</th>\n","      <td>train_12455</td>\n","      <td>#Person1# 실례합니다 맨체스터 출신의 그린 씨이신가요 .\\n#Person2#...</td>\n","      <td>탄 링은 흰머리와 수염으로 쉽게 인식되는 그린 씨를 만나 호텔로 데려갈 예정입니다....</td>\n","      <td>누군가를 태우다</td>\n","    </tr>\n","    <tr>\n","      <th>12453</th>\n","      <td>train_12456</td>\n","      <td>#Person1# 이윙 씨가 ! 우리가 컨퍼런스 센터에 오후 ! 4시에 . 도착해야...</td>\n","      <td>#Person1#과 #Person2#는 이윙 씨가 늦지 않도록 요청했기 때문에 컨퍼...</td>\n","      <td>컨퍼런스 센터</td>\n","    </tr>\n","    <tr>\n","      <th>12454</th>\n","      <td>train_12457</td>\n","      <td>#Person1# 오늘 , 어떻게 도와드릴까요\\n#Person2# 차를 . 빌리고 ...</td>\n","      <td>#Person2#는 #Person1#의 도움으로 5일 동안 소형 차를 빌립니다.</td>\n","      <td>차 렌트</td>\n","    </tr>\n","    <tr>\n","      <th>12455</th>\n","      <td>train_12458</td>\n","      <td>#Person1# 오늘 , 좀 ! 행복해 : : 보이지 : 않아 ? 무슨 일 , 있...</td>\n","      <td>#Person2#의 엄마가 일자리를 잃었다. #Person2#는 엄마가 우울해하지 ...</td>\n","      <td>실직</td>\n","    </tr>\n","    <tr>\n","      <th>12456</th>\n","      <td>train_12459</td>\n","      <td>#Person1# 엄마 ; ; 다음 토요일에 : 이 , ! 삼촌네 , 가족을 , 방...</td>\n","      <td>#Person1#은 다음 토요일에 이 삼촌네를 방문할 때 가방을 어떻게 싸야 할지 ...</td>\n","      <td>짐 싸기</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>24914 rows × 4 columns</p>\n","</div>"],"text/plain":["             fname                                           dialogue  \\\n","0          train_0  #Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나...   \n","1          train_1  #Person1#: 안녕하세요, 파커 부인, 어떻게 지내셨나요?\\n#Person2#...   \n","2          train_2  #Person1#: 실례합니다, 열쇠 한 묶음 보셨나요?\\n#Person2#: 어떤...   \n","3          train_3  #Person1#: 왜 너는 여자친구가 있다는 걸 말해주지 않았어?\\n#Person...   \n","4          train_4  #Person1#: 안녕, 숙녀분들! 오늘 밤 당신들은 정말 멋져 보여. 이 춤을 ...   \n","...            ...                                                ...   \n","12452  train_12455  #Person1# 실례합니다 맨체스터 출신의 그린 씨이신가요 .\\n#Person2#...   \n","12453  train_12456  #Person1# 이윙 씨가 ! 우리가 컨퍼런스 센터에 오후 ! 4시에 . 도착해야...   \n","12454  train_12457  #Person1# 오늘 , 어떻게 도와드릴까요\\n#Person2# 차를 . 빌리고 ...   \n","12455  train_12458  #Person1# 오늘 , 좀 ! 행복해 : : 보이지 : 않아 ? 무슨 일 , 있...   \n","12456  train_12459  #Person1# 엄마 ; ; 다음 토요일에 : 이 , ! 삼촌네 , 가족을 , 방...   \n","\n","                                                 summary     topic  \n","0      스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니...   건강검진 받기  \n","1      파커 부인이 리키를 데리고 백신 접종을 하러 갔다. 피터스 박사는 기록을 확인한 후...        백신  \n","2      #Person1#은 열쇠 한 묶음을 찾고 있고, 그것을 찾기 위해 #Person2#...     열쇠 찾기  \n","3      #Person1#은 #Person2#가 여자친구가 있고 그녀와 결혼할 것이라는 사실...  여자친구가 있다  \n","4      말릭이 니키에게 춤을 요청한다. 말릭이 발을 밟는 것을 신경 쓰지 않는다면 니키는 ...        댄스  \n","...                                                  ...       ...  \n","12452  탄 링은 흰머리와 수염으로 쉽게 인식되는 그린 씨를 만나 호텔로 데려갈 예정입니다....  누군가를 태우다  \n","12453  #Person1#과 #Person2#는 이윙 씨가 늦지 않도록 요청했기 때문에 컨퍼...   컨퍼런스 센터  \n","12454       #Person2#는 #Person1#의 도움으로 5일 동안 소형 차를 빌립니다.      차 렌트  \n","12455  #Person2#의 엄마가 일자리를 잃었다. #Person2#는 엄마가 우울해하지 ...        실직  \n","12456  #Person1#은 다음 토요일에 이 삼촌네를 방문할 때 가방을 어떻게 싸야 할지 ...      짐 싸기  \n","\n","[24914 rows x 4 columns]"]},"execution_count":299,"metadata":{},"output_type":"execute_result"}],"source":["ex = pd.concat([train_df, tmp], axis=0)\n","ex"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wDrdcfjLgAE"},"outputs":[],"source":["ex.to_csv(\"/home/data/train_agu.csv\", index=False)  # 결과를 CSV 파일로 저장합니다."]},{"cell_type":"markdown","metadata":{"id":"rhKTnCeBLgAE"},"source":["## 1. 모델 트레이닝"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y23X8ih-LgAE","outputId":"a2b82452-74ce-4d94-f44a-3e9ee81d5cea"},"outputs":[{"name":"stdout","output_type":"stream","text":["---------- device : cuda:0 ----------\n","2.4.1+cu121\n"]}],"source":["# 사용할 device를 정의합니다. GPU가 사용 가능하면 'cuda:0', 그렇지 않으면 'cpu'를 사용합니다.\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('-' * 10, f'device : {device}', '-' * 10,)\n","print(torch.__version__)"]},{"cell_type":"markdown","metadata":{"id":"h58mxRU1LgAE"},"source":["### 1) 토크나이저와 모델 로드"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qBstOFR_LgAE"},"outputs":[],"source":["def load_tokenizer_and_model_for_train(config, device):\n","    print('-' * 10, 'Load tokenizer & model', '-' * 10,)\n","    print('-' * 10, f'Model Name : {config[\"general\"][\"model_name\"]}', '-' * 10,)\n","\n","     #모델 이름 불러오기:\n","    # config['general']['model_name']을 통해 사전 학습된 모델의 이름을 설정 파일에서 불러옵니다.\n","    # 이는 Hugging Face의 모델 허브에서 가져올 모델의 이름입니다.\n","    model_name = config['general']['model_name']\n","\n","    # BART 설정 로드:\n","    # BartConfig를 사용하여 지정된 모델 이름으로부터 BART의 설정을 불러옵니다.\n","    # 이 설정은 모델의 구조와 하이퍼파라미터를 정의합니다.\n","    bart_config = BartConfig().from_pretrained(model_name)\n","\n","    # 토크나이저 로드:\n","    # AutoTokenizer.from_pretrained를 사용하여 지정된 모델 이름으로부터 토크나이저를 불러옵니다.\n","    # 이 토크나이저는 텍스트 데이터를 토큰화하는 역할을 합니다.\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","    # 사전 학습된 모델 로드:\n","    # BartForConditionalGeneration.from_pretrained를 사용하여 사전 학습된 BART 모델을 불러옵니다.\n","    # 이 모델은 텍스트 생성, 요약 등의 작업을 수행하는 데 사용됩니다.\n","    generate_model = BartForConditionalGeneration.from_pretrained(model_name, config=bart_config)\n","\n","\n","    # 특수 토큰 추가:\n","    # special_tokens_dict를 사용하여 설정에서 정의한 특수 토큰을 토크나이저에 추가합니다.\n","    # 이는 예를 들어, 특정 인물이나 장소를 나타내는 토큰을 추가하는 등의 작업에 사용됩니다.\n","    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n","    tokenizer.add_special_tokens(special_tokens_dict)\n","    # 토크나이저에 추가된 토큰에 맞추어 모델의 토큰 임베딩 크기를 조정합니다.\n","    # 모델의 토큰 임베딩 크기를 업데이트된 토크나이저의 크기에 맞게 조정합니다.\n","    generate_model.resize_token_embeddings(len(tokenizer))\n","\n","\n","    # 모델을 지정된 디바이스로 이동:\n","    # 모델을 device 변수에 지정된 디바이스(GPU 또는 CPU)로 이동시킵니다.\n","    # 이를 통해 모델이 올바른 디바이스에서 실행되도록 합니다.\n","    generate_model.to(device)\n","\n","\n","    # 모델 설정 출력:\n","    # 로드된 모델의 설정을 출력하여, 모델의 구조와 하이퍼파라미터 등을 확인할 수 있습니다.\n","    print(generate_model.config)\n","\n","    print('-' * 10, 'Load tokenizer & model complete', '-' * 10,)\n","\n","    # 모델과 토크나이저 반환:\n","    return generate_model, tokenizer\n","\n","    # 최종적으로 불러온 모델과 토크나이저를 반환하여, 이후 학습 과정에서 사용할 수 있도록 준비합니다.\n","    # 이 함수는 학습을 위해 필요한 모델과 토크나이저를 설정 파일에 따라 자동으로 불러오고 준비해 줍니다.\n","    # 특수 토큰의 추가 및 모델 임베딩의 재구성도 함께 처리하여, 모델이 주어진 작업에 최적화될 수 있도록 돕습니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yC8Tpu35LgAE","outputId":"737b8046-5486-4a69-c918-cf58d0323c29"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","#######  사용할 모델과 토크나이저를 로드합니다.  ################################################################################\n","---------- Load tokenizer & model ----------\n","---------- Model Name : csebuetnlp/mT5_multilingual_XLSum ----------\n"]},{"name":"stderr","output_type":"stream","text":["/home/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","You are using a model of type mt5 to instantiate a model of type bart. This is not supported for all configurations of models and can yield errors.\n","/home/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  do_lower_case = self.original_tokenizer.basic_tokenizer.do_lower_case\n","/home/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(checkpoint_file, map_location=map_location)\n"]},{"ename":"ValueError","evalue":"The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[84], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 사용할 모델과 토크나이저를 로드합니다.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m#######  사용할 모델과 토크나이저를 로드합니다.  ################################################################################\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m generate_model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_tokenizer_and_model_for_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer special tokens : \u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer\u001b[38;5;241m.\u001b[39mspecial_tokens_map, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n","Cell \u001b[0;32mIn[83], line 23\u001b[0m, in \u001b[0;36mload_tokenizer_and_model_for_train\u001b[0;34m(config, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 사전 학습된 모델 로드:\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# BartForConditionalGeneration.from_pretrained를 사용하여 사전 학습된 BART 모델을 불러옵니다. \u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 이 모델은 텍스트 생성, 요약 등의 작업을 수행하는 데 사용됩니다.\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m generate_model \u001b[38;5;241m=\u001b[39m \u001b[43mBartForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbart_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 특수 토큰 추가:\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# special_tokens_dict를 사용하여 설정에서 정의한 특수 토큰을 토크나이저에 추가합니다. \u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 이는 예를 들어, 특정 인물이나 장소를 나타내는 토큰을 추가하는 등의 작업에 사용됩니다.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m special_tokens_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madditional_special_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecial_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n","File \u001b[0;32m/home/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3480\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3472\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3473\u001b[0m     (\n\u001b[1;32m   3474\u001b[0m         model,\n\u001b[1;32m   3475\u001b[0m         missing_keys,\n\u001b[1;32m   3476\u001b[0m         unexpected_keys,\n\u001b[1;32m   3477\u001b[0m         mismatched_keys,\n\u001b[1;32m   3478\u001b[0m         offload_index,\n\u001b[1;32m   3479\u001b[0m         error_msgs,\n\u001b[0;32m-> 3480\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3488\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3491\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3492\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantization_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQuantizationMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBITS_AND_BYTES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3496\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3498\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   3499\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n","File \u001b[0;32m/home/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3752\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3750\u001b[0m base_model_expected_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(model_to_load\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   3751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m expected_keys_not_prefixed \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m base_model_expected_keys \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m loaded_keys):\n\u001b[0;32m-> 3752\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3753\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe state dictionary of the model you are trying to load is corrupted. Are you sure it was \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3754\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperly saved?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3755\u001b[0m     )\n\u001b[1;32m   3756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3757\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m {k\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mitems()}\n","\u001b[0;31mValueError\u001b[0m: The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?"]}],"source":[" # 사용할 모델과 토크나이저를 로드합니다.\n","print(\"\\n#######  사용할 모델과 토크나이저를 로드합니다.  ################################################################################\")\n","generate_model, tokenizer = load_tokenizer_and_model_for_train(config, device)\n","print('-' * 10, \"tokenizer special tokens : \", tokenizer.special_tokens_map, '-' * 10)"]},{"cell_type":"markdown","metadata":{"id":"JGf8OPP4LgAF"},"source":["### 2) 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYnkzi0dLgAF"},"outputs":[],"source":["\n","# 데이터 전처리를 위한 클래스로, 데이터셋을 데이터프레임으로 변환하고 인코더와 디코더의 입력을 생성합니다.\n","class Preprocess:\n","    def __init__(self,\n","            bos_token: str,\n","            eos_token: str,\n","        ) -> None:\n","\n","        # 클래스 초기화 메서드입니다. 시작 토큰(bos_token)과 종료 토큰(eos_token)을 인스턴스 변수로 저장합니다.\n","        self.bos_token = bos_token\n","        self.eos_token = eos_token\n","\n","    @staticmethod\n","    # 파일 경로를 입력 받아 CSV 파일을 읽고, 필요한 컬럼들만 선택하여 데이터프레임으로 반환합니다.\n","    def make_set_as_df(file_path, is_train = True):\n","\n","        # is_train이 True면 학습용 데이터로, False면 테스트용 데이터로 처리합니다.\n","        if is_train:\n","            df = pd.read_csv(file_path)\n","            train_df = df[['fname','dialogue','summary']]\n","            return train_df\n","\n","        # 테스트 데이터인 경우, 'fname'과 'dialogue' 컬럼만 선택하여 반환합니다.\n","        else:\n","            df = pd.read_csv(file_path)\n","            test_df = df[['fname','dialogue']]\n","            return test_df\n","\n","\n","    # BART 모델의 입력과 출력을 생성하는 메서드입니다.\n","    def make_input(self, dataset, is_test=False):\n","\n","        # is_test가 True면 테스트 데이터를 위한 입력만 생성하고, False면 학습 데이터를 위한 입력과 출력을 생성합니다.\n","        if is_test:\n","            # 테스트 데이터의 경우, 인코더 입력과 디코더의 시작 토큰으로만 구성된 입력을 반환합니다.\n","            # 인코더 입력으로 'dialogue' 텍스트를 사용합니다.\n","            encoder_input = dataset['dialogue']\n","            # 디코더 입력은 시작 토큰으로만 구성합니다.\n","            decoder_input = [self.bos_token] * len(dataset['dialogue'])\n","\n","            return encoder_input.tolist(), list(decoder_input)\n","        else:\n","            # 학습 데이터의 경우, 인코더 입력, 디코더 입력, 디코더 출력을 모두 생성합니다.\n","            encoder_input = dataset['dialogue']  # 인코더 입력으로 'dialogue' 텍스트를 사용합니다.\n","\n","            # 디코더 입력은 시작 토큰(bos_token)과 요약 텍스트(summary)로 구성됩니다.\n","            decoder_input = dataset['summary'].apply(lambda x: self.bos_token + str(x))\n","\n","            # 디코더 출력은 요약 텍스트(summary)와 종료 토큰(eos_token)으로 구성됩니다.\n","            decoder_output = dataset['summary'].apply(lambda x: str(x) + self.eos_token)\n","\n","            # 리스트로 변환하여 반환합니다.\n","            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JZht48EHLgAF","outputId":"96b1c74b-bcd1-47e4-e551-5f6c2a39f784"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","#######  학습에 사용할 데이터셋을 전처리하고 로드합니다.  ########################################################################\n"]}],"source":["# 학습에 사용할 데이터셋을 전처리하고 로드합니다.\n","print(\"\\n#######  학습에 사용할 데이터셋을 전처리하고 로드합니다.  ########################################################################\")\n","# 시작 토큰(beginning of sentence)과 종료 토큰(end of sentence)을 설정합니다.\n","preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])\n"]},{"cell_type":"markdown","metadata":{"id":"z4oKS3KdLgAF"},"source":["### 3) 학습 및 검증 데이터셋 준비"]},{"cell_type":"markdown","metadata":{"id":"agKhJpNrLgAF"},"source":["#### 3-1) Train, Validation, Test 클래스 정의"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_MWqASoLgAI"},"outputs":[],"source":["# Train에 사용되는 Dataset 클래스를 정의합니다.\n","class DatasetForTrain(Dataset):\n","    def __init__(self, encoder_input, decoder_input, labels, len):\n","        # 학습 데이터셋 초기화 메서드입니다. 인코더 입력, 디코더 입력, 레이블, 데이터 길이를 저장합니다.\n","        self.encoder_input = encoder_input\n","        self.decoder_input = decoder_input\n","        self.labels = labels\n","        self.len = len\n","\n","    def __getitem__(self, idx):\n","        # 주어진 인덱스(idx)에 해당하는 데이터를 반환하는 메서드입니다.\n","\n","        # 인코더 입력 데이터에서 해당 인덱스의 데이터를 복사하여 `item` 딕셔너리에 저장합니다.\n","        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}  # item[input_ids], item[attention_mask]\n","\n","        # 디코더 입력 데이터에서 해당 인덱스의 데이터를 복사하여 `item2` 딕셔너리에 저장합니다.\n","        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}  # item2[input_ids], item2[attention_mask]\n","\n","        # `item2` 딕셔너리의 'input_ids'와 'attention_mask'를 각각 'decoder_input_ids'와 'decoder_attention_mask'로 이름을 변경합니다.\n","        item2['decoder_input_ids'] = item2['input_ids']\n","        item2['decoder_attention_mask'] = item2['attention_mask']\n","        item2.pop('input_ids')  # 'input_ids' 키 제거\n","        item2.pop('attention_mask')  # 'attention_mask' 키 제거\n","\n","        # `item` 딕셔너리에 디코더의 입력 정보를 추가합니다.\n","        item.update(item2)  # item[input_ids], item[attention_mask], item[decoder_input_ids], item[decoder_attention_mask]\n","\n","        # 레이블로 사용할 'input_ids' 값을 `item` 딕셔너리에 추가합니다.\n","        item['labels'] = self.labels['input_ids'][idx]  # item[input_ids], item[attention_mask], item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n","\n","        return item\n","\n","    def __len__(self):\n","        # 데이터셋의 길이를 반환하는 메서드입니다.\n","        return self.len\n","\n","\n","# Validation에 사용되는 Dataset 클래스를 정의합니다.\n","class DatasetForVal(Dataset):\n","    def __init__(self, encoder_input, decoder_input, labels, len):\n","        # 검증 데이터셋 초기화 메서드입니다. 학습 데이터셋과 동일한 구조로 정의됩니다.\n","        self.encoder_input = encoder_input\n","        self.decoder_input = decoder_input\n","        self.labels = labels\n","        self.len = len\n","\n","    def __getitem__(self, idx):\n","        # 주어진 인덱스(idx)에 해당하는 데이터를 반환하는 메서드입니다.\n","        # 학습 데이터셋과 동일하게 정의됩니다.\n","        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()} # item[input_ids], item[attention_mask]\n","        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()} # item2[input_ids], item2[attention_mask]\n","        item2['decoder_input_ids'] = item2['input_ids']\n","        item2['decoder_attention_mask'] = item2['attention_mask']\n","        item2.pop('input_ids')\n","        item2.pop('attention_mask')\n","        item.update(item2) #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask]\n","        item['labels'] = self.labels['input_ids'][idx] #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n","        return item\n","\n","    def __len__(self):\n","        # 데이터셋의 길이를 반환하는 메서드입니다.\n","        return self.len\n","\n","\n","# Test에 사용되는 Dataset 클래스를 정의합니다.\n","class DatasetForInference(Dataset):     # inference뜻 : 추론\n","    def __init__(self, encoder_input, test_id, len):\n","        # 테스트 데이터셋 초기화 메서드입니다. 인코더 입력, 테스트 ID, 데이터 길이를 저장합니다.\n","        self.encoder_input = encoder_input\n","        self.test_id = test_id\n","        self.len = len\n","\n","    def __getitem__(self, idx):\n","        # 주어진 인덱스(idx)에 해당하는 데이터를 반환하는 메서드입니다.\n","        # 인코더 입력 데이터에서 해당 인덱스의 데이터를 복사하여 `item` 딕셔너리에 저장합니다.\n","        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n","        item['ID'] = self.test_id[idx]\n","        return item\n","\n","    def __len__(self):\n","        # 데이터셋의 길이를 반환하는 메서드입니다.\n","        return self.len\n"]},{"cell_type":"markdown","metadata":{"id":"Qra3QPO3LgAI"},"source":["#### 3-2) Train, Validation 데이터셋 준비"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Z0w3_vGLgAJ"},"outputs":[],"source":["def prepare_train_dataset(config, preprocessor, data_path, tokenizer):\n","\n","    ### 데이터 로드 및 변환:\n","    #학습 및 검증 데이터 파일을 읽어 데이터프레임으로 변환합니다.\n","    #데이터프레임으로 변환된 데이터에서 대화(dialogue)와 요약(summary) 텍스트를 각각 학습 입력과 라벨로 사용합니다.\n","    # 데이터셋 경로를 지정합니다.\n","    train_file_path = os.path.join(data_path, train)  # 학습 데이터 파일 경로\n","    val_file_path = os.path.join(data_path, 'dev.csv')  # 검증 데이터 파일 경로\n","    # 학습(train)과 검증(validation) 데이터셋을 데이터프레임으로 변환합니다.\n","    train_data = preprocessor.make_set_as_df(train_file_path)  # 학습 데이터 로드\n","    val_data = preprocessor.make_set_as_df(val_file_path)  # 검증 데이터 로드\n","    # 로드된 학습 데이터와 라벨의 첫 번째 샘플을 출력합니다.\n","    print('-' * 150)\n","    print(f'train_data:\\n {train_data[\"dialogue\"][0]}')\n","    print(f'train_label:\\n {train_data[\"summary\"][0]}')\n","    # 로드된 검증 데이터와 라벨의 첫 번째 샘플을 출력합니다.\n","    print('-' * 150)\n","    print(f'val_data:\\n {val_data[\"dialogue\"][0]}')\n","    print(f'val_label:\\n {val_data[\"summary\"][0]}')\n","\n","\n","    ### 데이터 전처리:\n","    # Preprocess 클래스의 make_input 메서드를 사용하여 인코더 입력, 디코더 입력, 디코더 출력을 생성합니다.\n","    # 이 과정에서 BART 모델에 적합한 형태로 텍스트 데이터를 구성합니다.\n","    # 학습 데이터에 대해 인코더 입력, 디코더 입력, 디코더 출력을 생성합니다.\n","    encoder_input_train, decoder_input_train, decoder_output_train = preprocessor.make_input(train_data)\n","    # 검증 데이터에 대해 인코더 입력, 디코더 입력, 디코더 출력을 생성합니다.\n","    encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)\n","    print('-' * 10, 'Load data complete', '-' * 10)\n","\n","\n","    ### 토큰화:\n","    # 텍스트 데이터를 tokenizer를 사용하여 토큰화합니다. 토큰화 과정에서:\n","    # 텍스트를 토큰 ID로 변환합니다.\n","    # 필요한 경우, 패딩(padding=True), 최대 길이(max_length), 특수 토큰(add_special_tokens=True) 등을 추가합니다.\n","    # 반환된 데이터는 PyTorch 텐서(return_tensors=\"pt\")로 반환됩니다.\n","    # 학습 데이터의 인코더 입력을 토크나이저로 토큰화합니다.\n","    tokenized_encoder_inputs = tokenizer(\n","        encoder_input_train, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False\n","    )\n","    # 학습 데이터의 디코더 입력을 토크나이저로 토큰화합니다.\n","    tokenized_decoder_inputs = tokenizer(\n","        decoder_input_train, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False\n","    )\n","    # 학습 데이터의 디코더 출력을 토크나이저로 토큰화합니다.\n","    tokenized_decoder_ouputs = tokenizer(\n","        decoder_output_train, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False\n","    )\n","\n","\n","    ### 데이터셋 생성:\n","    # 학습 데이터와 검증 데이터를 각각 DatasetForTrain과 DatasetForVal 클래스로 감싸 데이터셋 객체를 생성합니다.\n","    # 이 객체들은 PyTorch의 DataLoader와 함께 사용되어 모델 학습 및 검증에 활용됩니다.\n","    # 학습용 데이터셋을 생성합니다.\n","    train_inputs_dataset = DatasetForTrain(\n","        tokenized_encoder_inputs, tokenized_decoder_inputs, tokenized_decoder_ouputs, len(encoder_input_train)\n","    )\n","    # 검증 데이터의 인코더 입력을 토크나이저로 토큰화합니다.\n","    val_tokenized_encoder_inputs = tokenizer(\n","        encoder_input_val, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False\n","    )\n","    # 검증 데이터의 디코더 입력을 토크나이저로 토큰화합니다.\n","    val_tokenized_decoder_inputs = tokenizer(\n","        decoder_input_val, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False\n","    )\n","    # 검증 데이터의 디코더 출력을 토크나이저로 토큰화합니다.\n","    val_tokenized_decoder_ouputs = tokenizer(\n","        decoder_output_val, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False\n","    )\n","    # 검증용 데이터셋을 생성합니다.\n","    val_inputs_dataset = DatasetForVal(\n","        val_tokenized_encoder_inputs, val_tokenized_decoder_inputs, val_tokenized_decoder_ouputs, len(encoder_input_val)\n","    )\n","    print('-' * 10, 'Make dataset complete', '-' * 10)\n","\n","\n","    ### 출력 및 반환:\n","    # 학습용 데이터셋과 검증용 데이터셋을 반환합니다. 이를 통해 모델 학습 과정에서 필요한 데이터를 제공하게 됩니다.\n","    return train_inputs_dataset, val_inputs_dataset\n","\n","\n","    # 이 함수는 모델 학습 및 검증을 위한 데이터 준비 과정에서 필요한\n","    # 모든 전처리, 토큰화, 데이터셋 생성을 자동으로 처리하여, 최종적으로 Dataset 객체를 반환합니다.\n","    # 이를 통해 모델 학습 및 검증을 효율적으로 수행할 수 있습니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cXp2-yC2LgAJ","outputId":"e58d2a70-ddd1-41bd-d275-2ae8978abeaa"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","#######  학습 및 검증 데이터셋을 준비합니다.  ###################################################################################\n"]},{"name":"stdout","output_type":"stream","text":["------------------------------------------------------------------------------------------------------------------------------------------------------\n","train_data:\n"," #Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\n","#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\n","#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\n","#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\n","#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\n","#Person2#: 알겠습니다.\n","#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\n","#Person2#: 네.\n","#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \n","#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\n","#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\n","#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n","train_label:\n"," 스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니다. 호킨스 의사는 스미스씨가 담배를 끊는 데 도움이 될 수 있는 수업과 약물에 대한 정보를 제공할 것입니다.\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","val_data:\n"," #Person1#: 안녕하세요, 오늘 하루 어떠셨어요? \n","#Person2#: 요즘 숨쉬기가 좀 힘들어요.\n","#Person1#: 최근에 감기 같은 것에 걸리신 적이 있나요?\n","#Person2#: 아니요, 감기는 아니에요. 그냥 숨을 쉴 때마다 가슴이 무겁게 느껴져요.\n","#Person1#: 알고 있는 알레르기가 있나요?\n","#Person2#: 아니요, 알고 있는 알레르기는 없어요.\n","#Person1#: 이런 증상이 항상 나타나나요, 아니면 활동할 때 주로 나타나나요?\n","#Person2#: 운동을 할 때 많이 나타나요.\n","#Person1#: 저는 당신을 폐 전문의에게 보내서 천식에 대한 검사를 받게 할 거예요.\n","#Person2#: 도와주셔서 감사합니다, 의사 선생님.\n","val_label:\n"," #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다. \n","---------- Load data complete ----------\n","---------- Make dataset complete ----------\n"]}],"source":["print(\"\\n#######  학습 및 검증 데이터셋을 준비합니다.  ###################################################################################\")\n","# 학습 및 검증 데이터셋을 준비합니다.\n","data_path = config['general']['data_path']  # 데이터 경로를 설정에서 가져옵니다.\n","train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config, preprocessor, data_path, tokenizer)\n"]},{"cell_type":"markdown","metadata":{"id":"lo5zvotnLgAJ"},"source":["### 4) Trainer 클래스 초기화"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U9bBQRu4LgAJ"},"outputs":[],"source":["def compute_metrics(config, tokenizer, pred):\n","    ### rouge = Rouge():\n","    # ROUGE 점수를 계산하기 위해 Rouge 클래스를 초기화합니다.\n","    # ROUGE는 주로 텍스트 요약의 품질을 평가할 때 사용되는 지표입니다.\n","    rouge = Rouge()\n","\n","\n","    ### pred.predictions 및 pred.label_ids:\n","    # predictions: 모델이 예측한 토큰 ID의 배열입니다.\n","    # label_ids: 실제 레이블(정답) 토큰 ID의 배열입니다.\n","    # 예측된 토큰 ID와 실제 레이블 ID를 가져옵니다.\n","    predictions = pred.predictions\n","    labels = pred.label_ids\n","\n","\n","    ### 패딩 토큰 처리:\n","    # 예측 값과 레이블에서 -100으로 표시된 패딩 토큰을 실제 패딩 토큰 ID로 교체하여 평가에서 패딩이 영향을 미치지 않도록 합니다.\n","    # 모델 출력 중 패딩 토큰을 의미하는 -100 값을 tokenizer의 패딩 토큰 ID로 변경합니다.\n","    predictions[predictions == -100] = tokenizer.pad_token_id\n","    labels[labels == -100] = tokenizer.pad_token_id\n","\n","\n","    ### 토큰 디코딩:\n","    # 토큰 ID 배열을 원래의 텍스트 문자열로 변환합니다.\n","    # batch_decode는 여러 개의 토큰 배열을 한꺼번에 디코딩합니다.\n","    # 예측된 토큰 ID를 텍스트로 디코딩합니다.\n","    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)\n","    # 실제 레이블의 토큰 ID도 텍스트로 디코딩합니다.\n","    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n","\n","\n","    ### 불필요한 토큰 제거:\n","    # 모델이 생성한 텍스트에서 사전에 정의된 불필요한 토큰을 제거하여 평가의 정확성을 높입니다.\n","    # 평가를 위해 불필요한 토큰들을 제거합니다.\n","    replaced_predictions = decoded_preds.copy()  # 예측된 텍스트 복사\n","    replaced_labels = labels.copy()  # 실제 레이블 텍스트 복사\n","    remove_tokens = config['inference']['remove_tokens']  # 제거할 토큰 목록을 config에서 가져옵니다.\n","    # 각 불필요한 토큰을 제거합니다.\n","    for token in remove_tokens:\n","        replaced_predictions = [sentence.replace(token, \" \") for sentence in replaced_predictions]\n","        replaced_labels = [sentence.replace(token, \" \") for sentence in replaced_labels]\n","\n","\n","    ### 출력:\n","    # 평가를 위해 디코딩된 예측 텍스트와 실제 레이블의 일부 샘플을 출력합니다.\n","    # 첫 번째, 두 번째, 세 번째 예측과 실제 레이블을 출력합니다.\n","    print('-' * 150)\n","    print(f\"PRED: {replaced_predictions[0]}\")\n","    print(f\"GOLD: {replaced_labels[0]}\")\n","    print('-' * 150)\n","    print(f\"PRED: {replaced_predictions[1]}\")\n","    print(f\"GOLD: {replaced_labels[1]}\")\n","    print('-' * 150)\n","    print(f\"PRED: {replaced_predictions[2]}\")\n","    print(f\"GOLD: {replaced_labels[2]}\")\n","\n","\n","    ### ROUGE 점수 계산:\n","    # replaced_predictions와 replaced_labels를 사용하여 ROUGE 점수를 계산합니다.\n","    # ROUGE-1, ROUGE-2, ROUGE-L 등의 F1 점수를 계산하여 반환합니다.\n","    # 최종적으로 ROUGE 점수를 계산합니다.\n","    results = rouge.get_scores(replaced_predictions, replaced_labels, avg=True)\n","\n","\n","    ### 결과 반환:\n","    # ROUGE 점수 중 F1-score를 추출하여 딕셔너리 형태로 반환합니다.\n","    result = {key: value[\"f\"] for key, value in results.items()}\n","    return result\n","\n","\n","    # 이 함수는 모델이 생성한 텍스트의 품질을 ROUGE 지표로 평가하여,\n","    # 모델 성능을 평가하는 데 중요한 역할을 합니다.\n","    # ROUGE 점수를 통해 텍스트 요약 또는 생성 모델의 정확성을 평가할 수 있습니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NOQI8oGDLgAJ"},"outputs":[],"source":["def load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset):\n","    print('-' * 10, 'Make training arguments', '-' * 10,)\n","\n","    ### Seq2SeqTrainingArguments 설정:\n","    # 학습과 관련된 다양한 설정값들을 정의하는 Seq2SeqTrainingArguments 객체를 생성합니다.\n","    # 학습률, 배치 크기, 에포크 수, 로그 저장 위치 등 다양한 하이퍼파라미터와 옵션들이 포함됩니다.\n","    # 학습을 위한 설정값들을 정의합니다.\n","    training_args = Seq2SeqTrainingArguments(\n","        output_dir=config['general']['output_dir'],                                     # 모델 출력 디렉터리\n","        overwrite_output_dir=config['training']['overwrite_output_dir'],                # 출력 디렉터리를 덮어쓸지 여부\n","        num_train_epochs=config['training']['num_train_epochs'],                        # 전체 학습 에포크 수\n","        learning_rate=config['training']['learning_rate'],                              # 학습률\n","        per_device_train_batch_size=config['training']['per_device_train_batch_size'],  # 학습 시 디바이스당 배치 크기\n","        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],    # 평가 시 디바이스당 배치 크기\n","        warmup_ratio=config['training']['warmup_ratio'],                                # 학습 초기에 학습률을 점진적으로 증가시키는 비율\n","        weight_decay=config['training']['weight_decay'],                                # 가중치 감쇠 (과적합 방지)\n","        lr_scheduler_type=config['training']['lr_scheduler_type'],                      # 학습률 스케줄러 유형\n","        optim=config['training']['optim'],                                              # 옵티마이저 종류\n","        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],  # 기울기(gradient) 축적 단계 수\n","        evaluation_strategy=config['training']['evaluation_strategy'],                  # 학습 중 평가 전략 (예: 에포크마다 평가)\n","        save_strategy=config['training']['save_strategy'],                              # 모델 저장 전략\n","        save_total_limit=config['training']['save_total_limit'],                        # 저장할 체크포인트의 최대 개수\n","        fp16=config['training']['fp16'],                                                # 반정밀도(float16) 연산 사용 여부\n","        load_best_model_at_end=config['training']['load_best_model_at_end'],            # 학습 종료 시 가장 좋은 모델 로드\n","        seed=config['training']['seed'],                                                # 랜덤 시드 설정\n","        logging_dir=config['training']['logging_dir'],                                  # 로그 저장 디렉터리\n","        logging_strategy=config['training']['logging_strategy'],                        # 로그 기록 전략 (예: 에포크마다 기록)\n","        predict_with_generate=config['training']['predict_with_generate'],              # 텍스트 생성 후 평가 지표를 계산할지 여부\n","        generation_max_length=config['training']['generation_max_length'],              # 텍스트 생성 시 최대 길이\n","        do_train=config['training']['do_train'],                                        # 학습 수행 여부\n","        do_eval=config['training']['do_eval'],                                          # 평가 수행 여부\n","        report_to=config['training']['report_to']                                       # (선택 사항) wandb로 학습 과정을 기록할지 여부\n","    )\n","\n","\n","    ### wandb 초기화 (선택 사항):\n","    # (선택 사항) wandb를 사용하여 학습 과정을 추적할 때 초기화합니다.\n","    # WandB(Weights & Biases)로 학습 과정을 추적하고 시각화하려면 wandb.init()을 사용해 초기화할 수 있습니다.\n","    # 이 부분은 현재 주석 처리되어 있으며, 필요한 경우 활성화할 수 있습니다.\n","    wandb.init(\n","         entity=config['wandb']['entity'],\n","         project=config['wandb']['project'],\n","         name=config['wandb']['name']\n","    )\n","    # (선택 사항) 모델 체크포인트를 wandb에 저장하도록 환경 변수를 설정합니다.\n","    os.environ[\"WANDB_LOG_MODEL\"] = \"true\"\n","    os.environ[\"WANDB_WATCH\"] = \"false\"\n","\n","\n","    ### EarlyStoppingCallback 설정:\n","    # EarlyStoppingCallback을 사용하여 학습이 진행될 때 검증 손실이 더 이상 개선되지 않으면 학습을 조기에 중단시킵니다.\n","    # 설정된 early_stopping_patience와 early_stopping_threshold에 따라 작동합니다.\n","    # EarlyStoppingCallback: 검증 손실이 더 이상 개선되지 않을 때 학습을 중단시키는 콜백을 설정합니다.\n","    MyCallback = EarlyStoppingCallback(\n","        early_stopping_patience=config['training']['early_stopping_patience'],      # 개선이 없을 경우 중단까지 기다릴 에포크 수\n","        early_stopping_threshold=config['training']['early_stopping_threshold']     # 개선으로 간주할 최소 손실 감소량\n","    )\n","    print('-' * 10, 'Make training arguments complete', '-' * 10,)\n","    print('-' * 10, 'Make trainer', '-' * 10,)\n","\n","\n","    ### Seq2SeqTrainer 초기화:\n","    # Seq2SeqTrainer는 Hugging Face의 transformers 라이브러리에서 제공하는 훈련 도구로,\n","    # 시퀀스-투-시퀀스 모델의 학습과 평가를 위한 도구입니다.\n","    # 학습할 모델, 설정값, 학습/검증 데이터셋, 평가 메트릭 함수, 콜백 등을 인자로 받아 초기화합니다.\n","    trainer = Seq2SeqTrainer(\n","        model=generate_model,  # 학습할 모델\n","        args=training_args,  # 학습 설정값\n","        train_dataset=train_inputs_dataset,  # 학습 데이터셋\n","        eval_dataset=val_inputs_dataset,  # 검증 데이터셋\n","        compute_metrics=lambda pred: compute_metrics(config, tokenizer, pred),  # 성능 평가를 위한 메트릭 함수\n","        callbacks=[MyCallback]  # EarlyStoppingCallback을 포함한 콜백 리스트\n","    )\n","    print('-' * 10, 'Make trainer complete', '-' * 10,)\n","\n","    ### Trainer 객체 반환:\n","    # 생성된 trainer 객체를 반환하여, 이후 학습을 진행할 수 있게 합니다.\n","    return trainer\n","\n","\n","    # 이 함수는 모델 학습을 시작하기 위한 모든 준비 작업을 자동으로 처리하며,\n","    # 사용자에게 최적화된 Trainer 객체를 제공합니다. 이를 통해 학습 및 평가를 손쉽게 수행할 수 있습니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AaTDAv0rLgAJ","outputId":"0eb54fce-a43b-4866-fa4e-721674c4fccf"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","#######  학습을 위한 Trainer 클래스를 초기화합니다.  ############################################################################\n","---------- Make training arguments ----------\n"]},{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkkukky81\u001b[0m (\u001b[33mdonggunlim\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.17.9 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/code/wandb/run-20240906_113726-i979y2qm</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/donggunlim/NLP/runs/i979y2qm' target=\"_blank\">digit82/kobart-summarizationtest</a></strong> to <a href='https://wandb.ai/donggunlim/NLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/donggunlim/NLP' target=\"_blank\">https://wandb.ai/donggunlim/NLP</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/donggunlim/NLP/runs/i979y2qm' target=\"_blank\">https://wandb.ai/donggunlim/NLP/runs/i979y2qm</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/venv/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n","  warnings.warn(\n","/home/venv/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"]},{"name":"stdout","output_type":"stream","text":["---------- Make training arguments complete ----------\n","---------- Make trainer ----------\n","---------- Make trainer complete ----------\n"]}],"source":["print(\"\\n#######  학습을 위한 Trainer 클래스를 초기화합니다.  ############################################################################\")\n","# 학습을 위한 Trainer 클래스를 초기화합니다.\n","trainer = load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset)"]},{"cell_type":"markdown","metadata":{"id":"BLAszZLALgAK"},"source":["### 5) 모델 학습, wandb 세션종료"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z0VwUz5JLgAK","outputId":"b977917a-a206-46c9-e1e5-8a9075475ff6"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","#######  모델 학습을 시작합니다.  ##############################################################################################\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='52' max='31160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [   52/31160 00:16 < 2:51:16, 3.03 it/s, Epoch 0.03/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[74], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m#######  모델 학습을 시작합니다.  ##############################################################################################\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 모델 학습을 시작합니다.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m#######  wandb 세션을 종료합니다.  ############################################################################################\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# (선택) 모델 학습이 완료된 후 wandb 세션을 종료합니다.\u001b[39;00m\n","File \u001b[0;32m/home/venv/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n","File \u001b[0;32m/home/venv/lib/python3.10/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[1;32m   1838\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1839\u001b[0m     resume_from_checkpoint: Optional[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1843\u001b[0m ):\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;124;03m    Main training entry point.\u001b[39;00m\n\u001b[1;32m   1846\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;124;03m            Additional keyword arguments used to hide deprecated arguments\u001b[39;00m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resume_from_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   1861\u001b[0m         resume_from_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1863\u001b[0m     \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n","File \u001b[0;32m/home/venv/lib/python3.10/site-packages/transformers/trainer.py:2734\u001b[0m, in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2731\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled() \u001b[38;5;129;01mand\u001b[39;00m has_been_loaded:\n\u001b[1;32m   2732\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_issue_warnings_after_load(load_result)\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_model_checkpoint, SAFE_WEIGHTS_INDEX_NAME)) \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\n\u001b[0;32m-> 2734\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_model_checkpoint, WEIGHTS_INDEX_NAME)\n\u001b[1;32m   2735\u001b[0m ):\n\u001b[1;32m   2736\u001b[0m     load_result \u001b[38;5;241m=\u001b[39m load_sharded_checkpoint(\n\u001b[1;32m   2737\u001b[0m         model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_model_checkpoint, strict\u001b[38;5;241m=\u001b[39mis_sagemaker_mp_enabled()\n\u001b[1;32m   2738\u001b[0m     )\n\u001b[1;32m   2739\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n","File \u001b[0;32m/home/venv/lib/python3.10/site-packages/accelerate/accelerator.py:2192\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2193\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n","File \u001b[0;32m/home/venv/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/home/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/home/venv/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#%%time 추가\n","\n","print(\"\\n#######  모델 학습을 시작합니다.  ##############################################################################################\")\n","# 모델 학습을 시작합니다.\n","trainer.train()\n","\n","print(\"\\n#######  wandb 세션을 종료합니다.  ############################################################################################\")\n","# (선택) 모델 학습이 완료된 후 wandb 세션을 종료합니다.\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"WO0I9ESOLgAK"},"source":["## 2. 모델 추론하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tqLVB53rLgAK"},"outputs":[],"source":["# 이곳에 내가 사용할 wandb config 설정 \"추론에 사용할 ckt 경로 설정\"\n","# loaded_config['inference']['ckt_path'] = \"home/ckt\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KsdcMdsJLgAK","outputId":"0bf42db3-ac7b-42ca-f74b-839a12446101"},"outputs":[{"name":"stdout","output_type":"stream","text":["---------- device : cuda:0 ----------\n","2.4.1+cu121\n"]}],"source":["# 사용할 device를 정의합니다. GPU가 사용 가능하면 'cuda:0', 그렇지 않으면 'cpu'를 사용합니다.\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('-' * 10, f'device : {device}', '-' * 10,)\n","print(torch.__version__)"]},{"cell_type":"markdown","metadata":{"id":"sTrzBn5ZLgAK"},"source":["### 1) 모델과 토크나이저 불러오기"]},{"cell_type":"markdown","metadata":{"id":"mckIMkYpLgAK"},"source":["<font color=red size=30> ckt 값 바꾸기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDVWW8-DLgAK","outputId":"9e849b12-85ef-44ed-94a8-09131eda194e"},"outputs":[{"ename":"NameError","evalue":"name '멈춤' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[324], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43m멈춤\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name '멈춤' is not defined"]}],"source":["멈춤"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M8pH7Od-LgAK"},"outputs":[],"source":["# 추론을 위한 tokenizer와 학습시킨 모델을 불러옵니다.\n","def load_tokenizer_and_model_for_test(config, device):\n","    print('-' * 10, 'Load tokenizer & model', '-' * 10,)\n","\n","    print(\"\\n######  설정 파일에서 모델 이름과 체크포인트 경로를 불러옵니다.\")\n","    ### 모델 이름과 체크포인트 경로 설정:\n","    # 설정 파일에서 모델 이름과 체크포인트 경로를 불러옵니다.\n","    # config['general']['model_name']: 사전 학습된 모델의 이름을 설정 파일에서 가져옵니다.\n","    # config['inference']['ckt_path']: 학습된 모델의 체크포인트 경로를 설정 파일에서 가져옵니다. 이 경로는 학습이 완료된 후 저장된 모델의 위치를 나타냅니다.\n","    model_name = config['general']['model_name']\n","    ckt_path = config['inference']['ckt_path']  # 학습된 모델의 체크포인트 경로\n","    ckt_path = \"/home/code/checkpoint-7790\"    #####################################################################\n","    print(model_name, \" / \", ckt_path)\n","    print('-' * 10, f'Model Name : {model_name}', '-' * 10,)\n","\n","\n","    ### 토크나이저 로드 및 특수 토큰 추가:\n","    # AutoTokenizer.from_pretrained(model_name): 사전 학습된 모델 이름을 사용하여 토크나이저를 로드합니다.\n","    # tokenizer.add_special_tokens(special_tokens_dict): 설정 파일에서 정의된 특수 토큰을 토크나이저에 추가합니다. 이는 모델이 특정 단어들을 특수 토큰으로 처리할 수 있도록 합니다.\n","    print(\"\\n######  지정된 모델 이름으로부터 토크나이저를 로드합니다.\")\n","    # 지정된 모델 이름으로부터 토크나이저를 로드합니다.\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    print(\"\\n######  설정 파일에서 정의된 특수 토큰을 토크나이저에 추가합니다.\")\n","    # 설정 파일에서 정의된 특수 토큰을 토크나이저에 추가합니다.\n","    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n","    tokenizer.add_special_tokens(special_tokens_dict)\n","\n","\n","    ### 모델 로드:\n","    # BartForConditionalGeneration.from_pretrained(ckt_path): 학습된 모델의 체크포인트에서 BART 모델을 로드합니다.\n","    # generate_model.resize_token_embeddings(len(tokenizer)): 토크나이저에 추가된 특수 토큰에 맞춰 모델의 토큰 임베딩 크기를 재조정합니다.\n","    print(\"\\n######  학습된 체크포인트에서 BART 모델을 로드합니다.\")\n","    # 학습된 체크포인트에서 BART 모델을 로드합니다.\n","    generate_model = BartForConditionalGeneration.from_pretrained(ckt_path)\n","    print(\"\\n######  추가된 토큰에 맞게 모델의 토큰 임베딩 크기를 조정합니다.\")\n","    # 추가된 토큰에 맞게 모델의 토큰 임베딩 크기를 조정합니다.\n","    generate_model.resize_token_embeddings(len(tokenizer))\n","\n","\n","    ### 디바이스로 모델 이동:\n","    # generate_model.to(device): 로드된 모델을 지정된 디바이스(GPU 또는 CPU)로 이동시켜,\n","    # 추론 작업을 해당 디바이스에서 수행할 수 있도록 합니다.\n","    print(\"\\n######  모델을 지정된 디바이스(GPU 또는 CPU)로 이동시킵니다.\")\n","    # 모델을 지정된 디바이스(GPU 또는 CPU)로 이동시킵니다.\n","    generate_model.to(device)\n","\n","    print('-' * 10, 'Load tokenizer & model complete', '-' * 10,)\n","\n","\n","    ### 모델과 토크나이저 반환:\n","    # 최종적으로 불러온 모델과 토크나이저를 반환하여, 이후 추론 작업에서 사용할 수 있도록 합니다.\n","    return generate_model, tokenizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i_5dV8mNLgAL","outputId":"06674436-d393-407a-e34a-77ab652ddbda"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","#######  추론을 위한 모델과 토크나이저를 불러옵니다.  ##############################################################################\n","---------- Load tokenizer & model ----------\n","\n","######  설정 파일에서 모델 이름과 체크포인트 경로를 불러옵니다.\n","jx7789/kobart_summary_v3  /  /home/code/checkpoint-7790\n","---------- Model Name : jx7789/kobart_summary_v3 ----------\n","\n","######  지정된 모델 이름으로부터 토크나이저를 로드합니다.\n"]},{"name":"stderr","output_type":"stream","text":["/home/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]},{"name":"stdout","output_type":"stream","text":["\n","######  설정 파일에서 정의된 특수 토큰을 토크나이저에 추가합니다.\n","\n","######  학습된 체크포인트에서 BART 모델을 로드합니다.\n","\n","######  추가된 토큰에 맞게 모델의 토큰 임베딩 크기를 조정합니다.\n","\n","######  모델을 지정된 디바이스(GPU 또는 CPU)로 이동시킵니다.\n","---------- Load tokenizer & model complete ----------\n"]}],"source":["print(\"\\n#######  추론을 위한 모델과 토크나이저를 불러옵니다.  ##############################################################################\")\n","# 추론을 위한 모델과 토크나이저를 불러옵니다.\n","generate_model, tokenizer = load_tokenizer_and_model_for_test(config, device)"]},{"cell_type":"markdown","metadata":{"id":"_zV9ikmELgAL"},"source":["### 2) 데이터 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hWIoaefcLgAL","outputId":"0f341904-bc52-4920-b899-dc7c2147ab72"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","#######  데이터 경로와 전처리기를 설정합니다.  ####################################################################################\n"]}],"source":["print(\"\\n#######  데이터 경로와 전처리기를 설정합니다.  ####################################################################################\")\n","# 데이터 경로와 전처리기를 설정합니다.\n","data_path = config['general']['data_path']\n","preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])"]},{"cell_type":"markdown","metadata":{"id":"kU1cus9wLgAL"},"source":["### 3) 테스트 데이터셋 준비"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2u9ptZQLgAL"},"outputs":[],"source":["# tokenization 과정까지 진행된 최종적으로 모델에 입력될 데이터를 출력합니다.\n","def prepare_test_dataset(config, preprocessor, tokenizer):\n","\n","    ### 테스트 데이터 로드:\n","    # test_file_path에서 test.csv 파일을 읽어옵니다.\n","    test_file_path = os.path.join(config['general']['data_path'], 'test.csv')\n","    # Preprocess 클래스의 make_set_as_df 메서드를 사용하여 테스트 데이터를 데이터프레임으로 변환합니다. 이때, is_train=False로 설정하여 학습과 달리 요약(summary)이 포함되지 않은 테스트 데이터를 처리합니다.\n","    # test_data['fname']는 테스트 데이터의 고유 ID로, 각 샘플을 식별하는 데 사용됩니다.\n","    # 테스트 데이터를 데이터프레임으로 변환합니다. is_train=False로 설정하여 테스트 데이터셋을 로드합니다.\n","    test_data = preprocessor.make_set_as_df(test_file_path, is_train=False)\n","    test_id = test_data['fname']  # 테스트 데이터의 고유 ID (fname)을 가져옵니다.\n","\n","\n","    # 데이터 확인:\n","    # 테스트 데이터의 첫 번째 대화(dialogue) 샘플을 출력하여, 데이터가 올바르게 로드되었는지 확인합니다.\n","    print('-' * 150)\n","    print(f'test_data:\\n{test_data[\"dialogue\"][0]}')\n","    print('-' * 150)\n","\n","\n","    ### 인코더 및 디코더 입력 생성:\n","    # Preprocess 클래스의 make_input 메서드를 사용하여 인코더 입력과 디코더 입력을 생성합니다.\n","    # 테스트 데이터의 경우, 디코더 입력은 시작 토큰(bos_token)만 포함합니다.\n","    # 테스트 데이터에 대해 인코더 입력과 디코더 입력을 생성합니다. is_test=True로 설정하여 디코더 입력에만 시작 토큰을 넣습니다.\n","    encoder_input_test, decoder_input_test = preprocessor.make_input(test_data, is_test=True)\n","    print('-' * 10, 'Load data complete', '-' * 10,)\n","\n","\n","    ### 토큰화:\n","    # tokenizer를 사용하여 인코더 입력과 디코더 입력을 토큰화합니다.\n","    # 이 과정에서 패딩, 특수 토큰 추가, 최대 길이 제한 등을 설정하여 텐서 형태로 반환합니다.\n","    # 테스트 데이터의 인코더 입력을 토크나이저로 토큰화합니다.\n","    test_tokenized_encoder_inputs = tokenizer(\n","        encoder_input_test, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False,\n","    )\n","    # 테스트 데이터의 디코더 입력을 토크나이저로 토큰화합니다.\n","    test_tokenized_decoder_inputs = tokenizer(\n","        decoder_input_test, return_tensors=\"pt\", padding=True,\n","        add_special_tokens=True, truncation=True,\n","        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False,\n","    )\n","\n","\n","    ### 테스트 데이터셋 준비:\n","    # DatasetForInference 클래스를 사용하여 토큰화된 입력 데이터와 테스트 ID를 포함한 데이터셋 객체를 생성합니다.\n","    # 이 데이터셋 객체는 모델이 예측을 수행하는 데 사용됩니다.\n","    test_encoder_inputs_dataset = DatasetForInference(test_tokenized_encoder_inputs, test_id, len(encoder_input_test))\n","    print('-' * 10, 'Make dataset complete', '-' * 10,)\n","\n","\n","    ### 결과 반환:\n","    # 원본 테스트 데이터(test_data)와 모델에 입력될 토큰화된 데이터셋(test_encoder_inputs_dataset)을 반환합니다.\n","    return test_data, test_encoder_inputs_dataset\n","\n","\n","    # 이 함수는 테스트 데이터를 전처리하여 모델에 적합한 입력 데이터로 준비합니다.\n","    # 이를 통해 학습된 모델이 테스트 데이터에 대한 예측을 정확하게 수행할 수 있도록 합니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JG3gh32BLgAM","outputId":"5a5e45c7-cc2a-4b5e-9c10-ce6171855f9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","#######  테스트 데이터셋을 준비합니다.  ##########################################################################################\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","test_data:\n","#Person1#: 더슨 씨, 받아쓰기 좀 해주세요. \n","#Person2#: 네, 실장님...\n","#Person1#: 이것은 오늘 오후까지 모든 직원에게 내부 메모로 전달되어야 합니다. 준비되셨나요?\n","#Person2#: 네, 실장님. 시작하셔도 됩니다.\n","#Person1#: 모든 직원들에게 주의하라... 즉시 효력을 발휘하여, 모든 사무실 통신은 이메일 통신과 공식 메모로 제한됩니다. 근무 시간 동안 직원들이 즉시 메시지 프로그램을 사용하는 것은 엄격히 금지됩니다.\n","#Person2#: 실장님, 이것은 내부 통신에만 적용되는 건가요? 아니면 외부 통신에도 제한이 되는 건가요?\n","#Person1#: 이것은 모든 통신에 적용되어야 합니다, 이 사무실 내의 직원들 사이뿐만 아니라 외부 통신에도 마찬가지입니다.\n","#Person2#: 하지만 실장님, 많은 직원들이 고객과 소통하기 위해 즉시 메시지를 사용하고 있습니다.\n","#Person1#: 그들은 그들의 의사소통 방법을 바꾸어야만 합니다. 이 사무실에서 누구도 즉시 메시지를 사용하지 않기를 원합니다. 너무 많은 시간을 낭비하게 됩니다! 이제, 메모를 계속해주세요. 우리가 어디까지 했나요?\n","#Person2#: 이것은 내부와 외부 통신에 적용됩니다.\n","#Person1#: 그렇습니다. 즉시 메시지를 계속 사용하는 어떤 직원이라도 먼저 경고를 받고 직무 정지에 처해질 것입니다. 두 번째 위반 시에는 직원은 해고에 처해질 것입니다. 이 새로운 정책에 대한 어떤 질문이라도 부서장에게 직접 문의하면 됩니다.\n","#Person2#: 그게 다신가요?\n","#Person1#: 네. 이 메모를 오후 4시 전에 모든 직원에게 타이핑하여 배포해 주세요.\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","---------- Load data complete ----------\n"]},{"name":"stdout","output_type":"stream","text":["---------- Make dataset complete ----------\n"]}],"source":["print(\"\\n#######  테스트 데이터셋을 준비합니다.  ##########################################################################################\")\n","# 테스트 데이터셋을 준비합니다.\n","test_data, test_encoder_inputs_dataset = prepare_test_dataset(config, preprocessor, tokenizer)\n"]},{"cell_type":"markdown","metadata":{"id":"_YVTYW68LgAM"},"source":["### 4) 데이터 로더 생성 / 테스트 데이터 추론"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b6L2PDfELgAM","outputId":"b82aac8d-e1db-4d70-c474-7f7cabdc4420"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","#######  데이터 로더를 생성합니다. 배치 크기는 설정 파일에서 가져옵니다.  ##########################################################\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/32 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 32/32 [00:49<00:00,  1.56s/it]\n"]}],"source":["print(\"\\n#######  데이터 로더를 생성합니다. 배치 크기는 설정 파일에서 가져옵니다.  ##########################################################\")\n","# 데이터 로더를 생성합니다. 배치 크기는 설정 파일에서 가져옵니다.\n","dataloader = DataLoader(test_encoder_inputs_dataset, batch_size=config['inference']['batch_size'])\n","\n","summary = []  # 요약문을 저장할 리스트\n","text_ids = []  # 텍스트 ID를 저장할 리스트\n","\n","with torch.no_grad():  # 추론 중에는 기울기 계산을 비활성화하여 메모리를 절약합니다.\n","    for item in tqdm(dataloader):  # 데이터 로더를 통해 배치 단위로 데이터에 접근합니다.\n","        text_ids.extend(item['ID'])  # 현재 배치의 텍스트 ID를 저장합니다.\n","        generated_ids = generate_model.generate(\n","            input_ids=item['input_ids'].to(device),  # 인코더 입력을 디바이스로 이동시켜 모델에 전달합니다.\n","            no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],  # 반복 n-gram 방지 설정\n","            early_stopping=config['inference']['early_stopping'],  # 조기 종료 설정\n","            max_length=config['inference']['generate_max_length'],  # 생성할 텍스트의 최대 길이\n","            num_beams=config['inference']['num_beams'],  # 빔 서치(beam search)의 빔 수 설정\n","        )\n","        for ids in generated_ids:  # 생성된 요약문 ID를 디코딩하여 텍스트로 변환합니다.\n","            result = tokenizer.decode(ids, skip_special_tokens=False)  # 특수 토큰을 건너뛰고 디코딩합니다.\n","            summary.append(result)  # 생성된 요약문을 리스트에 추가합니다."]},{"cell_type":"markdown","metadata":{"id":"NkfduFgRLgAM"},"source":["### 5) 최종 결과/요약문 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x5Rz_wC-LgAM","outputId":"20f22917-c478-41f1-abec-f77894725b13"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","#######  스페셜 토큰을 제거하여 최종 요약문을 전처리합니다.  ######################################################################\n"]}],"source":["print(\"\\n#######  스페셜 토큰을 제거하여 최종 요약문을 전처리합니다.  ######################################################################\")\n","# 스페셜 토큰을 제거하여 최종 요약문을 전처리합니다.\n","remove_tokens = config['inference']['remove_tokens']\n","preprocessed_summary = summary.copy()\n","for token in remove_tokens:\n","    preprocessed_summary = [sentence.replace(token, \" \") for sentence in preprocessed_summary]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B1Oyj68KLgAN"},"outputs":[],"source":["\n","for i in range(len(preprocessed_summary)):\n","    tmp = preprocessed_summary[i]\n","    left_trimmed = tmp.lstrip()  # 왼쪽 공백 제거\n","    right_trimmed = left_trimmed.rstrip()  # 오른쪽 공백 제거\n","    tmp = right_trimmed\n","    tmp = tmp.replace('# ', '#')\n","    preprocessed_summary[i] = tmp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1qbLY4cpLgAN","outputId":"39524f68-78ba-47c5-d34e-64a2b7f88e11"},"outputs":[{"data":{"text/plain":["['더슨 씨는 #Person1#에게 모든 직원에게 내부 메모로 전달될 수 있도록 지시하고, 직원들이 즉시 메시지를 사용하는 것을 금지하도록 요청합니다. 또한 그들은 내부와 외부 통신에 적용하도록 지시하며, 모든 직원이 부서장에게 직접 문의하도록 합니다.',\n"," '#Person1#은 또 교통 체증에 걸렸다. #Person2#는 #Person1#에게 대중교통을 이용하거나 자전거를 타는 것을 제안한다. 그들은 환경 오염에 기여할 수 있을 것이라고 생각한다.',\n"," '케이트는 #Person1#에게 마샤와 히어로가 이혼을 신청했다고 말한다. 그들은 아이들이 어떻게 양육권을 가지게 되는지에 대해 이야기한다.',\n"," '브라이언은 #Person1#의 생일 파티에 참석하여 춤을 추고 있다. 그들은 서로의 목걸이가 드레스와 잘 어울리길 바란다.',\n"," '#Person1#과 #Person2#는 올림픽 공원의 크고 다양한 시설에 대해 이야기하고 있습니다.',\n"," '#Person1#은 #Person2#에게 회사를 창업하고 사업 계획서를 작성하고 투자자를 모집할 것이라고 말한다. #Person1#는 요약문을 포함하여 회사에 대해 설명해야 하며, 상품이나 서비스를 소개하는 것이 어렵다고 생각한다. 또한 재무 분석은 투자자들에게 가장 중요한 정보이다.',\n"," '#Person1#은 #Person2#가 긁고 있는 것을 보고 수두에 걸린 것 같다고 생각한다. #Person1#의 의사는 그것이 두드러기나 알레르기 때문일 수도 있다고 말한다.',\n"," '#Person2#는 세탁 서비스 비용을 청구하기 위해 #Person1#에게 찾아갑니다. 세탁 서비스는 이용하지 않았지만, 다른 사람의 것을 추가한 것 같습니다. #Person2#은 관련 부서에 확인하겠다고 합니다.',\n"," '스티븐은 비서와의 불륜 때문에 아내와 이혼하려고 한다. #Person1#은 최선을 다해 그를 설득할 것이다.',\n"," '#Person2#는 #Person1#에게 건전한 성격을 가진 남자 또는 여자로 눈에 띄는 사람에 대해 이야기합니다. 에이브러햄 링컨은 미국 대통령으로, 평등한 권리를 위해 싸우는 용기를 존경합니다.',\n"," '#Person2#는 #Person1#에게 허베이로 여행을 가려고 하지만 중국 북부에서 심각한 모래폭풍이 일어나고 있다고 말한다. #Person2#의 보고서에 따르면 이 지역에 사는 사람들의 호흡기 감염자 수가 증가한다고 한다.',\n"," '프란시스는 생일 파티에서 #Person1#의 생일 선물로 리모컨 자동차 모델을 받았습니다. #Person2#는 그 선물에 감사합니다.',\n"," '스티븐이 토니에게 부정행위를 하다가 걸렸다고 말한다. 토니는 열심히 공부해야 한다고 생각한다.',\n"," '톰은 아홉시 10분 전에 기차를 타야 하기 때문에 서둘러야 한다.',\n"," '#Person2#는 #Person1#에게 잠을 잘 못 자서 삶을 어떻게 조정해야 하는지 조언을 해줍니다.',\n"," '#Person1#과 #Person2#는 루오지아가 결혼했다는 소식을 듣고 파티에 가기로 결정한다. #Person1#은 그녀의 집에서 와인잔과 그녀의 행복한 결혼을 축하하는 카드를 가져갈 것이다.',\n"," '#Person1#과 #Person2#는 줄기를 당기는 방법에 대해 이야기하고 있다.',\n"," '마이크는 #Person1#에게 그녀의 누나가 키가 크고 예뻐서 자만심이 크다고 말했다.',\n"," '#Person2#가 머리가 아파요. #Person1#은 #Person2#에게 아프면 부모님에게 전화하라고 말합니다.',\n"," '#Person2#는 #Person1#의 도움으로 카메라와 MP3 플레이어가 있는 휴대폰을 구입했습니다.',\n"," '프랭크는 주디에게 우체국에서 일하게 될 것이라고 말한다. 주디는 그가 왜 그런 힘든 일을 선택하게 되었는지 이해한다.',\n"," '#Person2#는 #Person1#에게 의사 면허와 운전 면허가 있으며 사무 기술에 대한 특별한 교육을 받았다고 말합니다. 또한 #Person2#가 이 직업에 적격하게 만드는 일을 해본 경험에 대해 이야기합니다.',\n"," '#Person1#은 미디엄 레어로 스테이크를 주문했지만 너무 익혀져 있어서 바꾸고 싶어합니다. #Person2#는 몇 분 후에 가져다 줄 것입니다.',\n"," '#Person1#은 톰에게 소설이 노벨상을 받았다는 소식을 전한다. 톰은 그것을 축하한다.',\n"," '#Person2#는 #Person1#에게 자신이 할 수 있다고 생각하게 된 이유와 이전 직장에서 #Person2#가 담당했던 업무에 대해 설명합니다.',\n"," '#Person1#과 #Person2#는 술을 마시는 빈도에 대해 이야기하고 있습니다. #Person1#은 보통 밤새도록 술을 마시며 내일 밤 피처맥주 특별 할인을 위해 램 바 앤 그릴에 가자고 제안합니다.',\n"," '#Person1#은 메에게 피크닉 준비를 도와달라고 요청한다. 메이는 #Person1#에게 토스트, 치킨 윙, 과일 샐러드, 크래커를 가져다 달라고 요청하고, 다니엘에게 도움을 청하는 것을 거부한다.',\n"," '뮤리엘 더글라스씨와 제임스씨가 서로 인사를 나눕니다. 그들은 각자의 휴가에 대해 이야기합니다. 수잔은 제임스가 스키를 타는 것을 좋아하지 않는다는 것을 알게 됩니다. 제임스씨는 제임스를 소개할 것입니다.',\n"," '#Person1#은 ATM에서 돈을 인출하려고 합니다. #Person2#는 #Person1#에게 유니버설 은행을 이용해 준 것에 감사하며 카드를 슬롯에 넣으라고 요청합니다.',\n"," '#Person2#는 #Person1#에게 사회적인 사람이며, 동료들과의 소통에서 가장 중요한 것은 소통이라고 말합니다.',\n"," '폴리 씨는 끔찍한 일에서 벗어나고 싶어합니다. #Person1#은 폴리 씨에게 탄산음료를 사다 달라고 요청합니다.',\n"," '프란시스는 모니카에게 재무 보고서를 언제 작업할 수 있는지 물어봅니다. 그들은 금요일 오후에 만날 예정입니다.',\n"," '#Person1#과 #Person2#는 다음 주에 있을 면접을 준비하기 위해 워크숍을 진행하고 있다. #Person1#은 면접에서 도움이 될 것들을 배우고 있다.',\n"," '컷은 마이크에게 그녀가 더 이상 보고 싶지 않다고 말하지만 제이슨과 로라는 분노와 슬픔을 동시에 느낀다고 말합니다. #Person1#은 다른 방법을 시도해 볼 것을 제안합니다.',\n"," '토드 부인은 아내가 쇼핑하러 가려고 기다리고 있다고 말합니다. #Person1#은 제인에게 먼저 전화하지 않았어야 했다고 후회하지만 제안은 고마워합니다.',\n"," '빌이 피곤해 보인다. #Person1#은 빌에게 집에 가서 쉬고 휴식을 취하라고 권한다. 빌은 어제 친구와 만날 시간을 #Person2#에게 알려준다.',\n"," '클레오는 사이먼에게 핵무기 확산 반대 시위에 참석할 수 없다고 말한다. 왜냐하면 그가 다칠까봐 두려워하기 때문이다. 그러나 클레는 평화로운 시위를 선호하며, 물리학 시험 공부를 위해 함께 가자고 제안한다.',\n"," '#Person1#은 누군가가 카드를 보여주면 자동으로 그가 괜찮다고 생각하게 된다고 #Person2#에게 말한다.',\n"," '마크는 매기의 역사 과목 노트를 빌리고 싶어한다. 매기는 보통 식당에서 그것을 복습하기 때문에 동의한다. 그들은 함께 공부할 사람이 필요하고, 마크가 깨어 있게 해 줄 사람이 필요하다.',\n"," '#Person1#은 터너 교수님에게 고급 지질학 과목을 다시 수강하고 싶다고 말합니다. 터먼 교수는 #Person1#에게 버먼 교수님께 말해보라고 조언합니다.',\n"," '#Person1#은 펜던트가 부러져서 #Person2#에게 새 것으로 교체해달라고 요청했습니다.',\n"," '#Person1#과 #Person2#는 경기를 보고 있다. #Person1#은 계속 옷 얘기만 하고 있다. 결국, 그들은 세 번째 쿼터에서 계속 수다를 떨고 있다.',\n"," '#Person2#는 #Person1#에게 미디어에서 일하는 것이 재미있을 것 같다고 말한다. #Person2#의 컴퓨터와 함께 일하는 인터랙티브 미디어를 보라고 제안한다.',\n"," '#Person1#과 #Person2#는 지루한 연설에 대해 불평하고 있습니다. #Person1#은 연설자들을 인터뷰하기 위해 아이팟과 헤드폰을 가져왔고 십자말풀이를 하기 위해 신문을 읽고 있습니다.',\n"," '사라는 #Person1#에게 도심에서 멀리 떨어진 저렴한 집을 사는 것을 제안합니다. #Person2#는 사라의 시누이와 남편이 그것에 만족하고 있다고 말합니다. 그들은 집 상태가 좋지 않지만 위치가 좋다고 생각합니다.',\n"," '런던의 정보 담당관인 마크 리치가 브리튼 비즈니스 센터의 관광 사무소에 방문객들을 위한 관광 정보 서비스를 소개하고 있습니다. 마크는 정보 팀을 담당하고 있으며, 그들은 원스톱 쇼핑과 어디를 방문하고 어떤 길을 선택해야 하는지 제안할 수 있습니다.',\n"," '린팡은 루시에게 영어를 가장 좋아하는 과목이라고 말한다. 그녀는 낸시도 영어가 꽤 어렵다고 말하지만, 그녀가 가장 좋아한다고 생각한다.',\n"," '제임스는 토마스 부인에게 마당에 낙엽을 치우고, 개를 산책시키고, 오스카를 산책시키는 데 도움을 주고, 주말에 자전거를 사러 할아버지 댁에 갈 예정입니다.',\n"," '#Person1#과 #Person2#는 봄을 맞이하지만 밤에는 여전히 춥다는 것에 대해 이야기합니다.',\n"," '마이크는 컷에게 더 많은 화를 내고 싶다고 말한다. 컷은 제이슨과 로라가 3년 동안 함께했기 때문에 그의 반응이 화와 슬픔의 혼합이 될 수 없다고 생각한다.',\n"," '#Person2#는 택시를 타고 프렌드십 호텔로 갑니다. #Person1#은 #Person2#에게 요금을 지불하라고 요청합니다.',\n"," '#Person1#은 배가 고파서 다른 버스를 타야 합니다. #Person2#는 #Person1#에게 환승 요금을 알려줍니다.',\n"," '#Person2#는 #Person1#에게 #Person2#의 회사가 미쓰비시, HP, IBM 및 기타 유명 기업이 있으며 6층과 7층 두 개 층을 사용하고 있다고 말합니다.',\n"," '#Person1#은 #Person2#에게 루루와 빅이 헤어졌으며 루루가 전근을 요청했다고 말한다.',\n"," '데이브 톰슨이 짐에게 전화를 걸어 짐이 아직 돌아오지 않았다고 알립니다. 샐리는 그에게 나중에 다시 전화하겠다고 말합니다.',\n"," '#Person1#이 #Person2#에게 시청으로 가는 방법을 알려줍니다. #Person1#은 두 블록만 더 걸어가면 왼쪽에 시청이 보일 것이라고 말합니다.',\n"," '#Person1#은 여권을 반납한 사람이 있는지 묻습니다. #Person2#는 #Person1#에게 분실물 신고서를 작성하고 대사관에 연락하라고 조언합니다.',\n"," '나다니엘 국제학생처에서 온 레아가 전화를 걸어 콜린스 선생님에게 전화를 돌려놓지 못한 것에 대해 사과한다. 레아는 오늘 오후 리우 씨에게 전화할 예정이지만 오늘 오전 11시 30분에 전화하기로 한다.',\n"," '#Person1#은 #Person2#에게 딕과 같은 남자와 결혼하지 않겠다고 말했을 때 무서워했다고 말합니다.',\n"," '#Person1#과 #Person2#는 파티에 참석한 몇몇 남자들의 외모에 대해 이야기한다. #Person1#은 여자와 말하는데 긴장하고 있다. 그들은 조금 더 맥주 마시고 분위기에 취해볼 것이다.',\n"," '잭이 #Person1#에게 이번 학기 수업에 대해 이야기한다. 잭은 정치학 수업이 가장 좋았고 비즈니스 커뮤니케이션 수업을 들었다.',\n"," '#Person1#과 #Person2#는 어젯밤의 날씨에 대해 이야기하고 있습니다. #Person1#은 베이징의 가을은 아름답지만 겨울에는 너무 덥다고 생각합니다. 그들은 이맘때쯤 저녁에는 정말 시원할 수 있다고 합니다.',\n"," '#Person1#은 #Person2#에게 공포 영화를 보자고 제안한다. #Person1#는 추리 영화를 좋아하지 않지만, 로맨스는 지루하다고 생각한다.',\n"," '#Person1#은 아담에게 학교 구경을 시켜달라고 요청한다. 그들은 도서관, 학교 남쪽에 있는 건물, 그리고 수영장에 대해 이야기한다.',\n"," '#Person1#이 #Person2#에게 아기가 곧 태어날 것이라는 소식을 전한다.',\n"," '#Person1#과 #Person2#는 존이 그녀와 일곱 번이나 만났다는 사실에 놀랐습니다.',\n"," '#Person2#는 시스템 업그레이드를 고려하고 있습니다. #Person1#은 #Person2#에게 페인팅 프로그램을 추가하고, 더 빠른 프로세서, 더 강력한 하드 디스크, 더 많은 모뎀이 필요하다고 말합니다.',\n"," '#Person2#는 중국에서 왔고 멕시코 사람입니다. #Person1#은 #Person2#가 너무 젊어 보인다고 생각합니다. 그들은 서로의 나이를 묻고 스페인어로 이야기합니다.',\n"," '#Person1#은 체중 증가를 겪고 있습니다. #Person2#는 #Person1#에게 식단 조절을 제안하고 운동을 제안합니다.',\n"," '제임스는 8인 룸을 예약하고 식당에 왔다. #Person1#은 제임스를 직원에게 안내할 것이다.',\n"," '#Person2#는 #Person1#에게 공장이 70년대 초에 설립되었으며 곧 30주년을 맞이할 것이라고 말합니다. 그들은 조립 라인부터 시작하기로 결정합니다.',\n"," '레베카는 #Person1#에게 대학 졸업 후 첫 직장이 지역 신문사인 요크 헤럴드였다고 말합니다. 그녀는 2006년에 국가 신문사를 떠나 런던에 본사를 둔 신문사에서 자리를 찾을 수 있었습니다.',\n"," '#Person1#과 #Person2#는 그룹 발표를 위한 쇼핑 목록을 만들고 있습니다. #Person1#은 마커, 색연필, 형광펜, 브러시, 압정, 수정테이프, 클립을 가져올 것입니다.',\n"," '메리는 #Person1#에게 인터넷에서 일자리를 찾아보라고 제안한다. #Person2#는 인터넷에 접속하여 이력서를 이메일로 고용주에게 보내는 것이 안전하다고 말한다.',\n"," '#Person1#은 #Person2#에게 쇼핑 예산을 만들고 있다고 말한다. #Person1#의 예산 계획은 좋은 생각이다.',\n"," '제인과 헨리는 수잔을 보려고 병원에 가고 있다. 제인은 헨리에게 13번 버스를 탈 것이라고 말한다.',\n"," '#Person2#는 내년 판매 예측에 대해 이야기하고 싶어합니다. #Person1#은 #Person2#에게 스프레드시트를 어떻게 사용하는지 설명해달라고 요청합니다. 그들은 다음 화요일 오후 2시 30분에 만날 예정입니다.',\n"," \"#Person1#은 처음으로 뉴욕에 가는데 투어 가이드가 없습니다. #Person2#는 #Person1#에게 '뉴욕의 친구' 서비스를 추천하고 설문조사를 받을 수 있다고 말합니다.\",\n"," '#Person2#는 #Person1#에게 상하이 지사가 어린이용 장난감을 생산하기 때문에 영업 부서에서 일하고 싶다고 말합니다. #Person2#의 월급과 복리후생에 대해 묻습니다.',\n"," '#Person1#은 계약서에 서명하러 왔습니다. #Person2#는 #Person1#에게 초안에 포함된 수정 사항에 대해 물어봅니다.',\n"," '#Person1#은 ABC 렌트카 회사에 전화하여 나이아가라 폭포로 가는 길에 차량 사고가 났다고 알립니다. #Person2#는 구급차와 경찰을 불러달라고 요청합니다.',\n"," '#Person2#가 #Person1#에게 학교 클리닉으로 가는 방법을 알려줍니다.',\n"," '#Person2#는 엘리베이터 소음을 참을 수 없어서 방을 바꾸고 싶어합니다. #Person1#은 #Person2#에게 내일까지 기다릴 수 있는지 묻고, 그들은 내일 저녁에 편안하게 머무르기로 결정합니다.',\n"," '#Person1#은 #Person2#에게 베이징 호텔로 가는 비행기를 알려주고, 우 씨를 위한 저녁 식사에 대해 묻습니다.',\n"," '#Person1#은 길을 잃었습니다. #Person2#는 #Person1#에게 리우 이창에 있는 중국 골동품 가게에 가는 길을 알려줍니다.',\n"," '#Person1#은 컴퓨터가 잘 작동하지 않는다며 리 씨에게 도움을 청한다. #Person2#는 수리공에게 전화하는 것을 제안한다.',\n"," '#Person2#는 어머니의 생일 선물로 목걸이를 사려고 합니다. #Person1#은 작은 숫자가 있는 금 시계를 추천하고, #Person2#은 그것을 선택합니다.',\n"," '피셔 씨가 로스 씨에게 전화를 걸어 10월 24일 토요일에 뉴질랜드에 지사를 열 예정이라고 알리고 홍보실장을 설득하려고 합니다. 그들은 발표 시간과 날짜를 확인하고 커피와 간식 뷔페에 대한 비용을 청구할 것입니다. 그들은 또한 목요일에 100장의 초대장을 보낼 예정입니다.',\n"," '#Person2#는 #Person1#에게 러시아와 캐나다 사이의 주요 차이점에 대해 이야기합니다. 러시아 사람들은 빠르게 말하고, 차를 너무 빨리 몰고, 좋은 거래도 금방 지나가버립니다. 캐나다 사람들은 일반적으로 러시아인들보다 더 차분하지만 캐나다인들은 더 여유롭습니다.',\n"," '#Person1#과 #Person2#는 올해 휴가를 어디로 갈지에 대해 이야기하고 있습니다. 그들은 카리브해로 여행을 가기로 결정합니다.',\n"," '#Person1#은 소풍 갈 때 가져갈 과일을 찾고 있습니다. #Person2#는 포도를 가져가기로 결정했습니다.',\n"," '#Person2#는 #Person1#에게 소형차 렌트 비용과 운전 면허증을 보여줍니다.',\n"," '#Person2#는 #Person1#에게 엘리베이터 안에 미소를 짓는 것이 #Person2#의 무표정 유지에 질렸다고 말합니다.',\n"," '#Person1#과 #Person2#는 올해 연간 판매에 대해 이야기하고 있습니다. #Person1#은 월리스의 기여와 새로운 마케팅 전략 덕분이라고 말합니다. 그들은 이 분야에서 최고의 판매자가 되었습니다.',\n"," '#Person1#의 가방이 도난당했습니다. #Person2#는 #Person1#에게 택시를 타기 위해 자신의 신분증을 잃어버렸다고 말합니다. #Person3#는 금요일에 돈을 갚을 예정입니다.',\n"," '린과 스티븐이 베이징에서 식사를 하고 있습니다. 그들은 중국에서는 팁을 주지 않고 택시 기사들과 짐꾼들이 지불하는 금액만 지불할 수 있다고 말합니다.',\n"," '빌이 #Person1#에게 룸메이트 브레인 로커에 대해 이야기한다.',\n"," '톰 윌슨이 #Person1#의 청구서에 서명하고 체크인을 도와줍니다.',\n"," '수잔이 캐롤의 전화를 대신 받아 샤워 중입니다. 캐 롤은 캐럴에게 내일의 파티가 오늘 밤인지 질의 집에서인지 물었습니다.',\n"," '#Person1#과 #Person2#는 커먼 헬스장에서 일하거나 시내의 영화관에서 일했던 경험에 대해 이야기한다.',\n"," '#Person1#은 #Person2#에게 트럼프가 다시 우리 대통령이 된다면 상상할 수 없다고 말합니다. #Person1#과 #Person3#는 모두 그가 나라를 잘 돌볼 것이라고 생각합니다.',\n"," '#Person1#은 ATM을 사용해야 합니다. #Person2#는 #Person1#에게 어떻게 사용하는지 알려줍니다.',\n"," '수잔이 밀러에게 비서에게 사무 절차에 대한 메모의 복사본을 주고 싶어한다.',\n"," '#Person1#은 릴리에게 함께 소풍에 가자고 제안한다. 릴리는 동의한다.',\n"," '#Person1#과 #Person2#는 중국 테이블에서 식사하는 경험을 공유합니다. #Person1#은 젓가락을 밥그릇에 꽂는 것이 중국 식사 도구에 익숙해지기 어렵다고 생각하지만, 중국에서는 적절하게 행동해야 한다는 것에 동의하지 않습니다.',\n"," '메리와 프랭크는 여가 시간에 어떤 영화를 보는지에 대해 이야기한다. 메리는 보통 무비 살롱에서 영화를 빌리며, 거의 모든 신작을 찾을 수 있다.',\n"," '#Person1#은 녹색당에 가입하는 것을 생각해 본 적이 있습니다. #Person2#는 #Person1#에게 녹색당이 선거에서 이길 가능성이 없다고 말합니다. 왜냐하면 그들은 정치에 대해 그다지 알지 않기 때문입니다.',\n"," '#Person1#은 윌슨 씨에게 사과하고 샘플에 미치지 못하는 모든 상품을 교환하겠다고 약속했다.',\n"," '#Person1#은 강도를 보았습니다. #Person2#는 #Person1#에게 추가 질문을 위해 경찰서로 오라고 요청했습니다.',\n"," '#Person1#은 저녁 식사에 대해 #Person2#에게 불평한다. #Person1#의 아빠와 엄마는 예전의 전통을 되살리려고 계획하고 있다.',\n"," '캐롤은 새해 결심에 대해 #Person1#에게 말하고, 캐럴은 식습관을 바꾸기 위해 다이어트를 시작하기로 결정했다.',\n"," '카렌 후앙은 비교문학 287 수업에 등록하고 싶어합니다. #Person1#은 그에게 전화로 등록하라고 제안하지만 컴퓨터에서 안 된다고 말합니다. 그 후 그들은 대학 컴퓨터 시스템에 전화하여 안내문에 있는 지시사항을 따르라고 요청합니다. 코헨 교수님은 그에게 특별 코드를 제공합니다.',\n"," '#Person1#은 급해서 우산을 깜빡하고 왔습니다. #Person2#는 #Person1#에게 가든 호텔로 가는 길을 알려줍니다.',\n"," '#Person1#은 잭에게 새 차를 태워주겠다고 제안한다. 잭은 비싸다고 생각하지만 매우 빠르게 달릴 수 있다고 생각한다.',\n"," '#Person2#는 #Person1#에게 큰 불이 밤에 일어났다고 말합니다.',\n"," '#Person1#은 #Person2#에게 900위안의 서비스 요금을 지불하라고 요청합니다. #Person1#의 남편은 신용카드를 가지고 있기 때문에 신용카드로 지불합니다.',\n"," '#Person2#는 #Person1#의 도움으로 일반 세차 패키지를 구입합니다.',\n"," '해리와 #Person1#은 올해 휴가 계획에 대해 이야기하고 있습니다. 해리는 이집트로 가고 싶어하고 아내는 걱정을 하고 있습니다.',\n"," '존슨이 새로운 회원인 #Person1#에게 웨이트 트레이닝 기계를 어떻게 사용하는지 설명합니다. 존슨은 먼저 워밍업을 하고 각 기계에서 얼마나 많은 무게를 사용했는지 기록합니다. 그는 처음에는 자신을 너무 밀어붙이지 않고 처음에는 자신의 한계를 알아내는 것이 중요하다고 생각합니다.',\n"," '#Person2#는 #Person1#에게 집세를 내기 위해 필요한 일이라면 무엇이든 할 것이라고 말한다. 그들은 전기기사 프로그램에 지원하기로 결정한다.',\n"," '#Person1#은 #Person2#에게 강아지들에게 밥을 주었고, 목욕을 시키고 동물병원에 갈 예정입니다.',\n"," '#Person1#은 집주인에게 200달러를 빚지고 있다. 에이든은 #Person1#에게 월급을 받기 전에 돈을 빌리려고 한다. #Person2#는 돈을 갚기 위해 오늘 밤에 저녁 식사를 제안한다.',\n"," '#Person2#는 #Person1#에게 자선 단체에서의 경험이 #Person2#의 사고 방식에 영향을 미쳤다고 말합니다.',\n"," '#Person1#은 #Person2#에게 언제쯤 결정을 내려줄 수 있는지 물어봅니다.',\n"," '#Person2#는 #Person1#의 추천에 따라 베이징 오리구이를 주문했습니다.',\n"," '안젤라는 댄에게 어제 메시지를 받았지만 메시지를 받지 못했다고 말한다. 댄은 메건도 초대하고 싶어서, 그들은 함께 차를 타고 결혼식에 갈 예정이다.',\n"," '#Person1#과 #Person2#는 달달한 디저트를 먹고 있습니다. 그들은 이탈리아 티라미수와 바나나 튀김을 먹을 예정입니다.',\n"," '스미스 씨는 #Person1#에게 27살이며 예일 대학교를 다녔고 경제학 학사 학위를 가지고 있다고 말합니다. 그는 지난 5년 동안 은행에서 일했습니다.',\n"," '#Person1#은 조카를 위해 좋은 것을 사고 싶어합니다. #Person2#는 #Person1#의 도움으로 디지털 바비를 구매합니다.',\n"," '#Person1#은 #Person2#가 스포츠 신발을 사는 것을 도와줍니다.',\n"," '#Person2#는 과학 박물관에 가고 싶지만 길을 잃어버려서 티켓 기계를 어떻게 사용해야 할지 모른다. #Person1#은 #Person2#에게 4번 플랫폼에서 기차를 타라고 조언한다.',\n"," '사이먼은 #Person1#에게 은퇴 후 가족과 더 많은 시간을 보냈다고 말한다. 그는 단계적 은퇴 프로그램에 참여하여 프로젝트 작업을 신청할 수 있으며, 임시 직원을 고용하지 않고 은퇴한 직원들이 접근할 수 있는 웹사이트에 채용 공고를 올린다. 회사는 그에게 유연성을 제공한다.',\n"," '로키는 #Person1#에게 애정이 많고 모든 요구사항을 충족시키는 여자를 좋아한다고 말한다. #Person2#는 결혼할 수 없을 것 같다고 생각한다. 로키의 생각은 외향적이고, 사람들의 차이를 비판하지 않는 여자를 선호한다.',\n"," '#Person1#과 #Person2#는 어제 밤에 심한 폭풍에 대해 이야기하고 있습니다. #Person1#은 폭풍을 싫어하고 봄이 왔으면 좋겠다고 바랍니다.',\n"," '#Person1#은 TV를 보는 것이 지루하다고 생각하지만, #Person2#는 그것이 #Person1#에게 좋지 않다고 생각한다. 그들은 함께 나눌 수 있는 것을 제시할 것이다.',\n"," '#Person1#은 내일 수업에 대해 걱정하고 있다. 벤은 #Person1#에게 새로운 학교 생활에 적응하기 위해 숙제와 아침 독서 시간에 20분이 필요하다고 말한다. 그리고 휴식 시간에 먹을 것을 사올 것이다.',\n"," '애덤은 #Person1#에게 던지기 연습을 하고 토요일 경기에서 미시간에 대해 기대하고 있다고 말한다. 그들은 경기를 보기 위해 전체 훈련에 참여할 예정이다.',\n"," '#Person1#은 프린터가 고장나서 #Person2#에게 복사본을 출력해달라고 요청합니다.',\n"," '#Person1#이 #Person2#에게 커튼을 거는 것을 도와달라고 요청한다.',\n"," '잭은 #Person1#에게 모두에게 적합한 주말이 다다음 주말이라고 알려줍니다.',\n"," '#Person2#는 큰 산불을 끄기 위해 밤과 밤을 가리지 않고 일했습니다. #Person1#은 #Person2#가 임신했다는 것을 발견했습니다. 왜냐하면 이 아기는 자신의 아기가 아니기 때문입니다.',\n"," '#Person2#는 #Person1#에게 #Person2#의 딸이 대학에 결정하지 못하고 있다고 말한다. #Person3#는 그녀가 스스로 결정을 내릴 수 있도록 격려한다.',\n"," '#Person1#은 #Person2#에게 직장을 잃으면 생계를 유지하지 못하게 될 것이라고 말한다. #Person1#의 상사는 부정직한 사람이고 모든 것이 그의 잘못이라고 주장할 것이므로, 그들은 돈을 절약하면서 살기로 결정한다.',\n"," '#Person1#과 #Person2#는 에든버러 대학교의 박사과정 학생이 #Person1#의 연구실에서 하는 멀티모달 텍스트 생성에 관한 강연에 대해 이야기하고 있다.',\n"," '#Person1#은 #Person2#에게 존의 집에 가자고 제안하지만, #Person1#는 동료 중에 한 명이 감염되었다고 생각한다.',\n"," '#Person1#은 파버 씨가 요크 호텔에 3박 동안 더블룸을 예약하는 것을 도와줍니다.',\n"," '#Person1#은 웨스트 더비를 추천하고 저렴한 단칸방 연립 주택을 원합니다. #Person2#는 존 고드프리를 소개하고 존이 일요일에 그를 만날 수 있을 것이라고 말합니다.',\n"," '#Person1#과 #Person2#는 계속해서 모든 잘못을 #Person1#에게 돌리고 있다. 그들은 기차를 타는 대신 운전하자고 제안하지 않았다면 그들은 괜찮았을 것이라고 생각한다.',\n"," '댄은 주문한 컴퓨터가 2일이나 지연되었습니다. #Person1#은 댄에게 스티브에게 전화해서 확인해달라고 요청할 것입니다.',\n"," '#Person1#과 #Person2#는 워싱턴 포스트에 대해 이야기하고 있습니다.',\n"," '#Person2#의 컴퓨터에 바이러스가 침투했다. #Person1#은 #Person2#에게 텍스트 포트 번호를 알려주고, 이메일 용량을 초과한 첨부파일을 압축하여 보내라고 요청했다.',\n"," '#Person1#은 주말에 #Person2#를 초대하여 즐겁게 보냈습니다.',\n"," '#Person1#은 햄버거, 중국 음식, 그리고 판다 익스프레스를 먹고 싶어합니다. #Person2#는 중국 음식을 사올 것입니다.',\n"," '메리는 톰에게 판매 직원 자리를 다른 사람에게 제공하기로 결정했다고 알립니다. 톰은 메리에게 기회를 달라고 요청합니다.',\n"," '#Person1#은 #Person2#에게 열 번째 번호로 경찰에 신고하라고 요청합니다.',\n"," '#Person2#는 음악 선생님이 되고 싶어합니다. #Person1#은 음악과 예술 분야의 학사와 석사 학위를 가지고 있으며, 클래식 음악을 좋아한다고 #Person2#에게 말합니다. 클래식 음악은 두뇌에 좋다고 알려져 있으며, 학생들이 스트레스를 받을 때 더 많이 듣는 것이 도움이 된다고 설명합니다. 그 후 그들은 인터넷에서 가장 관심이 가는 것을 찾아보는 것을 제안합니다.',\n"," '#Person1#과 #Person2#는 집 아래에 사는 여자에 대해 이야기하고 있습니다. #Person1#은 그녀가 싱글이며 언젠가 그녀를 저녁 식사에 초대하려고 생각하고 있습니다.',\n"," '#Person1#과 #Person2#는 미렐라가 사무실에서 집처럼 편하게 지내고 있다는 사실에 대해 이야기한다. 그들은 캘리포니아의 업무 분위기가 훨씬 더 편안하고 캐주얼하다고 생각한다.',\n"," '#Person1#은 법률 사무소를 세우고 싶어합니다. #Person2#는 #Person1#에게 조언을 구합니다.',\n"," '케이트가 피터에게 어젯밤 온라인 게임을 그만두고 휴식이 필요하다고 말한다.',\n"," '#Person1#과 #Person2#는 배와 보트가 예전처럼 교통수단으로 중요하지 않다는 것에 동의합니다.',\n"," '앤디는 패니에게 어젯밤 무서운 악몽을 꾸었다고 말한다. 패니는 앤디에게 그녀가 미시간 대학교에 들어가는 것에 대해 걱정하고 있다고 말하고, 앤디가 다시 잠들기 무서워하는 것을 설명한다.',\n"," '#Person1#과 #Person2#는 밴드를 시작할 예정이다. #Person1#은 힙합을 좋아하기 때문에 바닐라 아이스의 노래를 연주할 것이다.',\n"," '#Person1#과 #Person2#는 오늘 밤 뉴올리언스 여행을 계획하고 있습니다. 그들은 강변 보트 투어, 생어 극장에 가는 것에 대해 이야기합니다.',\n"," '#Person1#은 #Person2#의 사이즈를 확인하고 신용카드를 사용합니다.',\n"," '#Person1#은 블레이크 씨에게 포스터 씨가 훈련 매뉴얼을 보내주길 원한다고 말합니다. #Person2#는 그것이 어렵다고 생각합니다.',\n"," '데이비드는 이번 휴가 때 솔트레이크시티로 여행을 갈 예정입니다. #Person1#은 그에게 충분한 침낭과 현지 음식을 준비하라고 조언합니다.',\n"," '#Person1#은 #Person2#에게 파멜라의 비행기를 호출하고 그녀를 돌봐달라고 요청합니다. #Person1#의 회사가 파산하자마자 친구라는 대부분의 사람들이 자신을 떠났기 때문입니다. 그런 다음 그들은 좋은 비행이 되길 바랍니다.',\n"," '#Person2#는 #Person1#에게 이 도시에서 가장 붐비는 거리에 대해 알려줍니다.',\n"," '#Person1#과 #Person2#는 프로그램에 대해 이야기하고 있다. #Person1#은 무슬림들이 메카로 순례를 가는 이유에 대해 설명하고, 사우디아라비아에서는 사고를 줄이기 위해 순례자 수를 제한하고 있다고 말한다. 프랑스에서는 사람들이 치유받으러 가는 루르드가 있다고 한다.',\n"," '#Person1#은 #Person2#가 센트럴 백화점으로 가는 길을 물어보는 것을 도와줍니다.',\n"," '#Person1#과 #Person2#는 다음 주에 런던에 가는데 함께 가기로 결정한다. #Person1#은 기차를 타는 것이 더 빠르다고 생각하지만, #Person3#는 고속버스를 타는 것을 선호한다.',\n"," '톰은 평소처럼 붐볐지만 패스트푸드가 건강에 좋지 않다고 생각하기 때문에 샌드위치를 먹지 못했다. 캐서린은 톰에게 맥도날드나 KFC와 같은 최고의 브랜드가 모두 미국 것이라고 말한다. 왜냐하면 미국인의 3분의 2는 그런 곳을 피할 것이기 때문이다.',\n"," '#Person1#은 #Person2#의 도움으로 바비큐 윙과 베이비 백 립을 주문했다.',\n"," '#Person1#은 #Person2#의 도움으로 더블 치즈버거, 용수철 감자, 라지 사이즈, 음료 주문을 도와줍니다.',\n"," '#Person1#은 아침으로 머핀을 만들었고, #Person2#는 #Person1#에게 계란 한 쪽만 프라이해 달라고 요청했다.',\n"," '#Person1#은 #Person2#를 그랜드 호텔로 데려갈 것입니다.',\n"," '#Person1#은 아이들에게 미국 경찰이 어떻게 생겼는지 보여주고 싶어합니다. #Person2#는 동의합니다.',\n"," '#Person2#는 #Person1#에게 선셋 호텔에 전화하여 내일 빈 방을 예약하도록 요청합니다.',\n"," '#Person2#는 미국에 공부하러 가기 위해 비자를 신청했습니다. #Person1#은 신청서가 매우 복잡하고 필요한 서류를 준비하는 데 많은 시간이 걸렸다고 말했습니다.',\n"," '앤이 삼오년 동안 재미있게 놀았지만 다시 보고 싶지 않다고 말한다. #Person1#은 내일 밤 술집에서 앤을 만나자고 제안한다.',\n"," '메리는 #Person1#에게 온라인 쇼핑이 편리하고 시간을 절약할 수 있다고 말한다. 온라인 은행에 계좌를 개설하면 모든 물건을 온라인으로 살 수 있다.',\n"," '#Person2#는 미국식 회계에 익숙하지 않습니다. #Person1#은 #Person2#에게 회계 과정의 가장 기본적인 개념이 무엇인지 설명합니다.',\n"," '피터와 제인은 여름에 시안에 가는 계획에 대해 이야기한다. 피터는 모니카와 함께 가고 싶어하고 제인도 동의한다. 그들은 돈이 많으면 휴가를 어디로 가고 싶을지에 대해 논의한다.',\n"," '#Person2#는 로맨스 영화를 대여하고 싶어합니다. #Person1#은 #Person2#에게 비디오 대여 추적에 편리하기 위해 카드를 발급하고 회원 양식을 작성하라고 요청합니다. 그런 다음 그들은 영화 대여료에 대해 이야기합니다.',\n"," '#Person1#은 미스터 리를 찾아서 #Person2#의 책상 위에 두었습니다.',\n"," '#Person2#는 #Person1#에게 공항이 폐쇄되어 내일 아침까지 비행기가 뜨지 못한다고 말합니다. #Person2#의 나쁜 습관은 불을 켜놓지 않으면 잠을 자지 못하는 것입니다.',\n"," '#Person1#은 #Person2#에게 대접을 원하지만 거절당합니다. 결국, 그들은 식당을 찾기로 결정합니다.',\n"," '#Person2#는 성적 때문에 걱정 때문에 잠을 잘 못 자고 있다. #Person1#은 #Person2#에게 긴장을 풀기 위해 요가 수업을 듣거나 이완 요법을 배우라고 조언한다. #Person3#는 또한 학생들에게 음악을 듣는 것을 권장한다.',\n"," '#Person1#은 인테리어가 구식이라고 생각하지만 #Person2#는 유행을 따라가는 것이 아니라고 생각합니다.',\n"," '스털링은 월터에게 우드 교수님이 뛰어난 과학자라고 생각한다.',\n"," '#Person1#은 추가 검사에 대해 #Person2#에게 상담하기 위해 오후에 #Person1#을 방문할 예정입니다.',\n"," '마틴은 램 선생님에게 돈 벌기 위해 아르바이트를 하고 있다고 말한다. 그녀는 학생 복지 클럽에 대해 불만 없다.',\n"," '#Person1#은 #Person2#의 도움으로 특급으로 소포를 한국에 보내기로 결정했습니다.',\n"," '#Person1#은 린다에게 전화를 걸어달라고 요청한다. 린다는 #Person1#에게 결혼식에 휴대폰을 가져가지 못할 수도 있다고 말한다. 그들은 빵집에 전화할 것이다.',\n"," '#Person2#는 로스앤젤레스 출신이며 이틀 더 있을 예정입니다. #Person1#은 #Person2#에게 음료를 더 가져다 줍니다.',\n"," '#Person1#은 여름방학 동안 공부에서 벗어나 쉬고 싶어한다. #Person2#는 #Person1#에게 휴식을 취하라고 제안한다.',\n"," '메리는 #Person1#에게 오늘 아침에 우유를 배달하러 갔다가 12번지의 정원 문이 잠겨 메모를 읽지 못해서 큰 개가 울타리를 뛰어넘지 않고 정원에서 커다랗게 짖었다고 말한다.',\n"," '#Person2#는 #Person1#에게 예전에는 열정적이었지만 지금은 일을 더 오래 하고 새로운 기술을 배울 수 있게 되었다고 말합니다. #Person2#의 회사는 이제 승진 준비가 된 것 같다고 생각합니다.',\n"," '#Person1#은 당좌예금 계좌를 개설하고 싶어합니다. #Person2#는 #Person1#에게 이자를 지급하지 않는다고 말하고, 돈을 인출하는 방법을 알려줍니다.',\n"," '#Person1#은 중국 요리를 먹어보고 싶어합니다. #Person2#는 #Person1#에게 중국에는 유명한 요리가 여덟 가지가 있다고 말하고, 그 중에서 광동 요리와 베이징 오리 구이를 먹어보라고 추천합니다. 식당은 전취덕 식당에 있습니다.',\n"," '#Person1#은 #Person2#에게 가격은 동의했지만 배송이 느리다고 말합니다. #Person1#이 양모가 빨리 도착하기를 바랍니다. 그래서 어젯밤에 오클랜드에 전화하여 주문 물량을 싣는 데 동의했습니다.',\n"," '#Person1#은 독서등을 깨뜨렸습니다. #Person2#는 #Person1#에게 비용을 지불하고 새 것을 가져다 줄 것입니다.',\n"," '#Person1#과 #Person2#는 2006 회계 연도 마케팅 계획에 대해 이야기하고 있습니다. 그들은 해외 시장에서 배송을 두 배로 늘리는 목표를 설정했습니다. #Person1#은 데이터를 보면서 고려해야 할 세 가지를 설명합니다.',\n"," '#Person1#은 휴가 동안 유럽을 여행할 계획을 세우고 있습니다. #Person2#는 #Person1#에게 어디로 갈 것인지 알려줍니다.',\n"," '폭스씨는 중형 차량 예약을 했습니다. #Person1#은 그에게 예약의 이유를 설명하지만 그는 차량을 여기에 보유하게 하는 것이라고 말합니다. 그는 컴팩트나 SUV를 원한다면 예약할 수 있다고 말하고, 보험을 들어줄 것을 약속합니다.',\n"," '#Person1#은 아빠가 공부방에서 일하고 있기 때문에 엄마에게 카드 게임을 하자고 제안하지만, 엄마는 소리 내지 말라고 경고한다.',\n"," '존스 씨가 앤에게 커피를 만들어주고, 이번 주 회의 일정을 알려줍니다.',\n"," '#Person2#는 지리 수업을 위해 세계 지도를 보고 있다. #Person1#는 지리적 특성만 보여준다고 말하고 #Person2#에게 인터넷에서 더 많은 정보를 찾아보라고 조언한다.',\n"," '#Person2#는 금연 방을 예약했지만 방에서 담배 냄새가 너무 심해서 방을 바꾸고 싶어합니다.',\n"," '#Person1#은 빌에게 젖은 페인트에 손대지 말라고 요청합니다. 빌은 존 샘슨처럼 무심코 행동하지 않겠다고 약속합니다.',\n"," '벤과 엘라는 즐거운 시간을 보냈습니다. 그들은 다음 주에 다시 만날 예정입니다.',\n"," '짐은 빌에게 멋진 딕이 이탈리아에서 돌아온 후 아팠다고 말한다.',\n"," '#Person1#은 #Person2#의 회사가 #Person1#의 생명력을 빨아들이고 있다고 불평한다. 그들은 최저임금과 야근에 대한 추가 급여를 받지 못한다. #Person3#는 보너스를 주는 것이 동기부여가 되지 않는다고 생각한다.',\n"," '리사는 #Person1#에게 마크의 남편이 두 달 동안 다른 사람을 만나고 있었다는 것을 알게 되었다고 말한다. 남편은 처음에는 마크에게 어이없는 변명을 했지만, 그 후에 작은 실수를 인정했다.',\n"," '#Person2#는 #Person1#에게 의사 선생님이 적색 육류를 줄이고 식단을 정리하라고 조언했다고 말했다.',\n"," '#Person1#은 가야 한다. #Person2#는 가야 한다고 #Person1#에게 말하지만 #Person3#는 거절한다.',\n"," '#Person1#은 #Person2#에게 도서관 이용 방법과 영어 회화에 관한 두 권의 책을 어디에서 빌릴 수 있는지 물어봅니다.',\n"," '#Person1#과 #Person2#는 오늘 날씨가 좋다고 생각합니다. 그들은 점심을 먹기 전에 날씨가 좋다면 어디를 갈지에 대해 이야기합니다.',\n"," '#Person1#은 박물관에서 사진을 찍을 수 없습니다. #Person2#는 #Person1#에게 카메라를 돌려주고, 슬라이드나 그림 엽서를 어디서 살 수 있는지 알려줍니다.',\n"," '#Person2#는 대출 신청에 대한 정보를 얻고 싶어합니다. #Person1#은 #Person2#의 신용 점수가 매우 낮다는 것을 발견하고, 은행은 대출을 승인하기 위해 개인 정보, 과거 대출, 자산 및 신용 점수 등의 정보를 평가해야 한다고 말합니다.',\n"," '모니카는 #Person1#에게 발표가 성공적이었으며 고객들이 프로젝트에 돈을 투자하게 될 것이라고 말한다. 그는 또한 동료들의 도움에 대해 감사의 말을 전한다.',\n"," '톰은 아침에 달리는 것을 좋아하므로, #Person1#은 그에게 내일 함께 조깅하러 가자고 제안한다.',\n"," '#Person2#는 #Person1#에게 일본 레스토랑의 메뉴와 영업 시간을 알려줍니다.',\n"," '#Person1#은 심슨 씨와 점심을 먹기 위해 목요일에 만나기로 했습니다.',\n"," '#Person1#은 데이트를 레스토랑에 데려가고 싶어합니다. #Person2#는 #Person1#에게 그래마시 타번을 추천합니다. 그들은 테이블을 예약할 것입니다.',\n"," '#Person2#는 #Person1#에게 오리구이, 돼지고기, 소고기, 맥주를 포함한 식료품 가격을 알려줍니다.',\n"," '#Person2#는 #Person1#에게 #Person2#의 나라에서 축구, 농구, 테니스, 그리고 익스트림 스포츠가 인기가 있다고 말합니다. 그들 나라에서는 많은 사람들이 골프를 즐기지만, 일부 사람들은 시도해보는 것이 두려워합니다.',\n"," '#Person2#는 헬싱키, 핀란드로 가는 비행기 예약을 하고 싶어합니다. #Person1#은 #Person2#에게 가장 저렴한 비행기를 예약해달라고 요청합니다. 그들은 비용과 시간에 대해 논의합니다.',\n"," '#Person1#과 #Person2#는 내일 플로리다로 가서 할머니를 방문할 예정입니다.',\n"," '#Person1#은 호주에 가고 싶어합니다. #Person2#는 #Person1#에게 특히 대장벽 산호초를 추천합니다.',\n"," '로라는 헬스장에서 #Person1#에게 몸매 관리를 위해 운동하는 여성들이 많다고 말한다. #Person2#는 운동과 즐거움을 위해 많이 걷는 편이다. 그들은 스포츠에 대해 이야기하고 언젠가 함께 가기로 한다.',\n"," '#Person2#는 시카고에서 태어나 고등학교를 졸업했습니다. #Person1#는 #Person2#가 유럽에 갔다가 뮌헨에 살았다고 말합니다.',\n"," '#Person1#은 큰 쇼핑 센터에서 #Person2#의 새로운 정장을 샀습니다.',\n"," '앤은 마사 자전거 클럽의 회장인 로빈에게 산호수 자전거 투어에 대해 설명합니다. 로빈은 사이클리스트들이 출발할 때 도로를 막는 문제를 피하기 위해 분산 출발을 도입했다고 말합니다.',\n"," '#Person1#은 #Person2#에게 애퍼티프, 컴파리, 시그니처 음료, 싱거, 라임 주스, 그레나딘을 주문하는 데 도움을 준다.',\n"," '그레고리는 #Person1#과 함께 점심을 먹으러 가자고 제안한다. 그들은 번지 점프에 대해 이야기하다가 에릭이 지갑을 잃어버린 것 같다고 생각한다.',\n"," '#Person2#는 제2차 세계 대전에 대한 정보를 찾고 싶어합니다. #Person1#은 #Person2#에게 노르망디 상륙작전에 대해 알려줍니다.',\n"," '#Person2#는 #Person1#에게 훈련을 시작했지만 어디서부터 시작해야 할지 모른다고 말한다. 그는 근육질이 되고 싶지 않으며 웨이트와 유산소 운동을 선호한다.',\n"," '#Person2#는 #Person1#에게 캠퍼스 서쪽에 주차할 공간이 있다고 말합니다. #Person2#가 그곳에 갔을 때는 가득 차 있었지만 지금은 가득 찼습니다.',\n"," '수잔이 에밀리에게 급여에 대해 묻는다. 에밀리는 FICA와 SUI Y 세금이 무엇인지, 그리고 건강 보험 계획에 대한 공제가 무엇인지에 대해 설명한다. 수는 또한 연방 공제 아래에는 주 공제와 연방 급여가 있다고 말한다. 수는 이것이 영국에서도 마찬가지라고 생각한다.',\n"," '#Person1#은 기계로 물건을 어떻게 사는지 알고 싶어합니다. #Person2#는 #Person1#에게 자판기를 신뢰할 수 없다고 말합니다.',\n"," '#Person1#은 겨울 휴가로 태국에 여자친구를 만나러 가려고 한다. #Person2#는 #Person1#에게 인터넷이 상호작용을 훨씬 빠르게 만들어 사람들이 훨씬 더 빨리 서로를 알게 한다고 말한다.',\n"," '제임스는 케이트에게 새 가구를 사고 장식을 다시 하기 위해 돈을 모았다고 말한다. 그는 카펫을 좋아하지만 너무 더러워서 청소를 두 번 했다.',\n"," '빌은 아침 식사 전에 조깅을 하고 점심시간에 걸어가야 하며, 일주일에 세 번 수영이나 핸드볼을 해야 합니다.',\n"," '마르켓은 예비 의학을 공부하고 있기 때문에 과학 과목에 등록하고 싶어합니다. #Person1#은 생물학, 화학, 지도학, 물리학을 수강해야 한다고 제안합니다.',\n"," '팀은 #Person1#에게 그의 프로젝트와 더욱 친환경적인 삶을 어떻게 이끌어갈 것인지에 대해 이야기한다. 팀과 그의 가족은 환경에 대해 많이 배웠다. 그들은 또한 학교에 있을 때 친환경적으로 어떻게 행동하는지, 그리고 학교 카페테리아에 대해 논의한다.',\n"," '#Person1#은 토니에게 크리스마스가 다가오고 있다고 말한다. 토니는 쇼핑 센터의 장난감 부서에서 일하고 있으며, 크리스마스 전에 아이들을 위한 선물을 파는 것이 끔찍하다고 느낀다.',\n"," '빌이 생일 파티를 위해 케이크를 만들어 준다. 수는 빌에게 알레르기 반응을 피하기 위해 특정 음식을 먹지 말라고 조언한다. 빌은 #Person1#에게 샐러드를 가져다 달라고 요청하지만, 수리는 거절한다.',\n"," '#Person1#은 여름 옷을 사고 싶어합니다. #Person2#는 #Person1#에게 할인된 티셔츠와 어울리는 치마와 바지를 추천합니다.',\n"," '#Person2#는 72시간 이내에 재확인하지 않는 모든 예약은 취소된다고 #Person1#에게 알려줍니다.',\n"," '#Person1#과 #Person2#는 부활절에 교회에 가고, 뷔페 파티를 열고, 아이들은 집에서 기다린다.',\n"," '#Person1#과 #Person2#는 토니의 나쁜 습관에 대해 이야기하고 있다. #Person1#은 토니에게 보상함으로써 토니가 협력하도록 동기를 부여하라고 조언한다.',\n"," '#Person1#은 #Person2#에게 줄리와 알렉스가 결혼할 것이라고 말한다. #Person1#의 친구 웬디는 그녀의 이별 파티에 가는 것을 신경 쓰지 않을 것이다. 왜냐하면 그녀의 친구는 웬디의 파티를 준비하고 있기 때문이다.',\n"," '#Person1#은 투표용지를 어디서 받아야 할지 모른다. #Person2#는 #Person1#에게 투표 부스로 가서 투표하라고 조언한다.',\n"," '#Person1#과 #Person2#는 회사의 인력 감축에 대해 이야기하고 있다. #Person1#은 조지, 앤디, 리사, 그리고 앤디가 해고될까 봐 걱정한다. 그들은 상사가 너무 엄격하다고 생각한다.',\n"," '#Person1#과 #Person2#는 이혼에 대해 이야기하고 있다. #Person1#은 중국 스타일 이혼이라는 중국 TV 시리즈를 보고 있다. 그들은 이혼이 빠르게 끝나는 경향이 있으며, 중국의 독자 세대는 결혼에 대한 관심을 거의 갖지 않는다고 생각한다.',\n"," '주디는 #Person1#에게 그들이 여행에 얼마나 많은 돈을 쓸지 계산해 달라고 요청한다. 그들은 교통비와 숙박비를 줄이기 위해 유스호스텔을 선호한다.',\n"," '#Person1#과 #Person2#는 메리와 제로드라는 결혼에 대해 이야기하고 있다.',\n"," '#Person2#가 지갑을 잃어버렸습니다. #Person1#은 #Person2#에게 50달러를 빌리려고 합니다. 그들은 함께 책을 사기 위해 갈 예정입니다.',\n"," '#Person1#은 #Person2#에게 머피 뮤직과 U-튠즈가 합병한다는 소문을 들었다고 말합니다. #Person1#는 그것이 믿을 수 없다고 생각합니다. 왜냐하면 그들은 합병하는 것이 더 엄격하기 때문입니다.',\n"," '톰은 새로운 비서가 도움이 된다고 생각하지만, 조는 거만한 것 같다고 생각한다.',\n"," '사라는 #Person1#에게 회의가 효율적이지 않았다고 말한다. 그는 회의의 주제에 대해 설명하고, 회의 중에 중요 포인트를 제기하고 회의 후에 직접 관련된 사람들과 이야기하는 것을 제안한다.',\n"," '#Person1#과 #Person2#는 프로그램에 대해 이야기하고 있다. #Person1#은 무슬림들이 메카로 순례를 가는 이유에 대해 설명한다. 그들은 또한 사우디아라비아와 영국에서 사람들이 칸터베리로 가고 기독교인들이 바티칸으로 가는 루르드가 치유되었다는 것에 동의한다.',\n"," '테드는 마이크에게 제니에게 고백할 용기가 없다고 말한다. 마이크는 테드에게 그녀가 어떻게 느끼는지 알려야 한다고 제안한다.',\n"," '#Person1#은 화가 났습니다. #Person2#는 #Person1#에게 화를 풀기 위해 음악을 듣는 것, 산책을 하는 것을 권장합니다.',\n"," '#Person1#은 아빠에게 뉴질랜드에 있는 가족에 대해 묻습니다. 아빠는 #Person1#의 삼촌 빌, 그의 아내와 두 딸, 그리고 사라의 계모라고 말합니다.',\n"," '스튜어트 씨가 도시 마라톤에서 우승한 것에 대해 #Person1#과 #Person2#모두 흥분하고 있습니다.',\n"," '#Person2#는 아이들과 아내를 위한 선물로 DENY 브랜드의 운동화를 사려고 합니다. #Person1#은 #Person2#에게 그것이 비싸다고 말하고, 그 이유를 설명합니다. 결국, 그들은 하나를 구매합니다.',\n"," '홍은 영국에 전화를 걸어야 하는데 로밍 요금을 감당할 수 없습니다. #Person1#은 홍에게 현지 SIM 카드를 사용하라고 제안합니다.',\n"," '브라운 씨는 #Person1#의 능력과 경험에 따라 한 달에 2,500 위안의 수수료를 지불할 것입니다.',\n"," '#Person1#과 #Person2#는 배리와 폴에 대해 이야기하고 있다. #Person1#은 배리를 낯을 가리지 않는 사람으로 생각한다. 그들은 금요일에 그들의 생일 파티에 참석할 예정이다.',\n"," '#Person1#은 집을 사는데 관심이 있습니다. #Person2#는 파사디나나 아카디아에 관심이 있으며, #Person1#에게 딱 맞는 집을 찾아줄 것입니다.',\n"," '#Person1#은 그랜트 씨에게 샘플들을 남겨두고 가라고 요청합니다.',\n"," '#Person1#은 #Person2#에게 PHS에서 Sons까지 가는 방법을 알려줍니다.',\n"," '이 씨가 워드 여사님을 집까지 태워다 줍니다. 이 씨는 감사해합니다.',\n"," '#Person1#은 #Person2#에게 자켓과 셔츠를 벗고 침대에 누워달라고 요청합니다. #Person1#의 어깨는 엑스레이를 찍는 것이 좋을 것 같습니다.',\n"," '셸리가 IBA 고객 서비스실에 전화를 걸어 카드를 잃어버렸다고 알립니다. #Person1#은 그녀의 요금을 내기 위해 사용했기 때문에 다시 받지 못했습니다.',\n"," '#Person1#은 잠에서 깨려고 한다. #Person2#는 #Person1#에게 저녁을 시작할 때 전화기를 뽑아놓으라고 요청한다.',\n"," '#Person2#는 중국 음식을 먹고 싶어합니다. #Person1#은 #Person2#에게 앞에 있는 테이블을 내려주고, 중국 음식과 서양 음식을 주문하는 데 도움을 줍니다.',\n"," '#Person1#과 #Person2#는 식기를 가져오고 줄을 서기 시작합니다. 그들은 크림 케이크를 좋아하지 않습니다.',\n"," '루시는 스탠리가 노래하는 것을 듣고 싶어합니다. #Person1#은 그에게 엘비스의 노래를 틀어줍니다.',\n"," '#Person2#는 #Person1#에게 홈 비디오 플레이어가 영화관을 대체할 것이라고 말합니다.',\n"," '#Person1#은 #Person2#의 도움으로 갈색 드레스를 샀습니다.',\n"," '#Person1#은 아빠가 매주 토요일마다 돈을 잊어버려서 13달러를 빚졌다. 조슈아는 아빠의 비밀 돈통을 사용하여 일부를 저금하고 일부를 가난한 사람들에게 나눠줄 계획이다.',\n"," '#Person1#과 #Person2#는 마이크의 생일 파티에 갈 예정이다.',\n"," '#Person1#은 우편으로 보내고 싶어합니다. #Person2#는 #Person1#에게 우표 창구에서 받아야 한다고 말합니다.',\n"," '#Person1#과 #Person2#는 중국에서의 지진 경험에 대해 이야기하고 있습니다. #Person1#은 중국이 많은 파괴적인 지진에 시달리고 있다고 생각합니다. 그들은 모두 자연 재해에 익숙하지만 원춘 지진을 겪으면서 약해지고 있습니다.',\n"," '#Person1#과 #Person2#는 날씨에 대해 이야기하고 있습니다. #Person1#은 습하고 추운 날씨를 견디기 어렵다고 생각합니다. #Person3#는 봄이 왔으면 좋겠다고 바랍니다.',\n"," '#Person1#과 #Person2#는 배구 경기 후에 기념품 가게에 가서 쇼핑할 예정입니다. #Person1#은 가족들에게 선물을 사는 것을 고려하고 있습니다. 그들은 올림픽 마스코트를 사기로 결정합니다.',\n"," '#Person2#는 양이 많은 음식을 요리하기 위해 팬을 사고 싶어합니다. #Person1#은 #Person2#에게 같은 크기의 알루미늄 팬과 가벼운 나무 핸들을 보여줍니다.',\n"," '#Person1#과 #Person2#는 퇴근 후에 만나기로 했습니다.',\n"," '베커 씨는 #Person1#에게 워싱턴의 공무원들이 일을 즐기고 일은 흥미롭다고 말합니다. 그는 급여와 근로 조건이 매력적이라고 생각합니다.',\n"," '피트와 헨리가 낮잠을 자고 있습니다. 피트는 브루클린에서 태어나고 자란 헨리 존슨을 만나고 그들이 중국에서 생활하는 외국인임을 알게 됩니다. 그들은 오늘 저녁의 계획을 세우고 노래방에 가기로 결정합니다.',\n"," '줄리는 지난 일요일에 동창들 몇 명이 좋은 식당에서 점심을 먹었는데, 이것이 식중독의 원인이 되었습니다. 존스 선생님은 줄리가 지금은 좀 나아졌다고 생각합니다.',\n"," '마이크는 오늘 저녁에 친구들이 몇 명 올 예정이다. 엄마는 마이크에게 음료를 준비하고 커피를 내릴 때 커피와 설탕을 사오라고 요청한다.',\n"," '#Person1#과 #Person2#는 베를린으로의 긴 버스 여행에 대해 이야기하고 있다. #Person1#은 버스가 불편하긴 하지만 친환경적이라고 생각하기 때문에 괜찮다고 생각한다.',\n"," '#Person2#는 #Person1#에게 #Person2#가 여행을 좋아하고 사람들을 만나는 것을 좋아하며, 외국어를 할 줄 안다고 말합니다.',\n"," '에이미는 지미에게 어제 소풍에 갔고 베이하이 공원에서 점심을 먹었다고 말한다. 그리고 그들은 저녁에 무엇을 했는지에 대해 이야기한다.',\n"," '#Person1#은 피터에게 차를 마시러 오라고 요청하지만, 피터는 너무 건조해서 정원에 물을 줄 필요가 없다고 말한다.',\n"," '#Person1#은 #Person2#에게 가능한 한 빨리 비상 회의를 소집하라고 요청하고, 켄이 돌아오지 않을 경우 일정을 잡으라고 요청합니다.',\n"," '#Person1#은 #Person2#에게 편지에 붙일 우표를 사는 데 도움을 줍니다.',\n"," '#Person1#은 스미스 씨에게 감염을 치료하기 위해 항생제와 크림을 처방하고 약국에서 할인을 받을 수 있다고 말한다.',\n"," '#Person1#은 #Person2#에게 중국 특유의 것이면서도 가볍게 들고 다닐 수 있는 자수품을 사는 것을 추천합니다.',\n"," '#Person1#과 #Person2#는 에펠탑에 대해 이야기하고 있습니다.',\n"," '브라이언은 #Person1#에게 대학에서 영어를 배웠으며 캘리포니아에 가본 적이 없다고 말했다.',\n"," '#Person2#는 #Person1#에게 한 시간 전에 집에 있어야 했기 때문에 전혀 듣고 있지 않았다고 말합니다.',\n"," '#Person1#과 #Person2#는 일의 긍정적인 면을 보지 않기 때문에 큰 문제에 휘말릴 것이라고 생각한다.',\n"," '#Person1#은 #Person2#에게 존이 일주일에 일곱 번 그녀와 데이트를 한다고 말한다.',\n"," '#Person1#과 #Person2#는 런던에 대해 이야기하고 있습니다. 그들은 웨스트민스터 대성당, 보아디케아의 동상, 엘리자베스 1세 여왕, 마가렛 대처, 그리고 밀랍 인형 박물관을 방문할 예정입니다.',\n"," '다니엘은 이번 학기에 새로운 과목을 추가했고, #Person1#은 과학을 가장 좋아한다. #Person2#는 과학이 장난꾸러기 같다고 생각하지만, 실제로는 과학에 관심이 있다.',\n"," '#Person1#과 #Person2#는 베이비 샤워를 하고 있다. #Person1#은 베티가 준 선물과 칼라가 준 놀이펜, 그리고 아기가 언제 태어날지 맞춘 것에 대해 감사한다.',\n"," '#Person1#은 중국에 관광하러 가고 싶어한다. #Person2#는 지금은 안된다고 말한다.',\n"," '팀과 카렌이 서로 인사를 나눈다.',\n"," '#Person2#는 어제 밤에 마이클을 만나러 갔습니다. #Person1#는 #Person2#에게 마이클이 오토바이를 좋아하지만 자전거를 사고 싶어한다고 말했습니다.',\n"," '#Person2#는 #Person1#에게 중국어 외에 영어와 프랑스어를 할 수 있다고 말합니다. #Person2#의 영어 능력은 일반적인 사무 업무를 수행하는 데 충분합니다.',\n"," '낸시가 타일러네에 전화를 걸었습니다. 타일러는 앤디에게 메시지를 전달할 것입니다.',\n"," '#Person1#은 뉴욕행 513편 비행기가 취소되었다고 #Person2#에게 알립니다. #Person1#이 취소된 비행기를 예약하는 것을 거부합니다. 왜냐하면 다른 항공사로 예약하려면 줄을 서야 하기 때문입니다.',\n"," '#Person1#과 #Person2#는 버거퀸에 가서 치즈버거와 프렌치 프라이에 대해 이야기하고 있다. #Person1#은 음식을 주문하기 위해 기다려야 한다는 것을 깨닫고 배가 고프다.',\n"," '#Person1#은 초과 수하물 요금을 내야 합니다. #Person2#는 #Person1#에게 취약한 물품 표시를 달아달라고 요청합니다.',\n"," '#Person1#은 목이 말라 죽겠다. #Person2#는 #Person1#에게 소다가 목마름을 해소하지 못한다고 말한다.',\n"," '#Person1#은 매니저 량의 사무실에 전화를 걸어 CEO인 그린 씨가 량 씨와 약속이 있었지만 내일로 변경되었다고 #Person2#에게 알립니다.',\n"," '#Person1#은 유광 가죽 신발 한 켤레를 구매합니다.',\n"," '벤자민은 #Person1#에게 프로젝트 보고서를 어떻게 써야 할지 모른다고 말한다. 그는 보통 연구 보고서만 쓴다고 말하고, 그 이유를 설명한다. 그는 또한 마이크로소프트 워드를 어떻게 사용하는지 모른다.',\n"," '#Person2#는 #Person1#의 도움으로 해산물 피자를 두 개 주문합니다. #Person2#의 주소는 holyrood 9A입니다.',\n"," '#Person2#는 #Person1#의 추천에 따라 새우 칵테일, 토마토 수프, 미네랄 워터, 그리고 블랙 커피를 주문했습니다.',\n"," '#Person1#은 #Person2#에게 그 집에 대해 물어봅니다. 그 집은 멋진 거실, 넓은 식당, 그리고 편안한 침실 3개가 있습니다. 부엌은 현대적입니다.',\n"," '#Person1#은 #Person2#에게 개인용 스테레오의 앞면에 큰 긁힘이 있다고 말합니다. 이는 #Person1#이 구매할 때 상품을 확인하는 것이 고객님의 책임이기 때문입니다. #Person3#는 이 문제를 해결하기 위해 영수증을 보여줍니다.',\n"," '#Person1#은 내년 9월에 기숙사 방을 얻기 위해 주택 사무소에서 줄을 서야 했다. #Person2#는 #Person1#을 설득하여 캠퍼스 밖에서 살기로 결정했다.',\n"," '#Person2#는 중고 서점을 둘러보는 것을 즐긴다. 어라는 오래된 어린이 이야기 책을 #Person1#에게 보여준다. 그 책에는 1893년에 쓴 윌리엄 셰익스피어의 서명이 있다. #Person2#의 시집은 75센트짜리다.',\n"," '#Person2#는 베이징 중앙에서 #Person1#의 도움으로 20분 후에 베이징 오리를 주문합니다.',\n"," '#Person1#은 편지를 받고 싶어합니다. #Person2#는 #Person1#에게 공인 우편이나 등기 우편으로 보내는 것을 추천합니다.',\n"," '존이 샐리에게 톰의 편지에 대해 묻습니다. 존은 감사하다고 답장합니다.',\n"," '#Person1#은 좌절감을 느끼고 있습니다. #Person2#는 #Person1#이 스스로 컴퓨터를 구입하는 날을 기대하고 있습니다.',\n"," '#Person1#과 #Person2#는 날씨에 대해 이야기합니다. 그들은 이번 주말에 소풍을 가려고 합니다. 하지만 #Person1#은 비가 올 것이라고 생각합니다.',\n"," '#Person2#는 컴퓨터에 대한 일반적인 정보를 찾고 싶어합니다. #Person1#은 #Person2#에게 참고자료실을 보여줍니다.',\n"," '#Person1#은 프렌치 가든 레스토랑에 혼자 있습니다. #Person2#는 #Person1#에게 생수, 주스, 콜라를 주문하고 참치 샌드위치, 야채 수프 한 그릇을 주문합니다.',\n"," '#Person1#은 #Person2#에게 레모네이드, 바비큐 윙, 베이비 백 립을 주문합니다.',\n"," '#Person1#은 #Person2#에게 물과 디저트를 주문하라고 요청했습니다.',\n"," '#Person1#은 #Person2#에게 에릭이 로또에 당첨되면 세계 여행을 위한 티켓 두 장을 구매할 것이라고 말한다.',\n"," '잭이 #Person1#에게 자신의 강아지 사진을 보여줍니다. 잭은 그것이 귀엽다고 생각합니다.',\n"," '#Person1#은 엄마에게 제인 이모가 톰에게 자전거를 사줬다고 말한다. 엄마는 톰이 예의 바르게 행동하고 있다고 생각한다.',\n"," '#Person2#는 최신 치마를 찾고 있습니다. #Person1#은 #Person2#에게 400달러라고 말합니다.',\n"," '#Person2#는 #Person1#에게 #Person2#의 도시는 200년 전만 해도 그저 보잘것없는 작은 마을이었다고 말합니다. 하지만 지금은 큰 도시로 성장하고 있습니다.',\n"," '#Person1#과 #Person2#는 세계 지도자들이 모여서 행동 계획에 동의할 수 있기를 바랍니다. 그들은 대기 오염, 열대우림, 사막화 문제, 그리고 환경 개선 프로젝트에 대해 이야기합니다.',\n"," '데니스는 #Person2#에게 채팅방에서 많은 시간을 보내고 있다고 말한다. #Person1#은 데니스에게 새로운 온라인 친구에 대해 이야기한다. #Person3#는 데니가 이상적인 여자라고 생각한다.',\n"," '네이선은 이제 거의 시카고로 연습하러 갈 준비가 되었다. 그는 큰 신문사에서 일하는 것에 대해 걱정하지 않는다. #Person1#은 네이선에게 큰 도시에서 어떻게 일해야 하는지 알려준다.',\n"," '#Person2#는 3일 동안 토요타 캐롤라를 렌트하려고 합니다. #Person1#은 #Person2#에게 신분증을 보여주고 차 키를 복사하라고 요청합니다.',\n"," '#Person1#은 사업 계약을 체결하기 위해 뉴욕에 갈 예정입니다. #Person2#는 #Person1#에게 UN 건물, 컬럼비아 대학교, 그리고 관광객 안내소를 방문하라고 추천합니다.',\n"," '#Person1#은 #Person2#에게 필름을 현상해 달라고 요청합니다.',\n"," '#Person1#은 비행기가 폭우로 인해 지연되었다고 #Person2#에게 알렸습니다. #Person1#의 비행기는 정오 전에 비가 그칠 예정입니다.',\n"," '#Person1#은 #Person2#에게 지도에서 베이징 대학교로 가는 길을 물어봅니다.',\n"," '#Person2#는 #Person1#에게 신문에 나쁜 소식이 많다고 말합니다. 일기 예보는 #Person2#가 아직 모르고 스포츠 란에는 스포츠 점수가 나와 있습니다.',\n"," '#Person1#은 컴퓨터 게임이 너무 폭력적이기 때문에 싫어하지만, #Person2#는 소년들이 컴퓨터 게임을 통해 컴퓨터를 사용하는 법을 배울 수 있다고 생각한다.',\n"," '짐과 #Person1#은 저녁 식사 후에 맥주를 마시며 휴식을 취하기로 결정한다. 그들은 메리와 샐리가 종종 그곳에 가서 탁구를 친다고 들었다. 그들은 그들에게 춤을 추러 가기로 결정했다.',\n"," '#Person1#과 #Person2#는 찐 새우, 닭발, 와인을 주문합니다.',\n"," '#Person1#은 #Person2#에게 안내 경험이 많지는 않지만 몇몇 외국인 관광객 그룹을 안내했다고 말한다.',\n"," '잭이 #Person1#의 도움으로 더블룸을 예약했다.',\n"," '질이 마크에게 전화를 걸어 어제 밤 데이비드의 생일 파티에 대해 이야기한다. 마크는 질에게 어제 밤에 아들을 낳았다고 말한다. 그들은 내일 다시 술을 마실 예정이다.',\n"," '#Person1#과 #Person2#는 내일 밤 극장에 가기로 결정했다.',\n"," '#Person1#과 #Person2#는 야구 경기를 보기 위해 핫도그와 맥주를 마시고 있습니다. #Person1#은 볼티모어가 이기고 있다고 말합니다.',\n"," '#Person1#은 #Person2#의 도움에 대해 감사의 말을 전합니다.',\n"," '#Person2#는 #Person1#에게 #Person2#의 나라에서 천연자원을 수출하는 방법과 그들의 사회 기반 시설에 투자하는 방법에 대해 설명합니다.',\n"," '#Person1#과 #Person2#는 예술 전시회에 대해 이야기하고 있습니다. #Person1#은 내일 국립 갤러리에서 열리는 그리스와 로마 조각의 전시회를 좋아합니다. 그들은 함께 가고 싶어합니다.',\n"," '#Person2#는 책을 반납하고 비디오를 대출하고 싶어합니다. #Person1#은 #Person2#에게 도서관 카드를 관리하라고 요청합니다.',\n"," '#Person1#은 #Person2#에게 볼링 게임에 대해 더 자세히 알려달라고 요청합니다.',\n"," '#Person1#은 #Person2#에게 송금할 수 있는 새로운 서비스에 대해 설명합니다. #Person1#의 증권사는 맞춤형 상담 서비스를 제공할 수 있으며, 투자에 대한 정확한 계획을 제공합니다.',\n"," '#Person1#은 짐을 둘 곳이 필요합니다. #Person2#는 #Person1#에게 VISA를 제시하여 보증금을 지불하라고 제안합니다.',\n"," '#Person1#은 파티에 세 명을 초대합니다. #Person2#는 뷔페와 차가운 요리, 그리고 알코올 음료에 대해 알려줍니다.',\n"," '#Person2#는 해외에서 공부하는 모든 사람을 대상으로 하는 해외유학 대출을 신청하려고 합니다. #Person1#은 #Person2#에게 나이 제한 없이 55세 이상이면 가능하다고 알려줍니다.',\n"," '#Person2#는 #Person1#에게 영어 노래들을 찾는 데 몇 분 더 걸릴 수 있다고 말합니다.',\n"," '#Person1#은 #Person2#에게 신문사에서 이 직무에 관심을 가지게 된 이유를 설명합니다.',\n"," '#Person1#은 잭에게 면접 결과를 물어보는 것이 최선이라고 말한다. 잭은 또한 문의서가 간결하게 작성되어야 한다고 생각한다.',\n"," '#Person1#과 #Person2#는 #Person1#의 집을 둘러보고 있습니다. 그들은 나무에 매달린 옥수수 이삭에 대해 이야기합니다.',\n"," '#Person1#은 40분 후에 호텔에서 체크아웃할 예정입니다. #Person2#는 #Person1#에게 보증금을 납부하고 보관소에 수하물을 맡길 수 있다고 말합니다.',\n"," '브랜든은 #Person1#에게 새로운 웹사이트에 가입하고 글쓰기 기술을 향상시키려고 한다고 말한다. 처음에는 #Person2#가 이해하지 못하지만 나중에는 관심이 생기기 시작한다. 그런 다음 브렌든이 자신의 동생이 사기에 속는 것을 발견한다.',\n"," '#Person1#과 #Person2#는 새로운 실험실 건물에 대해 논의하고 있습니다. 그들은 교장이 지방 정부에게 도움을 청할 것이라고 믿습니다.',\n"," '헨리는 케이트에게 은행에서 준 신용카드를 가지고 다니게 된 것이 기쁘다고 말한다. 왜냐하면 카드로 결제한 물건에 대해 은행은 청구할 것이기 때문이다.',\n"," '스미스씨는 #Person1#에게 내일의 마을 방문에 대해 설명합니다. 그는 산악 지역의 마을을 방문하는 것이 좋을 것 같다고 생각합니다. 그들은 마을에서 점심을 먹을 예정입니다.',\n"," '벳은 스트레스와 우울증을 다루는 데 보통 오이를 먹고 잠을 자며, 아기를 가지게 되면 목표나 꿈을 이루는 데 방해가 된 적이 있는지 물어봅니다. 그녀는 변호사가 되고 유타를 떠나고 싶고 고등학교 졸업과 결혼을 계획하고 있습니다. 그녀는 청소년들에게 조언을 해주고 싶습니다.',\n"," '#Person2#는 #Person1#에게 9년 동안 우표 수집에 관심을 가지게 되었다고 말합니다. 그 이유는 첫 우표가 퀸 빅토리아의 사진이 그려져 있기 때문입니다.',\n"," '톰은 이제 꽤 성공한 사업가이며, 그의 가족은 1986년에 부다페스트를 떠나 캐나다에 정착했습니다. 그는 회사를 위한 자금이 약 1800만 달러 마련되었습니다. 그는 간단한 것들을 하고 자전거를 즐깁니다. 그는 낭비적인 일을 하지 않습니다.',\n"," '존 데이 형사 부장이 #Person1#에게 니스든의 보행자 지하도에서 발견된 40대로 추정되는 남자의 행동에 대해 설명합니다. 존 부장은 사망 원인이 다발성 두부 손상 때문일 수 있으며, 클럽이 문을 닫은 시간, 술집의 위치, 그리고 술집에서 취한 남자를 식별할 수 없을 수도 있다고 말합니다.',\n"," '셰리는 밥에게 친구의 결혼식에 참석하기 위해 내년 여름에 퀘벡을 방문할 것이라고 말한다. 밥은 #Person1#에게 영어를 연습하라고 조언한다.',\n"," '제인은 톰에게 학생 센터에서 수영하자고 제안하지만 톰은 논문 마감 때문에 거절한다. 그들은 저녁 식사를 하고 6시쯤 그릴에서 만나기로 한다.',\n"," '#Person1#은 등에 통증이 있다. #Person2#는 #Person1#이 시킨 대로 하루에 세 번 약을 복용할 것이다.',\n"," '#Person1#의 휴대폰이 먹통이 되었기 때문에 #Person1#이 프레드에게 200 위안을 빌리려고 합니다. 프레드는 동의합니다.',\n"," '#Person1#은 #Person2#에게 #Person1#의 글씨를 개선하기 위해 인내심을 가지라고 조언합니다.',\n"," '#Person1#은 #Person2#가 속눈썹을 집는 것을 무서워하고 있다.',\n"," '#Person2#는 주말 운전 수업에 대해 #Person1#에게 소개하고 코치는 보통 3명이며, 각 차량마다 코치 2명이 배정된다고 말한다.',\n"," '티나는 8년 동안 피아노를 배웠고 #Person1#에게 그 선생님을 소개해 달라고 요청했습니다.',\n"," '#Person2#는 #Person1#에게 #Person2#의 장점, 단점, 약점, 그리고 배울 수 있는 모든 것에 대해 묻는다.',\n"," '스테파니는 젠킨스 선생님이 다음 주 월요일 회의에 필요하다고 했기 때문에 내일 보고서를 제출해야 합니다. 조지는 그에게 잠이 필요하다고 말합니다.',\n"," '#Person1#과 데이비드는 크리스마스에 무엇을 할지에 대해 이야기하고 있다. #Person1#은 뉴욕에 있고 싶어하고, #Person2#의 아빠는 바바라가 크리스마스 파티에 초대했다고 말한다.',\n"," '밥은 지난 주말에 댄스, 쇼핑, 그리고 테니스를 즐겼다. #Person1#은 밥과 게임을 하기로 결정했다.',\n"," '#Person1#은 #Person2#에게 50달러를 빌리려고 한다. #Person1#는 일자리가 없으므로 집으로 돌아가서 아버지 농장에서 일해야 한다고 말한다.',\n"," '#Person1#은 #Person2#에게 재킷과 넥타이를 빌리려 했지만 #Person1#이 다른 사람으로부터 빌릴 수 없다고 하자 회의 시간을 9시 30분으로 미룹니다.',\n"," '#Person2#는 #Person1#에게 아이들이 휴가 캠프에서 어떤 활동을 했는지, 그리고 아이들의 운동 활동에 대해 이야기한다.',\n"," '#Person1#은 아빠에게 대학 입학 요건에 대해 묻습니다. 아빠는 #Person1#이 직장을 그만두고 컴퓨터 프로그래밍 쪽으로 진출하려고 한다고 말합니다.',\n"," '칼리나는 #Person1#에게 클라크 교수님이 차를 나무에 박아서 몇 일 동안 학교를 쉬어야 한다고 말한다.',\n"," '#Person1#은 집주인과의 문제로 세입자 지원 센터에 전화를 합니다. #Person2#는 #Person1#에게 집주인이 너무 오래 걸려서 수리를 하지 않는다고 말합니다.',\n"," '#Person1#은 터너 인테리어에서 수출용 L / C를 수령하기 위해 사유리 베드에서 와야 한다고 #Person2#에게 알려줍니다.',\n"," '미르달은 찰리가 지갑을 잃어버렸다고 생각한다. 찰리는 핫도그 판매대로 가기로 한다.',\n"," '#Person1#과 #Person2#는 번갈아 설거지를 하기로 합의했습니다.',\n"," '#Person1#과 #Person2#는 이 관계가 어디로 가고 있는지 알고 싶어한다. 그들은 서로에게 사랑을 표현한다. #Person1#은 미안해한다.',\n"," '#Person1#과 #Person2#는 정부가 많은 사회 문제에 직면해 있다고 생각합니다.',\n"," '#Person1#은 다음 토요일에 화려한 복장 파티에 참석하기 위해 #Person2#에게 쇼핑하러 가자고 제안한다. #Person1#의 사무실의 한 여성은 파티를 위해 캐나다, 캐나다 경찰용 카우걸을 구입할 것이다.',\n"," '#Person1#과 #Person2#는 베이징의 야간 생활에 대해 이야기합니다. 그들은 춤추러 가기로 결정합니다. #Person1#은 디스코 댄스를 제안하고 왈츠를 시도합니다.',\n"," '#Person1#은 사무실에서 9시부터 5시까지 일하는 것에서 벗어나고 싶어한다. #Person2#는 #Person1#이 농부가 되기 전에 많은 훈련이 필요하다고 생각한다.',\n"," '#Person1#은 #Person2#에게 하이네켄과 버드 반 파인트, 나초와 모짜렐라 스틱을 주문합니다.',\n"," '메리는 #Person1#에게 어제 밤 앤과 큰 싸움을 겪었다고 말한다. 앤은 메리에게 그녀의 우정을 신경 쓰지 않는다고 말했고, 메리의 남자친구도 앤이 이기적이라고 생각한다. #Person2#는 나중에 전화해서 화해할 것이다.',\n"," '#Person2#는 #Person1#에게 1층에 있는 가게에서 중국산과 외국산 담배를 판매하고 기념품을 판매한다고 말합니다.',\n"," \"캐서린은 '패스트 푸드 네이션'이라는 영화가 사색할 거리가 많은 영화라고 생각합니다. 톰은 패스트푸드가 건강에 좋은지 아닌지에 대해 신경 쓰지 않고 건강한 메뉴 옵션을 제공한다고 말합니다. 하지만 사람들은 급하게 식사를 할 때 그것에 대해 생각할 여유가 없습니다.\",\n"," '#Person2#는 #Person1#에게 버스가 그린위치 빌리지까지 피프스 애비뉴를 따라 가며, 워싱턴 스퀘어 파크로 가는 올바른 버스라고 말합니다.',\n"," '#Person2#는 9-11 테러 공격 당시에 베이징의 아파트에 있었습니다. #Person1#은 #Person2#에게 테러범들이 쌍둥이 타워를 충돌시키고 파괴하는 것을 보았다고 말합니다. 그들은 그날이 끔찍했을 것이라고 생각합니다.',\n"," '척 존스와 칼은 이웃이 되었습니다. 척은 시카고 출신이지만 이웃들과 친해지려는 사람이 별로 없습니다. 그들은 더 평화로운 공동체를 선호합니다.',\n"," '#Person1#은 #Person2#의 도움으로 계란과 토스트, 그리고 오렌지 주스를 주문했습니다.',\n"," '스티븐은 가계 예산을 살펴보기 위해 피곤해서 침대에 들어간다. 그는 #Person1#에게 잘 자라고 인사한다.',\n"," '#Person1#과 제인은 다음 주 월요일 오후 3시에 회의 전에 만나기로 결정했다.',\n"," '#Person2#는 다음 주 금요일이 그녀의 5주년이기 때문에 여자친구를 위한 선물을 고르는 데 도움을 줍니다. #Person1#는 그녀에게 진주 귀걸이 세트나 아름다운 하트 모양 펜던트를 선물하는 것을 제안합니다. 제시가 프로포즈하기 좋은 시기라고 생각해서, 그들은 약혼 반지를 볼 예정입니다.',\n"," '#Person1#은 오래된 포드 핀토를 매입하고 싶어합니다. #Person2#는 #Person1#에게 그 차가 매우 가볍지만 강력한 연료 효율성을 가지고 있다고 말합니다. 그 후 그들은 멋진 차량의 가격에 대해 이야기합니다.',\n"," '발람씨가 애플 코퍼레이션에 컴퓨터 엔지니어 직위를 제안하고 월급을 올려주겠다고 전화했습니다. #Person1#은 월 4,000 위안을 지급하기로 결정했습니다.',\n"," '#Person2#는 #Person1#에게 통합 자금 이체와 중앙 집중식 배분, 네트워크 결제 서비스에 대해 설명합니다. 그들은 내일 모든 서류 작업을 마치기로 결정합니다.',\n"," '#Person1#은 저녁에 배가 고프다. #Person2#는 #Person1#에게 간식을 만들어 먹으라고 제안한다.',\n"," '#Person1#은 피를 뽑고 백혈구 수치를 확인하기 위해 의사의 지시를 따릅니다. 의사는 터니켓을 달아줄 것입니다.',\n"," '스티븐이 세입자에게 전기가 나갔다고 말합니다. 셀러 씨는 전기를 복구하기 위해 회로 상자를 열고, 탄 퓨즈를 풀어서 좋은 것으로 교체하라고 제안합니다.',\n"," '파울라는 #Person1#에게 빌린 집주인이 임대료에서 차감된 비용을 요구하며 코너스 씨에게 퇴거시키겠다고 위협하고 있다고 말한다. #Person2#는 이 문제를 해결할 수 있을 것이라고 생각한다.',\n"," '#Person1#은 에어컨이 작동하지 않아서, #Person2#는 #Person1#에게 댄에게 수리공을 부르라고 요청합니다. 그들은 또한 주차장에서의 사고에 대해 이야기합니다.',\n"," '#Person1#과 #Person2#모두 일자리를 찾고 있다. #Person1#은 전기기사 수습생 프로그램 공고를 봤다.',\n"," '#Person2#는 #Person1#에게 사장님이 놀라운 능력을 가지고 어려운 상황에서도 올바른 결정을 내리는 데 뛰어나다고 말합니다.',\n"," '#Person1#은 탕 씨에게 투어 가이드 자격증을 보여줍니다. 탕 씨는 두 가지 언어와 러시아어를 할 수 있습니다.',\n"," '#Person2#는 #Person1#의 도움으로 순금 시계를 구입합니다.',\n"," '사라씨가 톰에게 전화를 걸어 딸 마리아와 아들 켄을 돌봐달라고 요청합니다. 톰은 동의합니다.',\n"," '에이미는 #Person1#에게 그녀의 첫 직장과 영업 부서에서의 근무 경험에 대해 이야기합니다.',\n"," '앤드류는 #Person1#에게 크리스마스 이후로 살이 찐 것 같다고 말한다. #Person2#는 자신이 너무 뚱뚱하다고 생각하고 와푸 다이어트를 하고 있다고 생각한다. 그 후 그들은 스팸 메일에서 정보를 찾는데, 그 정보는 400달러와 90달러이다. 그들은 아침에 균형 잡힌 식사를 하라고 조언한다.',\n"," '브라운 대학의 그렉 손더스가 메리에게 전화를 걸어 인상적인 성적 평균과 대학 스포츠에 관심이 있는지 물어봅니다. 메리는 배구를 하고 싶어합니다.',\n"," '#Person1#은 택시를 타고 기차역으로 갑니다. #Person2#는 #Person1#에게 천천히 조심해서 운전하라고 당부합니다.',\n"," '브라이언은 #Person1#에게 대학에서 영어를 배웠으며 라스베가스에 가본 적이 있다고 말했다.',\n"," '#Person2#는 새로운 건강보험에 가입하기 위해 건강검진을 받으러 옵니다. #Person1#은 #Person2#의 건강 상태를 확인하고 알레르기 검사와 혈액 검사를 제안합니다.',\n"," '#Person1#과 #Person2#는 복권에 당첨되면 무엇을 할지에 대해 이야기하고 있다. #Person1#은 세계 일주를 하고 싶지만 #Person3#은 월급 인상을 받을 가능성이 없으므로 거절한다. 그들은 맥주를 마시기로 결정한다.',\n"," '#Person1#은 결혼하고 싶지만 #Person2#는 아직 35살이고 아직 정착할 수 없다고 말한다. 왜냐하면 #Person1#이 에이미에게 수치심을 느끼고 위험에 빠뜨릴 것이기 때문이다. <unk>은 헛소리 그만하고 그냥 결혼하자고 제안한다.',\n"," '#Person1#은 이전에 사용하던 것과 같은 2000장의 명함을 인쇄하려고 합니다. #Person2#는 일주일 후에 찾아가겠다고 제안합니다.',\n"," '브라이언은 #Person1#에게 7시까지 체크인을 해야 하지만 대기열이 길어질 수 있다고 말한다. 그는 회의가 시작되기 전에 프로그램을 준비할 것이다.',\n"," '#Person1#은 추수감사절 저녁 식사에 폴을 다시 초대한다. 폴은 다음 주에 해야 할 일이 많기 때문에 거절한다. #Person1#의 여동생은 훌륭한 호박 파이를 만들 것이다.',\n"," '수잔은 #Person1#에게 존의 사촌이 대학에 면접을 보러 올 예정이며, 토요일에는 도서관에서 근무해야 해서 나갈 수 없다고 말한다. 존은 강당에서 음악 축제를 열어 존을 데려다 줄 계획이다.',\n"," '벤은 #Person1#에게 꽃꽂이의 일반 과정을 신청하라고 요청한다. 처음에는 거절하다가 결국 동의한다. 그들은 저녁 수업 정보를 공유하고 인도 요리를 시도하기로 결정한다.',\n"," '#Person1#과 #Person2#는 피자 체험에 가기로 결정한다. #Person1#은 브리짓과 킹피셔는 고기를 먹지 않지만, 아서는 안 먹는다고 말한다.',\n"," '진은 #Person1#에게 토요일 아침에 운전 면허 시험을 볼 예정이라고 말한다. 그는 시험을 통과하면 2016년형 혼다 어코드를 구매할 계획이다.',\n"," '#Person1#은 #Person2#에게 펜을 보여줍니다. 그들은 카드를 사용합니다.',\n"," '#Person2#는 #Person1#에게 회사의 기금 모금 행사는 재미로 몇몇 경쟁사들과 파트너십을 맺어 미국 암 협회를 위한 마라톤을 후원했다고 말합니다. 이것은 #Person2#의 회사에게도 좋은 일이었습니다.',\n"," '#Person1#은 #Person2#에게 피크 트램으로 가는 방법을 알려줍니다.',\n"," '#Person1#은 #Person2#가 향기로운 샌들우드 부채를 구매하는 데 도움을 줍니다.',\n"," '#Person2#는 회사의 관점에 큰 영향을 미치는 외부 요인은 인적 자원, 팀워크, 그리고 다른 부서 간의 조정이라고 생각합니다.',\n"," '#Person1#은 주제를 다루기 위해 다시 모였으면 좋겠다고 바랍니다. #Person2#는 오늘 오후 2시에 준비 회의를 가질 예정입니다.',\n"," '#Person2#는 #Person1#에게 강을 따라 가는 짧은 투어를 추천합니다.',\n"," '#Person1#은 #Person2#가 소고기 버거, 프렌치 프라이, 그리고 밀크 쉐이크를 주문하는 것을 도와줍니다.',\n"," '#Person1#과 #Person2#는 등 축제에 대해 이야기하고 있습니다. 그들은 등 축제 동안 등에 적힌 퍼즐을 풀어보는 것이 전통이라는 것을 알게 됩니다. #Person1#은 거대한 등을 보고 행운을 느낍니다.',\n"," '티나는 #Person1#에게 ABC 컴퍼니와의 첫 두 라운드 면접에 통과했다고 알립니다. 그들은 오늘 저녁에 축하 파티를 열 예정입니다.',\n"," '팀은 카렌에게 다시 만나자고 제안하고, 그들은 잘 가겠다고 한다.',\n"," '#Person1#과 #Person2#는 10시에 부서 회의가 있다. #Person1#은 몇 가지 문구를 준비하고, 줄리에게 몇 장의 사본을 요청하고, 사무실 근처에 있는 사무실에서 일하는 것을 즐긴다.',\n"," '#Person1#은 목욕을 하려고 합니다. #Person2#는 #Person1#에게 어떻게 사용하는지 물어봅니다.',\n"," '#Person2#는 기차에서 시간을 보내기 위한 책에 대해 #Person1#에게 조언을 구합니다.',\n"," '#Person2#는 #Person1#에게 읽는 법을 가르치는 것에 대해 어떻게 생각하는지 묻습니다. #Person2#의 친구는 78살이고 남미에서 새로 이민 온 사람이며, 읽는 것부터 시작해야 합니다.',\n"," '#Person1#은 #Person2#의 티켓을 찾으러 왔고, #Person1#의 예약 확인서를 가져왔습니다.',\n"," '#Person1#과 #Person2#는 건강한 음식을 만들기 위해 함께 저녁 식사를 만들기로 결정했습니다. 그들은 채소를 씻은 다음 잘게 썰고, 볶음용 팬을 데우고, 프라이팬을 따로 데울 것입니다. 또한 #Person1#은 건강한 현미를 싫어합니다.',\n"," '#Person2#는 #Person1#에게 부스에 있는 두 대의 전화기가 어떻게 작동하는지 설명합니다.',\n"," '톰은 돈이 부족해서 중고 컴퓨터를 사려고 생각하고 있다. #Person1#은 그에게 중고 물품이 새 것만큼 좋다고 말한다.',\n"," '#Person1#은 모건에게 중국 사람들이 보통 식당에서 남은 음식을 포장하는 것이 흔하지 않다고 말한다. 모건은 사람들이 음식을 적게 주문하기 때문에 낭비라고 생각한다. #Person1#과 #Person2#는 모건의 남은 음식에 대해 이야기한다.',\n"," '해리는 #Person1#에게 중국의 거리 시장에서 절대로 쇼핑하지 않을 것이라고 말합니다. 그는 지난 일요일에 야외 장터에서 백이십을 지불한 가방을 흥정 없이 바로 사버렸기 때문에 화가 났습니다. #Person2#는 해리에게 물건의 실제 가치에 대해 미리 알아두는 것이 중요하다고 조언합니다.',\n"," '#Person1#은 #Person2#에게 영어를 능숙하게 구사해야 한다고 생각하며, 영어 교육에 대해 이야기합니다.',\n"," '#Person2#는 새로운 계좌를 개설하고 싶어합니다. #Person1#는 #Person2#에게 당좌 계좌와 저축 계좌 모두 개설할 수 있으며 초과 인출에 대한 벌금이 없다고 말합니다.',\n"," '#Person2#는 #Person1#에게 더 많이 사면 특별 할인 쿠폰을 받을 수 있다고 말합니다. 설탕 3봉지를 사면 10펜스를 할인받을 수 있으며, 유효기간 동안 사용할 수 있습니다.',\n"," '스티브는 로빈슨 부인에게 쟈니가 깨는 것을 깨서 모든 접시를 청소하는 것을 도와줬다고 말한다.',\n"," '#Person1#은 기차 표를 잃어버려 상하이로 가야 합니다. #Person2#는 #Person1#에게 위치를 알려줍니다. 그 후 #Person3#는 편안한 좌석을 원하면 $610 RMB를 사용하라고 권장합니다. 그런 다음, 그들은 지갑을 확인합니다.',\n"," '앤은 #Person1#에게 다음 주에 뉴욕으로 여행을 가는데 비행 중에 항공 웰빙 프로그램을 하고 시차 적응도 전혀 하지 않았다고 말한다. 앤의 비행기는 광천수보다 더 인기가 있었고 많은 승객들이 운동을 했다.',\n"," '메리는 존의 파티에 가고 싶지만 일자리를 찾았다. 톰은 맥도날드에서 부점장이지만 다른 일을 찾고 싶어한다. 메리도 시도해볼 것이다.',\n"," '해리는 끔찍한 경험을 했다. #Person1#은 해리에게 차 번호를 기억하고 차는 사라졌다고 말한다. #Person2#는 그런 운전자들은 처벌을 받아야 한다고 생각한다.',\n"," '#Person1#은 자전거 상점을 인수한 케인씨를 인터뷰합니다. 케인은 자신이 항상 레이싱 자전거를 타고 수리하는 것을 좋아했다고 말합니다. 그는 정규 근무 시간을 지키지 않고 일찍 퇴근할 수 있으며, 함께 일하는 친구들을 고용했습니다.',\n"," '#Person1#과 #Person2#는 중년에 대해 이야기하고 있습니다. #Person1#은 중년의 가장자리 부분이 다라고 말합니다.',\n"," '#Person1#과 #Person2#는 멋진 식사를 마치고 디저트를 결정합니다. #Person1#은 사과 크리스프와 초콜릿 무스 케이크, 그리고 커피 네 잔을 주문합니다.',\n"," '#Person1#은 #Person2#에게 아래쪽 침대를 고르는 데 동전을 던져서 결정하는 것을 제안한다. <unk>은 또한 카운터 위에 스테레오를 놓을 수 있는 창가 옆의 책상을 가져가고, 여자들과 관련해서 엄청나게 운이 좋기 때문에 침대 옆에 하나를 놓는 것을 선호한다. 그들은 함께 놀기로 한다.',\n"," '머레이 씨는 #Person1#의 도움으로 도서관 카드를 발급받습니다. #Person2#는 <unk>의 면허증과 신청서를 확인합니다.',\n"," '#Person2#는 전형적인 9시부터 5시까지 일하는 직장인이며, 초과근무 수당을 받지 않습니다. 점심과 오후에 커피를 마시며 휴식을 취할 수 있습니다. #Person1#은 칸막이를 좋아하지 않으며, #Person2#가 신경 쓰지 않는다고 말합니다.',\n"," '앨리스는 #Person1#에게 세탁기, 건조기, 자판기 등 다양한 기계를 어떻게 사용하는지 가르친다. 닉은 자신이 열아홉 살이지만 옷을 빨래해 본 적이 없다고 말한다. 그는 자신이 대만 출신이라고 주장하고, #Person2#의 엄마가 더 열심히 공부해야 한다고 생각한다. 그녀는 자신이 어떻게 살 수 있을지 모르지만 배울 수 있다.',\n"," '<unk>과 #Person1#은 바빴다. 그들은 오늘 밤에 전화할 예정이다.',\n"," '케이티는 #Person1#에게 고객이 없을 때 항상 멍하니 서 있기 때문에 항상 일할 준비가 되어 있다고 말한다.',\n"," '#Person1#과 #Person2#는 할아버지의 생일 깜짝 파티에 대해 이야기하고 있습니다. 그들은 집에서 하는 것이 최선이라고 생각합니다. 그들은 생일 선물로 무엇을 준비해야 할지 결정합니다.',\n"," '로버트는 이탈리아 서비스업체에서 프리랜서로 일하고 있는 지안 루카 도나텔리를 소개한다.',\n"," '#Person1#과 #Person2#는 시골의 새들의 울음소리가 시끄럽다고 생각한다. 그들은 보통 산 근처에서 시간을 보낸다. #Person1#은 새의 배설물이 어디에나 있을 것이라고 생각하지만, #Person3#는 캘리포니아에는 없다고 말한다.',\n"," '#Person2#는 #Person1#의 도움으로 칠면조 샌드위치, 소고기 스프, 콜라를 주문합니다.',\n"," '제임스는 기차에 탑승하기 위해 짐을 거의 다 싸놓았다. #Person1#은 제임스에게 재킷을 입고 사진을 찍을 수 있는지 물어본다. #Person2#는 제임스가 쿠키는 가방에 넣지 않고 할아버지와 할머니를 위해 쿠키를 만들었다고 말한다.',\n"," '테드는 #Person1#에게 올해 휴가를 어디로 갈지 아직 결정하지 못했다고 말한다. 그는 중국에서 몇 주를 보낼 예정이며, 남편과 그 나라를 방문하고 싶어한다.',\n"," '#Person1#과 #Person2#는 오후에 맥도날드에 가서 노인을 위한 나라는 없 영화를 볼 예정입니다.',\n"," '<unk>의 집이 털렸습니다. #Person1#은 경찰에 전화할 예정이지만 #Person2#는 기다려야 합니다.',\n"," '잭이 찰리에게 비디오 게임을 제안한다. 찰리는 잭에게 캐릭터를 만드는 게임을 추천한다. 잭은 그것을 보고 싶어한다.',\n"," '#Person2#는 컨트리 음악에 관심을 가지게 된 계기와 라디오 방송국에서 일하기 시작한 시기에 대해 #Person1#에게 이야기합니다.',\n"," '앨리스는 세탁기, 건조기, 비누를 사용하는 방법을 #Person1#에게 알려줍니다. #Person2#는 세탁기가 세탁물을 깨끗하게 하지 못하고 거품이 먼지를 가둬서 세균이 쌓이게 만든다고 말합니다. 앨리아는 자신이 학교에서 좋은 성적을 받기 위해 모든 것을 아이들 대신 해줬다고 설명합니다.',\n"," '스티브가 매튜에게 전화를 걸어 최근에 살 곳을 찾고 있다고 말한다. 매스티는 그녀의 이웃인 다우 부인을 방문할 예정이다.',\n"," '프랭크는 벳시에게 승진하고 친구들 모두를 위한 큰 파티를 열 것이라고 말한다. 그들은 파티에 참석할 예정이다.']"]},"execution_count":334,"metadata":{},"output_type":"execute_result"}],"source":["preprocessed_summary"]},{"cell_type":"markdown","metadata":{"id":"2-Sb8fNfLgAN"},"source":["### 6) 최종 결과 데이터프레임으로 정리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"186dx3l0LgAN","outputId":"2ae78591-fd0a-4008-cb48-37f13e04224f"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","#######  최종 결과를 데이터프레임으로 정리합니다. ###############################################################################\n"]}],"source":["print(\"\\n#######  최종 결과를 데이터프레임으로 정리합니다. ###############################################################################\")\n","# 최종 결과를 데이터프레임으로 정리합니다.\n","output = pd.DataFrame({\n","    \"fname\": test_data['fname'],  # 파일 이름\n","    \"summary\": preprocessed_summary,  # 전처리된 요약문\n","})"]},{"cell_type":"markdown","metadata":{"id":"WGVsEzeKLgAN"},"source":["### 7) CSV 파일 저장"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dWZ0M21SLgAN","outputId":"0f853bf4-4506-40b8-e860-42e414e77fdf"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","#######  결과를 저장할 경로를 설정하고, 결과를 CSV 파일로 저장합니다.  ##########################################################\n"]}],"source":["print(\"\\n#######  결과를 저장할 경로를 설정하고, 결과를 CSV 파일로 저장합니다.  ##########################################################\")\n","# 결과를 저장할 경로를 설정하고, 결과를 CSV 파일로 저장합니다.\n","result_path = config['inference']['result_path']\n","if not os.path.exists(result_path):\n","    os.makedirs(result_path)  # 경로가 존재하지 않으면 디렉토리를 생성합니다.\n","output.to_csv(os.path.join(result_path, model.replace(\"/\", \"-\") + para + \"output.csv\"), index=False)  # 결과를 CSV 파일로 저장합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJu_pakmLgAO","outputId":"99cb0be9-7ff1-444e-c05a-d7da2ec04458"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fname</th>\n","      <th>summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>test_0</td>\n","      <td>더슨 씨는 #Person1#에게 모든 직원에게 내부 메모로 전달될 수 있도록 지시하...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>test_1</td>\n","      <td>#Person1#은 또 교통 체증에 걸렸다. #Person2#는 #Person1#에...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>test_2</td>\n","      <td>케이트는 #Person1#에게 마샤와 히어로가 이혼을 신청했다고 말한다. 그들은 아...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>test_3</td>\n","      <td>브라이언은 #Person1#의 생일 파티에 참석하여 춤을 추고 있다. 그들은 서로의...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>test_4</td>\n","      <td>#Person1#과 #Person2#는 올림픽 공원의 크고 다양한 시설에 대해 이야...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>494</th>\n","      <td>test_495</td>\n","      <td>잭이 찰리에게 비디오 게임을 제안한다. 찰리는 잭에게 캐릭터를 만드는 게임을 추천한...</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>test_496</td>\n","      <td>#Person2#는 컨트리 음악에 관심을 가지게 된 계기와 라디오 방송국에서 일하기...</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>test_497</td>\n","      <td>앨리스는 세탁기, 건조기, 비누를 사용하는 방법을 #Person1#에게 알려줍니다....</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>test_498</td>\n","      <td>스티브가 매튜에게 전화를 걸어 최근에 살 곳을 찾고 있다고 말한다. 매스티는 그녀의...</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>test_499</td>\n","      <td>프랭크는 벳시에게 승진하고 친구들 모두를 위한 큰 파티를 열 것이라고 말한다. 그들...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>499 rows × 2 columns</p>\n","</div>"],"text/plain":["        fname                                            summary\n","0      test_0  더슨 씨는 #Person1#에게 모든 직원에게 내부 메모로 전달될 수 있도록 지시하...\n","1      test_1  #Person1#은 또 교통 체증에 걸렸다. #Person2#는 #Person1#에...\n","2      test_2  케이트는 #Person1#에게 마샤와 히어로가 이혼을 신청했다고 말한다. 그들은 아...\n","3      test_3  브라이언은 #Person1#의 생일 파티에 참석하여 춤을 추고 있다. 그들은 서로의...\n","4      test_4  #Person1#과 #Person2#는 올림픽 공원의 크고 다양한 시설에 대해 이야...\n","..        ...                                                ...\n","494  test_495  잭이 찰리에게 비디오 게임을 제안한다. 찰리는 잭에게 캐릭터를 만드는 게임을 추천한...\n","495  test_496  #Person2#는 컨트리 음악에 관심을 가지게 된 계기와 라디오 방송국에서 일하기...\n","496  test_497  앨리스는 세탁기, 건조기, 비누를 사용하는 방법을 #Person1#에게 알려줍니다....\n","497  test_498  스티브가 매튜에게 전화를 걸어 최근에 살 곳을 찾고 있다고 말한다. 매스티는 그녀의...\n","498  test_499  프랭크는 벳시에게 승진하고 친구들 모두를 위한 큰 파티를 열 것이라고 말한다. 그들...\n","\n","[499 rows x 2 columns]"]},"execution_count":337,"metadata":{},"output_type":"execute_result"}],"source":["# 결과 확인\n","output"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}